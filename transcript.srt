1
00:00:00,000 --> 00:00:01,800
这不是应该是你吗?这是你的节目

2
00:00:01,800 --> 00:00:03,160
你的节目啊

3
00:00:03,160 --> 00:00:07,919
所以说我觉得这些东西叠加起来就可能世界上只有5个人

4
00:00:07,919 --> 00:00:10,080
但是我在这一步里面看到了两步

5
00:00:10,080 --> 00:00:14,599
一个是底层的模型和他的这个reinforcement learning这一步

6
00:00:14,599 --> 00:00:15,240
欢迎啊

7
00:00:15,240 --> 00:00:16,239
Hello

8
00:00:16,239 --> 00:00:16,839
好大家好

9
00:00:16,839 --> 00:00:18,120
欢迎来到onboard

10
00:00:18,120 --> 00:00:18,920
我是Monica

11
00:00:18,920 --> 00:00:19,920
快点把你整

12
00:00:19,920 --> 00:00:20,879
哈哈哈

13
00:00:20,879 --> 00:00:21,760
来继续继续

14
00:00:21,760 --> 00:00:22,160
对对对

15
00:00:22,160 --> 00:00:23,839
这加的好加的好

16
00:00:23,839 --> 00:00:25,199
对对大家可以感受到了

17
00:00:25,199 --> 00:00:25,519
没错

18
00:00:25,519 --> 00:00:28,719
我们这一次又是一次非常有意思的串台

19
00:00:00,000 --> 00:00:02,240
就是Monica在跟这个柯代表

20
00:00:02,240 --> 00:00:07,000
我们在西雅图终于又在美国第一次线下见面了

21
00:00:07,000 --> 00:00:09,000
对虽然都是前亚马逊同事

22
00:00:09,000 --> 00:00:10,279
但是那个时候没见过

23
00:00:10,279 --> 00:00:11,000
对对对

24
00:00:11,000 --> 00:00:14,519
然后我们上一次见面应该还是在深圳

25
00:00:14,519 --> 00:00:15,720
然后我也是刚回

26
00:00:15,720 --> 00:00:17,719
我们都是刚回国不久

27
00:00:18,199 --> 00:00:21,199
没有第一次是刚回不久

28
00:00:21,199 --> 00:00:23,920
第二次的深圳是我刚要回美国

29
00:00:23,920 --> 00:00:24,679
对对对

30
00:00:24,679 --> 00:00:27,719
就每次我们见面都是一个非常重要的时间节点

31
00:00:00,000 --> 00:00:03,200
而这一次特别的一个地方也是在于说

32
00:00:03,200 --> 00:00:07,200
我们之所以来有这么一次这个录制这个话题的这个

33
00:00:07,200 --> 00:00:11,400
机会也是非常的意外和惊喜

34
00:00:11,400 --> 00:00:15,800
也是我们那次最近我正好来到西雅图这边出差

35
00:00:15,800 --> 00:00:17,800
然后我们在一次这个聚餐上

36
00:00:17,800 --> 00:00:21,800
然后发现聚餐中有一位小哥哥特别的有insight

37
00:00:21,800 --> 00:00:23,399
跟我们这个火花不断

38
00:00:23,399 --> 00:00:25,800
我们就觉得说这个一次吃饭显然是不够的

39
00:00:25,800 --> 00:00:29,399
所以说就专门把这位小哥哥这个邀请到了

40
00:00:00,000 --> 00:00:07,440
我们的这个节目来跟大家一起去探讨很多他所自己经历的这一路以来

41
00:00:07,440 --> 00:00:10,279
这个Machine Learning他所看到的大语言模型

42
00:00:10,279 --> 00:00:15,160
以及他所在的公司所经历大语言模型的一些这个探索

43
00:00:15,160 --> 00:00:17,719
而且我相信这一次因为有柯代表的加持

44
00:00:17,719 --> 00:00:23,800
我们可以在很多技术历史甚至实的虚的思考上

45
00:00:23,800 --> 00:00:25,879
相信能够碰上出更多的这个火花

46
00:00:25,879 --> 00:00:29,679
那我们要不先由这位小哥来做自我介绍

47
00:00:00,000 --> 00:00:02,600
谢谢那个Monica的介绍

48
00:00:02,600 --> 00:00:04,599
还有欢迎谢谢克莱比奥

49
00:00:04,599 --> 00:00:07,040
邀请我一起做这期节目

50
00:00:07,040 --> 00:00:09,400
然后我可以简单做一下自我介绍

51
00:00:09,400 --> 00:00:10,640
然后我叫卢毅

52
00:00:10,640 --> 00:00:14,199
我现在在一家C轮的创业公司

53
00:00:14,199 --> 00:00:16,679
做智能客服的创业公司叫ForSot

54
00:00:16,679 --> 00:00:18,559
做他们的head of machine learning

55
00:00:18,559 --> 00:00:20,920
然后我同时还在University of Washington

56
00:00:20,920 --> 00:00:22,519
做他们的科做教授

57
00:00:22,519 --> 00:00:26,679
之前我是在苹果做一些机器学习方面的研究

58
00:00:26,679 --> 00:00:28,839
主要在theory里面做他们的

59
00:00:00,000 --> 00:00:02,240
比如说事实性问题的问答

60
00:00:02,240 --> 00:00:04,320
也做一些检索相关的工作

61
00:00:04,320 --> 00:00:06,679
所以我也是非常高兴

62
00:00:06,679 --> 00:00:09,000
这次能够和大家一起来分享一些我的观点

63
00:00:09,000 --> 00:00:10,759
然后和大家一起探讨学习

64
00:00:12,160 --> 00:00:14,199
好的 我叫孙宇峥

65
00:00:14,199 --> 00:00:16,039
然后我的网名叫柯代表立正

66
00:00:16,039 --> 00:00:17,760
是B站和油管的账号

67
00:00:17,760 --> 00:00:19,440
所以说叫我柯代表就行

68
00:00:19,440 --> 00:00:21,239
我是一个做数据的

69
00:00:21,239 --> 00:00:22,480
之前是经济学博士

70
00:00:22,480 --> 00:00:23,480
然后在亚马逊

71
00:00:23,480 --> 00:00:24,160
Facebook

72
00:00:24,160 --> 00:00:27,000
腾讯都做数据方面的工作

73
00:00:00,000 --> 00:00:02,919
最近去加入了一家startup叫statseek

74
00:00:02,919 --> 00:00:05,559
是就是statistical significant的

75
00:00:05,559 --> 00:00:06,200
缩写

76
00:00:06,200 --> 00:00:08,839
它是做ABCN平台的公司

77
00:00:08,839 --> 00:00:13,359
我平时也对就是技术啊商业啊都非常感兴趣

78
00:00:13,359 --> 00:00:17,000
不管是工作中还是课外中都会有很多思考和产出相关的内容

79
00:00:17,000 --> 00:00:19,879
我因为在2月份的时候

80
00:00:19,879 --> 00:00:21,679
应该是去年10月份的时候吧

81
00:00:21,679 --> 00:00:25,320
跟Howie聊了之后就一直非常关注AGI的东西

82
00:00:25,320 --> 00:00:27,160
结果就跟他聊完了以后

83
00:00:27,160 --> 00:00:28,280
Mid Journey出来了

84
00:00:00,000 --> 00:00:05,080
然后ChaiGBT出来了之后就一直思考也写了很多这方面的文章

85
00:00:05,080 --> 00:00:09,080
包括在M小姐的公众号上发过关于ChaiGBT的最重要的5个问题

86
00:00:09,080 --> 00:00:16,039
也一直在不断的寻找有insight的人去进行交流

87
00:00:16,039 --> 00:00:17,600
提升我自己的认知

88
00:00:17,600 --> 00:00:19,640
从如意这里边学到了很多东西

89
00:00:19,640 --> 00:00:21,559
所以说就赶紧抓住机会

90
00:00:21,559 --> 00:00:24,760
趁着Monica在的时候我们赶紧一起做个节目

91
00:00:24,760 --> 00:00:27,359
我好奇就是我们要不作为一个开场

92
00:00:00,000 --> 00:00:02,839
就那天我们吃饭的时候聊到了很多话题

93
00:00:02,839 --> 00:00:07,799
我好奇是当时卢毅聊到了哪几个点

94
00:00:07,799 --> 00:00:11,919
让你萌生了要跟他做进一步交友的想法

95
00:00:11,919 --> 00:00:13,599
有哪几个你比较印象深刻的点

96
00:00:13,599 --> 00:00:15,199
我们可以作为一个开场

97
00:00:15,480 --> 00:00:20,079
首先我觉得我们在那个群里边有各种各样的人

98
00:00:20,079 --> 00:00:20,440
对吧

99
00:00:20,440 --> 00:00:24,960
就是我觉得大家对这件事情的理解到位的人就已经不多了

100
00:00:25,079 --> 00:00:26,839
就是虽然这么多人关注

101
00:00:26,839 --> 00:00:29,480
虽然我们的群里边已经是这里边讨论最内部

102
00:00:00,000 --> 00:00:02,759
但是我觉得真的对他有比较到位的

103
00:00:02,759 --> 00:00:08,000
就是事实性都是事实正确且理解到位的人

104
00:00:08,000 --> 00:00:09,759
我觉得应该不到100个

105
00:00:09,759 --> 00:00:12,000
然后在这些人里边

106
00:00:12,000 --> 00:00:17,480
你又能有一线经验且能不断帮助你update认知的人就更少

107
00:00:17,480 --> 00:00:22,359
但是更重要的是卢伊本身他在苹果的时候

108
00:00:22,359 --> 00:00:23,760
就是做这方面的

109
00:00:23,760 --> 00:00:26,480
只不过不是用GPT的方式去做

110
00:00:26,480 --> 00:00:29,359
而和现在所在的这个公司

111
00:00:00,000 --> 00:00:02,240
就是一個人工智能

112
00:00:02,240 --> 00:00:05,000
人工智能enable的客服公司

113
00:00:05,000 --> 00:00:07,120
又是大約模型

114
00:00:07,120 --> 00:00:10,080
最promising的應用場景

115
00:00:10,080 --> 00:00:12,580
所以說我覺得這些東西疊加起來

116
00:00:12,580 --> 00:00:14,580
就可能世界上只有五個日月

117
00:00:18,300 --> 00:00:21,800
就是他在講他們的技術選擇的時候

118
00:00:21,800 --> 00:00:24,800
應該是在講17年前後

119
00:00:24,800 --> 00:00:27,339
有一個人在亞馬遜做了一個模型

120
00:00:00,000 --> 00:00:04,280
然后他可以在review里边extract sentiment

121
00:00:04,280 --> 00:00:07,160
然后他当时没有

122
00:00:07,160 --> 00:00:09,359
大家觉得这个东西是一个很正常的东西

123
00:00:09,359 --> 00:00:10,800
可是他们看到了一个

124
00:00:10,800 --> 00:00:13,000
就是很大的希望

125
00:00:13,000 --> 00:00:17,239
因为你只是做从embedding里边就可以extract sentiment

126
00:00:17,239 --> 00:00:18,600
哦 我明白你的意思

127
00:00:18,600 --> 00:00:21,199
我忘了具体是什么模型和具体是

128
00:00:21,199 --> 00:00:24,280
应该是在Elia的一次访谈里边

129
00:00:24,280 --> 00:00:26,280
有人问他就是说你的那个

130
00:00:26,280 --> 00:00:29,359
我们可以现在找一下出一系列的research

131
00:00:00,000 --> 00:00:04,280
training on just max token character in the text of the amazon review

132
00:00:04,799 --> 00:00:07,080
这个其实我觉得一方面来讲

133
00:00:08,439 --> 00:00:10,519
怎么说surprise也可以说不surprise

134
00:00:10,519 --> 00:00:13,279
我们它其实怎么说的是unsupervised learning

135
00:00:13,279 --> 00:00:15,599
对这其实这里面如果更specific

136
00:00:15,599 --> 00:00:17,440
他们其实是叫self-supervised

137
00:00:17,440 --> 00:00:20,920
就是我们说什么叫unsupervised

138
00:00:20,920 --> 00:00:22,399
就是说完全没有label对吗

139
00:00:22,399 --> 00:00:24,199
就是你完全不知道你下面要predict

140
00:00:24,199 --> 00:00:26,320
那正好像一个cluster的问题

141
00:00:26,320 --> 00:00:27,600
把东西分类

142
00:00:27,600 --> 00:00:29,199
就是group在一起

143
00:00:00,000 --> 00:00:01,000
那其实是没有label

144
00:00:01,000 --> 00:00:02,480
那就是unsupervised

145
00:00:02,480 --> 00:00:04,120
那有label就是supervised

146
00:00:04,120 --> 00:00:06,759
那像这种next token prediction

147
00:00:06,759 --> 00:00:09,519
实际上是一种比较聪明的方式去自己supervised自己

148
00:00:09,519 --> 00:00:09,960
对

149
00:00:09,960 --> 00:00:12,359
因为你其实没有人给你提供任何一个label

150
00:00:12,359 --> 00:00:14,080
但是下一个token就是你的label

151
00:00:14,080 --> 00:00:14,519
对

152
00:00:14,519 --> 00:00:16,960
那其实这个最开始的时候

153
00:00:16,960 --> 00:00:18,679
当然了这是我的观点

154
00:00:18,679 --> 00:00:19,960
我的理解就是说

155
00:00:20,519 --> 00:00:22,120
最开始的时候这种语言模型

156
00:00:22,120 --> 00:00:25,359
他们的这个language modeling task

157
00:00:25,359 --> 00:00:27,839
其实最开始就是predict next token

158
00:00:27,839 --> 00:00:28,839
到现在也是

159
00:00:28,839 --> 00:00:29,559
到现在也是

160
00:00:00,000 --> 00:00:04,080
但中间有时候大家会去就是从中偏离出来

161
00:00:04,080 --> 00:00:06,719
就是也许不认为是这种方式是最好的

162
00:00:06,719 --> 00:00:10,119
那一开始大家就想法就是说

163
00:00:10,119 --> 00:00:11,560
如果你能够prepare下一个token

164
00:00:11,560 --> 00:00:14,400
你相当于对之前的语言有一个很好的理解

165
00:00:14,400 --> 00:00:15,960
那这个理解什么呢

166
00:00:15,960 --> 00:00:18,399
其实你可以理解成就是它的embedding的东西

167
00:00:18,399 --> 00:00:20,440
如果你对这个语言有很好的理解

168
00:00:20,440 --> 00:00:22,640
那是不是这个embedding就应该包含了

169
00:00:22,640 --> 00:00:25,120
这个语言当中本来的一些信息

170
00:00:25,120 --> 00:00:27,359
剩下的部分只是从这个embedding中

171
00:00:27,359 --> 00:00:29,160
提取出你已经学到的信息

172
00:00:00,000 --> 00:00:03,000
这就是为什么要选择一个task

173
00:00:03,000 --> 00:00:04,599
作为language modeling task

174
00:00:04,599 --> 00:00:07,599
那就是说他们认为如果你能完成这个task

175
00:00:07,599 --> 00:00:09,400
代表你对语言有掌握

176
00:00:09,400 --> 00:00:14,199
那这种方面就是说不见得一定是next token prediction

177
00:00:14,199 --> 00:00:15,800
才是language modeling task

178
00:00:15,800 --> 00:00:16,600
比如说BERT

179
00:00:16,600 --> 00:00:17,679
它其实一开始的时候

180
00:00:17,679 --> 00:00:20,920
GPT或者说这种类型叫auto-regressor模型

181
00:00:20,920 --> 00:00:22,440
你有前面的东西

182
00:00:22,440 --> 00:00:23,519
然后predict下一个

183
00:00:23,519 --> 00:00:25,600
就非常适常于生成

184
00:00:25,600 --> 00:00:26,879
那人们就发现有一些问题

185
00:00:26,879 --> 00:00:28,920
就是比如说你在学习这个

186
00:00:00,000 --> 00:00:01,760
因为我的目标是学习语言嘛

187
00:00:01,760 --> 00:00:05,320
并不是并不完全所有的task都是为了生成

188
00:00:05,320 --> 00:00:06,559
那我为了学习语言

189
00:00:06,559 --> 00:00:08,519
那么我去做这种

190
00:00:09,039 --> 00:00:11,039
predict下一个token的情况下的时候

191
00:00:11,039 --> 00:00:13,199
那有可能会失去后面的context

192
00:00:13,400 --> 00:00:13,960
对不对

193
00:00:14,080 --> 00:00:15,519
就是我并不知道后面的东西

194
00:00:15,519 --> 00:00:18,719
没有让我后面的词来去描述之前的词

195
00:00:18,719 --> 00:00:20,000
去学这个词的意思

196
00:00:20,000 --> 00:00:20,399
对

197
00:00:20,760 --> 00:00:21,760
对就是

198
00:00:21,760 --> 00:00:23,039
所以这就是所谓的

199
00:00:23,039 --> 00:00:24,960
我觉得这counterintuitive和

200
00:00:24,960 --> 00:00:27,079
为什么这个研究如此的重要

201
00:00:00,000 --> 00:00:03,399
和他为什么就是伊莲娜觉得如此的重要

202
00:00:03,399 --> 00:00:08,240
就是他这个contract interative和需要你有一定的信仰的东西

203
00:00:08,240 --> 00:00:11,480
就是你的你刚刚说的next token

204
00:00:12,320 --> 00:00:14,679
包含了所有的信息

205
00:00:14,679 --> 00:00:15,279
或者说

206
00:00:15,279 --> 00:00:16,719
不包含之前所有的信息

207
00:00:16,719 --> 00:00:18,559
不光是能够包含所有的信息

208
00:00:18,559 --> 00:00:21,280
而且他从一定程度上也包含了之后的信息

209
00:00:27,000 --> 00:00:28,160
这个不好说

210
00:00:28,160 --> 00:00:29,079
对这个怎么理解

211
00:00:00,000 --> 00:00:01,159
它还可以包含之后

212
00:00:01,159 --> 00:00:02,240
就刚刚比如说的

213
00:00:02,240 --> 00:00:03,759
就是刚刚说的一个很重要的

214
00:00:03,759 --> 00:00:06,440
BERT和这个GPT思路的区别

215
00:00:06,440 --> 00:00:07,719
就是BERT是双向的

216
00:00:07,719 --> 00:00:07,919
对吧

217
00:00:07,919 --> 00:00:09,000
然后GPT是单向的

218
00:00:09,000 --> 00:00:09,759
对

219
00:00:09,759 --> 00:00:11,679
然后它为什么要用双向

220
00:00:11,679 --> 00:00:13,839
就是因为语言中的上下文

221
00:00:13,839 --> 00:00:15,480
你既然已经能看到全文了

222
00:00:15,480 --> 00:00:18,600
但你当然也应该用全文去理解这个词

223
00:00:18,600 --> 00:00:21,800
那你的全文就包含了之前的信息和之后的信息

224
00:00:21,800 --> 00:00:22,079
对

225
00:00:22,079 --> 00:00:23,679
所以说从这个角度来说

226
00:00:23,679 --> 00:00:25,519
我们想要精确的预测出来

227
00:00:25,519 --> 00:00:27,000
就是给你一篇文章

228
00:00:27,000 --> 00:00:28,280
现在你要去学习这篇文章

229
00:00:00,000 --> 00:00:04,080
然后从这些词里面清确的知道这个词是什么的情况下

230
00:00:04,080 --> 00:00:06,080
你就应该知道全文是最好的

231
00:00:06,080 --> 00:00:07,400
对 肯定的是

232
00:00:07,400 --> 00:00:09,519
你有双向的注意力是最好的

233
00:00:09,519 --> 00:00:10,800
但现在就是像是说GBT

234
00:00:10,800 --> 00:00:12,519
它选择了只有单向的注意力

235
00:00:12,519 --> 00:00:13,640
没有双向的注意力

236
00:00:13,640 --> 00:00:16,280
后来我觉得这个发展应该是一开始的时候

237
00:00:16,280 --> 00:00:17,079
人们认为单向

238
00:00:17,079 --> 00:00:22,239
那这个学习语言就是需要这种做这种单向的生成就可以了

239
00:00:22,239 --> 00:00:25,519
这就是一个by definition什么是language modeling task

240
00:00:25,519 --> 00:00:28,480
那之后人们认为双向的注意力是有好处的

241
00:00:00,000 --> 00:00:02,279
我觉得这也是人们发现确实是有好处的

242
00:00:02,279 --> 00:00:03,080
我觉得不是

243
00:00:03,080 --> 00:00:03,839
我觉得是一个

244
00:00:03,839 --> 00:00:05,639
就是我说发现这个词

245
00:00:05,639 --> 00:00:06,919
我不是说确实是有好处的

246
00:00:06,919 --> 00:00:07,320
肯定是

247
00:00:07,320 --> 00:00:08,800
但是我觉得说发现这个词

248
00:00:08,800 --> 00:00:09,640
我不同意

249
00:00:09,640 --> 00:00:10,519
为什么呢

250
00:00:10,519 --> 00:00:12,240
就是在这之上

251
00:00:12,240 --> 00:00:13,439
其实我就看到了

252
00:00:13,439 --> 00:00:15,800
就是我自己对OpenAI的感觉

253
00:00:15,880 --> 00:00:17,800
就是他们一直在走这一条路

254
00:00:17,800 --> 00:00:20,600
这条路是可以lead to AGI的那一

255
00:00:20,600 --> 00:00:21,719
唯一的一条路

256
00:00:21,719 --> 00:00:23,199
然后剩下所有的人呢

257
00:00:23,199 --> 00:00:26,480
都是在application的过程中take各种shortcut

258
00:00:26,480 --> 00:00:27,559
cut各种corners

259
00:00:27,559 --> 00:00:28,320
可以这么理解

260
00:00:28,320 --> 00:00:28,679
对

261
00:00:00,000 --> 00:00:03,319
它的card corners的体现在什么

262
00:00:03,319 --> 00:00:05,879
就是体现在你用更多的信息

263
00:00:05,879 --> 00:00:07,719
或者更specific的任务

264
00:00:07,719 --> 00:00:10,119
或者说你加入了人更多的hard coding

265
00:00:10,119 --> 00:00:11,960
就是它是不同程度的hard coding

266
00:00:11,960 --> 00:00:14,480
像亚马逊Alexa就是写各种if function

267
00:00:14,480 --> 00:00:15,880
如果这个人说了这个

268
00:00:15,880 --> 00:00:16,399
对

269
00:00:16,399 --> 00:00:19,960
然后你就可以在一些任务上perform的更好

270
00:00:19,960 --> 00:00:21,800
它在一些任务上perform更好

271
00:00:21,800 --> 00:00:22,839
它就不general

272
00:00:22,839 --> 00:00:23,760
对

273
00:00:23,760 --> 00:00:28,359
然后OpenAI从头到尾就是二和三为什么坚持下来那么难

274
00:00:00,000 --> 00:00:04,040
和我觉得那么就是展示的这些人非常不一样的地方

275
00:00:04,040 --> 00:00:06,400
就是因为R是比Bert的差很远

276
00:00:06,400 --> 00:00:07,400
各方面都不如

277
00:00:07,400 --> 00:00:10,439
3是有来有回或者说开始超越了

278
00:00:10,439 --> 00:00:13,320
但是从2和3之间的这个坚持是很难的

279
00:00:13,320 --> 00:00:13,960
是的

280
00:00:13,960 --> 00:00:17,280
对他们就坚持了那个唯一一个最general的方向

281
00:00:17,280 --> 00:00:20,719
就是你想想如果一个最general的东西

282
00:00:20,719 --> 00:00:24,079
就必须要用只用上文去predict下文

283
00:00:24,079 --> 00:00:26,160
之外的所有的hard coding或者什么的

284
00:00:26,160 --> 00:00:29,120
都会让他也许在一些事情上做得更好

285
00:00:00,000 --> 00:00:04,160
就是他在这过程中他真的把这些人力物力拿过去

286
00:00:04,160 --> 00:00:06,320
去做一个specific任务的话

287
00:00:06,320 --> 00:00:08,320
我相信他早就做得很好

288
00:00:08,320 --> 00:00:10,160
但是他就到不了

289
00:00:10,160 --> 00:00:11,720
我觉得你确实我同意你说的

290
00:00:11,720 --> 00:00:13,640
这其实是非常需要信仰的

291
00:00:13,640 --> 00:00:16,120
就是说在这个发展的过程当中

292
00:00:16,120 --> 00:00:18,199
其实那transformer出现了以后

293
00:00:18,199 --> 00:00:20,079
出现了三个发展的方向

294
00:00:20,079 --> 00:00:23,000
可以说第一个就是纯Auto-regressive的模型

295
00:00:23,000 --> 00:00:24,559
那这种就是Next-token prediction

296
00:00:24,559 --> 00:00:25,800
那他们的目标就是为了

297
00:00:25,800 --> 00:00:27,679
只有一个目标就是为了生成

298
00:00:00,000 --> 00:00:02,439
因为我只有我为生成目标的时候

299
00:00:02,439 --> 00:00:03,480
我是没有后文的

300
00:00:03,480 --> 00:00:04,320
我只有前文

301
00:00:04,320 --> 00:00:05,559
我要做生成

302
00:00:05,559 --> 00:00:08,480
那如果你的目标或者说你认为真正的AGI

303
00:00:08,919 --> 00:00:11,560
最后就一定要完成很好的生成性任务的话

304
00:00:11,560 --> 00:00:15,160
那其实这种Auto-regressive的component是不能少的

305
00:00:15,160 --> 00:00:16,920
你一定要去完成这件事情

306
00:00:16,920 --> 00:00:19,199
T5的时间是什么时候

307
00:00:19,719 --> 00:00:21,679
我们可以查一查T5是什么时候

308
00:00:21,679 --> 00:00:26,160
但是我估计也是个1920年左右的

309
00:00:26,160 --> 00:00:26,640
对

310
00:00:00,000 --> 00:00:06,400
就是他们有信仰去说自己用Auto-regressive这件事情去做AGI

311
00:00:06,400 --> 00:00:09,160
是17年的时候的信仰

312
00:00:09,160 --> 00:00:15,359
然后这个T5也就是所有的自然语言任务都可以被生成式任务统一

313
00:00:15,359 --> 00:00:16,559
是20年的文章

314
00:00:16,559 --> 00:00:20,920
就是他们在不知道T5的那个结论的时候

315
00:00:20,920 --> 00:00:24,920
我们现在回去看的时候觉得好像一切都make sense对吧

316
00:00:24,920 --> 00:00:29,559
但是在那个时候你不知道你的所有任务是可以被生成式任务给统一的

317
00:00:00,000 --> 00:00:01,760
然后你就走了AutoRegressive

318
00:00:01,760 --> 00:00:03,160
那是很难很难的一件事

319
00:00:03,160 --> 00:00:06,200
还有就是像另外就是说你

320
00:00:06,200 --> 00:00:08,000
当然你当时人民就可以选择说

321
00:00:08,000 --> 00:00:09,480
如果我要得到一个好的效果

322
00:00:09,480 --> 00:00:11,240
我可以选择是一种双向注意力

323
00:00:11,240 --> 00:00:12,320
就是这种AutoEncoding

324
00:00:12,320 --> 00:00:13,080
像BERT的方向

325
00:00:13,080 --> 00:00:14,359
同时还有一种就是

326
00:00:14,359 --> 00:00:19,079
那结合了AutoEncoder和AutoRegressive的方法

327
00:00:19,079 --> 00:00:20,320
比如说有一个叫BART

328
00:00:20,320 --> 00:00:23,519
它的模型就是先做像BERT一样的东西

329
00:00:23,519 --> 00:00:26,679
获得它的像是表达

330
00:00:26,679 --> 00:00:29,320
再通过一个生成性模型再去做生成

331
00:00:00,000 --> 00:00:02,759
那如果我们其实在那一刻的时候

332
00:00:02,759 --> 00:00:05,320
其实我也在那一刻的时候去了解这种这样的模型嘛

333
00:00:05,320 --> 00:00:07,400
那一刻是什么时候

334
00:00:07,400 --> 00:00:09,240
就是比如说BERT刚刚出来的时候

335
00:00:09,240 --> 00:00:14,519
就比如说是2021年或者是20年21年的时候

336
00:00:14,519 --> 00:00:17,559
那种时候就是大家其实确实非常的以后

337
00:00:17,559 --> 00:00:21,199
就是说这三个思路到底要往哪个方向去发展

338
00:00:21,199 --> 00:00:25,359
对明显我们知道这种像BERT这样的模型

339
00:00:25,359 --> 00:00:26,640
它的发展是更多的

340
00:00:26,640 --> 00:00:28,519
注意力更多效果会更好

341
00:00:00,000 --> 00:00:04,480
大家有很多文章都是讲的是如何提高这种东西的模型

342
00:00:04,480 --> 00:00:06,719
甚至就是说我如何去改变

343
00:00:07,599 --> 00:00:09,800
就用一种非常巧妙的方式

344
00:00:09,800 --> 00:00:13,400
用BERT这种双向注意力去完成生雄式问题

345
00:00:13,400 --> 00:00:15,919
这其实我觉得在那一刻你也是无可厚非的

346
00:00:15,919 --> 00:00:18,399
就是就算是为了要完成这个好问题

347
00:00:18,760 --> 00:00:20,920
那你可能人们都不知道是用哪种方式

348
00:00:20,920 --> 00:00:23,120
更是能够有最好的结果

349
00:00:23,120 --> 00:00:25,800
但最后其实OpenAI也向大家证明了

350
00:00:25,800 --> 00:00:28,839
就是这种Auto-regressive单向注意力的

351
00:00:00,000 --> 00:00:04,000
就通過各種各樣的方法是可以完成到很好的

352
00:00:04,000 --> 00:00:07,400
對 就是這個信仰之月是月在哪裡呢

353
00:00:07,400 --> 00:00:10,400
就是其他的東西都是短時間

354
00:00:10,400 --> 00:00:11,720
一定程度內效果好

355
00:00:11,720 --> 00:00:14,599
問題是GPT這個方向你根本不知道能不能成

356
00:00:14,599 --> 00:00:17,000
對 就是你剛才所說的這個就是

357
00:00:17,000 --> 00:00:20,199
二到三之間就是他們看到了什麼

358
00:00:20,199 --> 00:00:22,920
就是在二到三之間他們看到了什麼

359
00:00:22,920 --> 00:00:24,280
別人沒有看到的

360
00:00:24,280 --> 00:00:25,760
或者說有一些大家都看到了

361
00:00:25,760 --> 00:00:28,239
但是他們從裡面看到了一個更

362
00:00:00,000 --> 00:00:01,919
对,我觉得17年这个文章真的很重要

363
00:00:01,919 --> 00:00:04,120
就是这个unsupervised sentiment neuron

364
00:00:04,120 --> 00:00:07,719
就是这个是告诉他们这个东西是可能可以的

365
00:00:07,719 --> 00:00:10,839
不是说可行甚至,就是可能可行

366
00:00:10,839 --> 00:00:12,839
然后他们就背着这个可能可行去了

367
00:00:12,839 --> 00:00:18,239
而且是通过这种暴力美学的加算的方式

368
00:00:18,239 --> 00:00:21,399
是可能可行到一个突破threshold的

369
00:00:21,399 --> 00:00:22,760
对

370
00:00:00,000 --> 00:00:08,759
我好奇你前面也提到

371
00:00:08,759 --> 00:00:10,759
我相信你那篇文章出來以後

372
00:00:10,759 --> 00:00:13,119
也會有很多人找你來探討

373
00:00:13,119 --> 00:00:15,119
你感覺到GPT

374
00:00:15,119 --> 00:00:18,239
你看GPT出來也大半年已經過去了

375
00:00:18,239 --> 00:00:19,559
你感覺這過程中

376
00:00:19,559 --> 00:00:24,519
大家仍然最常見的一些誤解可能是在哪

377
00:00:24,519 --> 00:00:26,399
最常見的誤解就是

378
00:00:26,399 --> 00:00:29,879
永遠會發生的短期高估和長期低估

379
00:00:00,000 --> 00:00:01,740
這個Howie當時跟他聊的時候

380
00:00:01,740 --> 00:00:04,660
他說在雲和Mobile上都出現過這樣的情況

381
00:00:04,660 --> 00:00:06,259
那他不理解就是說為什麼會這樣的

382
00:00:06,259 --> 00:00:07,860
現在我就理解了為什麼會這樣

383
00:00:07,860 --> 00:00:09,660
就是因為短期的時候

384
00:00:09,660 --> 00:00:13,419
大家想的是用Chai GPT能解決我現在手上的什麼問題

385
00:00:13,419 --> 00:00:15,660
這一定是一個小機會

386
00:00:15,660 --> 00:00:17,100
因為Chai GPT對比

387
00:00:17,100 --> 00:00:21,019
因為現在能看到的問題都會被現有的技術解決的還不錯

388
00:00:21,019 --> 00:00:22,899
所以Chai GPT也許能增效

389
00:00:22,899 --> 00:00:24,579
能增效個50%是了不起了

390
00:00:24,579 --> 00:00:29,460
但是它真正能給我們帶來的一定是我們現在還不存在的問題

391
00:00:00,000 --> 00:00:02,520
嗯对就是你在mobile之前

392
00:00:02,520 --> 00:00:04,320
Uber是不可能做的对吧

393
00:00:04,320 --> 00:00:06,559
但是有了mobile才可能做Uber

394
00:00:06,559 --> 00:00:09,759
但是你需要很多东西都存在了以后才能做Uber

395
00:00:09,759 --> 00:00:11,519
嗯你互联网之前

396
00:00:11,519 --> 00:00:13,720
Google是不可能做的

397
00:00:13,720 --> 00:00:14,880
但是互联网刚出来的时候

398
00:00:14,880 --> 00:00:15,599
Google也做不了

399
00:00:15,599 --> 00:00:17,280
因为那个时候没有什么信息在网上

400
00:00:17,280 --> 00:00:19,600
嗯我觉得这是大家最大的误解

401
00:00:19,600 --> 00:00:21,359
诶卢伊呢从你的角度

402
00:00:21,359 --> 00:00:23,760
我觉得正好我有一个特别相关的话题

403
00:00:23,760 --> 00:00:26,120
就是其实大学模型出来了以后

404
00:00:26,120 --> 00:00:27,800
我就在思考一个问题

405
00:00:00,000 --> 00:00:03,200
就是说我们所谓的应用型的机器学习

406
00:00:03,200 --> 00:00:04,679
像applied machine learning

407
00:00:04,679 --> 00:00:06,559
这个领域到底之后还会不会存在

408
00:00:06,559 --> 00:00:09,359
或者说什么是应用型机器学习

409
00:00:09,359 --> 00:00:13,240
比如说我们原来就认为机器学习分成几个部分

410
00:00:13,240 --> 00:00:15,679
有些人是更加偏工程型的

411
00:00:15,679 --> 00:00:16,000
对

412
00:00:16,000 --> 00:00:17,800
那就是有一些工程性的问题

413
00:00:17,800 --> 00:00:19,480
就其实和software engineer

414
00:00:19,480 --> 00:00:21,000
我认为其实没有本质上的区别

415
00:00:21,000 --> 00:00:21,960
我是learning engineer嘛

416
00:00:21,960 --> 00:00:22,399
对

417
00:00:22,399 --> 00:00:23,719
其实没有本质上区别

418
00:00:23,719 --> 00:00:26,480
他只做的是不同类型的engineering问题

419
00:00:26,480 --> 00:00:28,239
不可能说有人叫as engineer

420
00:00:00,000 --> 00:00:03,200
有些人叫什么Gateway Engineer

421
00:00:03,200 --> 00:00:04,679
但其实都是Software Engineer

422
00:00:04,679 --> 00:00:05,440
Machine Learning Engineer

423
00:00:05,440 --> 00:00:07,519
其实在很多时候只是它的context有一些不一样

424
00:00:07,519 --> 00:00:09,119
我觉得这是最engineering的程度

425
00:00:09,119 --> 00:00:10,759
那其实还有另外一个极端

426
00:00:10,759 --> 00:00:12,679
就是像Scientist

427
00:00:12,679 --> 00:00:16,239
他们就是做的东西其实就比较脱离

428
00:00:16,239 --> 00:00:18,960
因为我也做过一些关于research方面的工作嘛

429
00:00:18,960 --> 00:00:21,480
然后其实就主要就是

430
00:00:21,480 --> 00:00:24,719
你有一个具体的一个非常well defined的一个task

431
00:00:24,719 --> 00:00:27,199
你去如何找到这些数据

432
00:00:00,000 --> 00:00:03,200
如何提高在这个东西上面的效果

433
00:00:03,200 --> 00:00:04,280
也是一个比较玻璃的

434
00:00:04,280 --> 00:00:06,879
就比较怎么说理论比较强的

435
00:00:06,879 --> 00:00:08,560
那中间就有一个

436
00:00:08,560 --> 00:00:10,560
applied scientist

437
00:00:10,560 --> 00:00:12,119
or applied machine learning

438
00:00:12,119 --> 00:00:15,519
就是把这些技术通过工程的方式

439
00:00:15,519 --> 00:00:19,000
应用在一个具体的产品或者一种形式上

440
00:00:19,000 --> 00:00:21,399
去所谓实现商业价值

441
00:00:21,399 --> 00:00:25,800
那之后这个中间的行业到底还是什么样子

442
00:00:25,800 --> 00:00:27,199
未来会变成怎么样的发展

443
00:00:00,000 --> 00:00:02,919
我觉得我自己是特别的好奇这个方面

444
00:00:02,919 --> 00:00:04,879
嗯 那其实在今天来讲

445
00:00:04,879 --> 00:00:08,640
就你当然了我们长期看也许这个行业就已经消失了

446
00:00:08,640 --> 00:00:11,599
嗯 就变成了每个人都应该获得的技能

447
00:00:11,599 --> 00:00:13,519
就比如说我们每个人不会说

448
00:00:13,519 --> 00:00:16,120
我需要一个人去帮我去做Google engineer

449
00:00:16,120 --> 00:00:17,640
就如果我们去做Google search

450
00:00:17,640 --> 00:00:19,199
每个人都会去做这件事情

451
00:00:19,199 --> 00:00:21,519
每个人可能都应该会去做Prompt engineer

452
00:00:21,519 --> 00:00:23,960
那这个apply到底是有什么样的变化

453
00:00:23,960 --> 00:00:24,839
以后会怎么发展

454
00:00:24,839 --> 00:00:29,399
嗯 就是也是一个我们日后可以讨论的一个有意思的话题吧

455
00:00:00,000 --> 00:00:03,759
对,这个其实我本来是想留到最后有一个类似的讨论嘛,

456
00:00:03,759 --> 00:00:06,719
其实就是我们吃饭的时候和我当时总结的问题第4个,

457
00:00:06,719 --> 00:00:12,160
就是当JPT把它的reinforcement learning这个layer开放给大家的时候,

458
00:00:12,160 --> 00:00:18,160
它是不是市面上现在所有的这种所谓的垂直大模型之类的全都会被干掉,

459
00:00:18,160 --> 00:00:20,359
我们的观点是会的,对吧?

460
00:00:20,359 --> 00:00:22,079
我觉得只是reinforcement learning嘛,

461
00:00:22,079 --> 00:00:25,039
就是说比如说它只是如果把那个,

462
00:00:25,039 --> 00:00:28,039
比方说fine tuning这一块也放也放开的更彻底的话,

463
00:00:00,000 --> 00:00:02,000
你觉得足够吗?还是说必须得要把

464
00:00:02,000 --> 00:00:04,480
reinforcement learning这块放开才可以。

465
00:00:05,480 --> 00:00:07,919
我在那个文章里边一直强调的就是

466
00:00:07,919 --> 00:00:10,960
Fenton这个词现在太broad,所以说大家在说

467
00:00:10,960 --> 00:00:13,199
Fenton的时候,不同的人指代的是不同的意思,

468
00:00:13,199 --> 00:00:15,759
所以就不能随便的使用那个词。

469
00:00:16,079 --> 00:00:19,039
呃,然后我我的理解啊,就是

470
00:00:19,039 --> 00:00:22,280
卢奕欢,你纠正我,随时纠正我,就是

471
00:00:22,280 --> 00:00:26,920
GPT4是它的底层大模型,接下来的那一层是

472
00:00:26,920 --> 00:00:29,039
reinforcement learning with human feedback,

473
00:00:00,000 --> 00:00:02,120
但是其实你也可以用不同的feedback

474
00:00:02,120 --> 00:00:05,879
你可以让他直接只跟code来对话

475
00:00:05,879 --> 00:00:09,240
然后给他要构建相应的reward function来做

476
00:00:09,240 --> 00:00:11,560
那这一层其实自由度是非常之高的

477
00:00:11,560 --> 00:00:13,919
然后它能带来的效果也非常之

478
00:00:13,919 --> 00:00:18,039
就是为什么GPT3大家都觉得很这样子

479
00:00:18,039 --> 00:00:20,399
但是XGPT就觉得很惊艳

480
00:00:20,399 --> 00:00:23,399
除了它本身3到3.5可能有一些变化之外

481
00:00:23,399 --> 00:00:27,239
就是它在reinforcement with human feedback这一步

482
00:00:27,239 --> 00:00:28,960
和人的alignment做得太好了

483
00:00:00,000 --> 00:00:01,760
所以说所有人都懂了

484
00:00:01,760 --> 00:00:04,160
所有人突然懂了它是一个特别牛逼的东西

485
00:00:04,160 --> 00:00:08,160
那你这这是跟人类的偏好的alignment

486
00:00:08,160 --> 00:00:10,560
但是你也可以跟不同的知识

487
00:00:10,560 --> 00:00:12,439
不同的能力去进行alignment

488
00:00:12,439 --> 00:00:15,119
所以说这能解锁的可能性是非常之高的

489
00:00:15,119 --> 00:00:16,960
再往下是prompt对吧

490
00:00:16,960 --> 00:00:21,399
然后现在的prompt也是给了你一个非常死的方式

491
00:00:21,399 --> 00:00:23,120
但是这个prompt其实是在

492
00:00:23,120 --> 00:00:25,960
Chad的这个reinforcement learning的

493
00:00:25,960 --> 00:00:28,960
这个环节之下的一个方式之一

494
00:00:00,000 --> 00:00:05,919
我的预判是它会开放更多更定制化的prompt的方式

495
00:00:05,919 --> 00:00:08,119
比如说更长或者说全种可调

496
00:00:08,119 --> 00:00:13,000
或者说你可以给它的网上再多记一些东西等等

497
00:00:13,000 --> 00:00:14,839
或者说多开几层layer

498
00:00:14,839 --> 00:00:17,199
但是在网上它现在就能做到

499
00:00:17,199 --> 00:00:18,199
只是还没有做的东西

500
00:00:18,199 --> 00:00:20,760
就是把reinforcement learning这一步给开放出来

501
00:00:20,760 --> 00:00:23,480
我把从reinforcement learning到prompt

502
00:00:23,480 --> 00:00:25,640
这整个环节都叫funtune

503
00:00:25,640 --> 00:00:29,039
但是它和我们之前的funtune那种方式不一样

504
00:00:00,000 --> 00:00:02,319
因为就像那个罗伊说的

505
00:00:02,319 --> 00:00:03,799
他没有改变模型的权重

506
00:00:03,799 --> 00:00:04,480
对吧

507
00:00:04,480 --> 00:00:07,160
他没有改变楼型权重和之前我们的那个

508
00:00:07,160 --> 00:00:08,359
Machine Learning的Phantom

509
00:00:08,359 --> 00:00:10,359
其实在这上面是有很大的区别

510
00:00:10,359 --> 00:00:11,960
其实他在做这个

511
00:00:11,960 --> 00:00:13,240
我稍微查一下

512
00:00:13,240 --> 00:00:15,359
就是他在做这种就所谓

513
00:00:15,359 --> 00:00:17,600
Reinforcement Learning with Human Feedback的过程当中

514
00:00:17,600 --> 00:00:21,079
实际上也是改变了模型的参数的

515
00:00:21,559 --> 00:00:24,399
他是拿出来了Distill出来

516
00:00:24,399 --> 00:00:27,199
相当于就是把那个4里边的一部分拿出来了

517
00:00:27,199 --> 00:00:28,960
还是改变了4本身的权重

518
00:00:00,000 --> 00:00:01,639
他应该是改变了我的理解

519
00:00:01,639 --> 00:00:03,600
当然了没有人知道具体这个东西怎么做的

520
00:00:03,600 --> 00:00:05,040
但是我的理解就是说

521
00:00:05,040 --> 00:00:08,599
他首先先通过我们正常的语言模型的问题

522
00:00:08,599 --> 00:00:10,359
去训练出来一个普通的模型

523
00:00:10,359 --> 00:00:12,759
然后他用先把他

524
00:00:12,759 --> 00:00:15,759
首先他做了一个模型的clone

525
00:00:15,759 --> 00:00:20,120
然后这个模型专门去训练成他的一个reward模型

526
00:00:20,120 --> 00:00:23,440
这个reward模型就通过一些人工的数据

527
00:00:23,440 --> 00:00:24,399
去告诉他这个人

528
00:00:24,399 --> 00:00:25,920
这人告诉他好还是不好

529
00:00:25,920 --> 00:00:28,239
所以有一个clone就专门做这个reward模型

530
00:00:00,000 --> 00:00:04,200
然后他用这个reward模型去重新再用强化学习训练

531
00:00:04,200 --> 00:00:06,480
他之前的那个底层模型

532
00:00:06,480 --> 00:00:08,519
那在训练这个底层模型当中的时候

533
00:00:08,519 --> 00:00:10,359
这个底层模型实际上也是在变化的

534
00:00:11,320 --> 00:00:14,439
这个我觉得我们可以回头把instructGPT

535
00:00:14,439 --> 00:00:16,160
那个paper拿出来重新看一下

536
00:00:16,160 --> 00:00:19,559
我当时还因为张俊林在那个他的

537
00:00:19,559 --> 00:00:21,679
就是大语言模型技术经验那个里边

538
00:00:21,679 --> 00:00:24,719
有讲in-context learning和讲这方面的东西

539
00:00:24,719 --> 00:00:26,239
然后我看了他了以后

540
00:00:26,239 --> 00:00:29,879
我回去再看instructGPT和chaiGPT的两个论文是吧

541
00:00:00,000 --> 00:00:03,399
其实instruct cpt是蓝色的,chart cpt是绿色的

542
00:00:03,399 --> 00:00:04,879
就是他们在那个图里边

543
00:00:04,879 --> 00:00:06,320
然后流程是一样的

544
00:00:06,320 --> 00:00:07,280
就是你刚刚说的

545
00:00:07,280 --> 00:00:09,439
当然他们不光是做了一个reward model

546
00:00:09,439 --> 00:00:11,720
就是他们是先构造了一个

547
00:00:11,720 --> 00:00:15,359
用instruct和chart的方式去构造数据集

548
00:00:15,359 --> 00:00:18,800
然后去用大模型在这数据集下去perform

549
00:00:18,800 --> 00:00:22,679
然后在这个perform里边再用reward model去选择是

550
00:00:22,679 --> 00:00:25,600
就是他看的是哪个叫policy

551
00:00:25,600 --> 00:00:26,280
对吧

552
00:00:26,280 --> 00:00:28,280
我当时的理解

553
00:00:00,000 --> 00:00:03,520
就是这是一个distill或者说用in-context learning

554
00:00:03,520 --> 00:00:08,400
去选择性的激活大模型的不同的位置

555
00:00:08,400 --> 00:00:10,320
但是没有改变那个大模型本身

556
00:00:10,320 --> 00:00:13,199
就是说GPT-4它有这么多个参数是吧

557
00:00:13,199 --> 00:00:14,519
然后每个参数不同权重

558
00:00:14,519 --> 00:00:16,199
这个权重其实是没有改

559
00:00:16,199 --> 00:00:20,440
对 但是如果说你在in-context learning的这个部分

560
00:00:20,440 --> 00:00:22,839
其实in-context learning并没有learn任何的东西

561
00:00:22,839 --> 00:00:25,600
对 他只是说你提供了一些东西

562
00:00:25,600 --> 00:00:28,440
在额外的作为一个例子给他

563
00:00:00,000 --> 00:00:04,480
那个时候确实是指示说你可以理解成激活了模型的某一个部分

564
00:00:04,480 --> 00:00:08,560
但是我们在训练一个模型想通过强化学习的时候

565
00:00:08,560 --> 00:00:13,720
那我们需不需要找到的是一个更好的in-context examples呢

566
00:00:13,720 --> 00:00:17,719
还是我们要找到的是一个模型在不提供in-context example

567
00:00:17,719 --> 00:00:20,600
或者提供同样类似in-context example的时候

568
00:00:20,600 --> 00:00:21,839
得到一个更好的结果

569
00:00:21,839 --> 00:00:22,679
我觉得是后者

570
00:00:22,679 --> 00:00:23,480
我也觉得是后者

571
00:00:23,480 --> 00:00:24,440
如果是后者的话

572
00:00:24,440 --> 00:00:26,800
那就比模型本身就要发生一些变化

573
00:00:26,800 --> 00:00:27,199
对吗

574
00:00:00,000 --> 00:00:03,439
这就是Gin林里边说in context learning的魔力

575
00:00:03,439 --> 00:00:06,200
就是模型的权重并没有发生变化的情况下

576
00:00:06,200 --> 00:00:07,559
它却能改

577
00:00:07,559 --> 00:00:09,119
有一个具体的截图

578
00:00:09,119 --> 00:00:10,439
我回头可以找出来

579
00:00:10,439 --> 00:00:11,480
就是基本

580
00:00:11,480 --> 00:00:13,599
而且这里边还有一个很神奇的点

581
00:00:13,720 --> 00:00:16,320
就是在你in context learning的情况下

582
00:00:16,320 --> 00:00:18,839
你不需要给他提就是一个y

583
00:00:18,839 --> 00:00:21,480
你给他提供的例子是y等于这些x

584
00:00:21,480 --> 00:00:21,800
对吧

585
00:00:21,800 --> 00:00:22,719
y等于这些x

586
00:00:22,719 --> 00:00:25,519
然后你给他一个新的一堆x的项链

587
00:00:25,519 --> 00:00:26,760
然后他能得到一个新的y

588
00:00:26,760 --> 00:00:29,399
结果他发现你在in context learning给他提供的时候

589
00:00:00,000 --> 00:00:03,600
只要是一个X的集合和一个Y的集合对应起来就行了

590
00:00:03,600 --> 00:00:05,599
你不需要它的等号一一对应

591
00:00:05,599 --> 00:00:07,200
它都能学到东西

592
00:00:07,200 --> 00:00:08,560
所以就是

593
00:00:08,560 --> 00:00:09,320
不是学到东西

594
00:00:09,320 --> 00:00:10,759
只是说你提供了这个例子

595
00:00:10,759 --> 00:00:11,400
它就可以

596
00:00:11,400 --> 00:00:13,519
那可以像这样方式去表现

597
00:00:13,519 --> 00:00:14,000
对

598
00:00:14,000 --> 00:00:14,480
对

599
00:00:14,480 --> 00:00:15,519
就是它这个

600
00:00:15,519 --> 00:00:18,920
所以就是in context learning的魔力起码

601
00:00:18,920 --> 00:00:19,679
according to Jun

602
00:00:19,679 --> 00:00:20,199
一样就是

603
00:00:20,199 --> 00:00:21,000
就是

604
00:00:21,000 --> 00:00:22,879
看来其实应该连线他

605
00:00:22,879 --> 00:00:23,440
对

606
00:00:23,440 --> 00:00:24,559
就是我没有

607
00:00:24,559 --> 00:00:28,760
我我我我基于对他的信任和那个理解

608
00:00:00,000 --> 00:00:02,720
也就是说他不需要改变模型成就

609
00:00:02,720 --> 00:00:07,320
就可以去激发他举一反三的表现

610
00:00:07,320 --> 00:00:09,080
但是在你训练一个模型

611
00:00:09,080 --> 00:00:10,759
训练一个更好的模型

612
00:00:10,759 --> 00:00:13,400
去完成同样的income tax earning的时候

613
00:00:13,400 --> 00:00:16,239
你是不是应该改变这个模型的参数呢

614
00:00:16,239 --> 00:00:18,600
我不确定这是一个问号

615
00:00:18,600 --> 00:00:20,399
我换一个举个例子

616
00:00:20,399 --> 00:00:24,399
就是说我们现在可能会有一个LAMA模型

617
00:00:24,399 --> 00:00:25,000
对吗

618
00:00:25,000 --> 00:00:25,359
对

619
00:00:25,359 --> 00:00:27,960
如果我们想要通过一个强化学习

620
00:00:27,960 --> 00:00:29,679
去强化这个LAMA模型的话

621
00:00:00,000 --> 00:00:05,200
那么我们原来能够提供的这个例子是123

622
00:00:05,200 --> 00:00:08,599
我是不是强化了之后给了强化的模型

623
00:00:08,599 --> 00:00:09,759
同样的例子123

624
00:00:09,759 --> 00:00:11,199
它应该给我一个更好的结果

625
00:00:11,560 --> 00:00:12,080
是

626
00:00:12,359 --> 00:00:14,160
那这个模型本身变化了吗

627
00:00:18,719 --> 00:00:22,239
未必还是我还是想说

628
00:00:22,239 --> 00:00:24,879
你是把这个看成了一部对吧

629
00:00:24,879 --> 00:00:26,280
我给了它的例子

630
00:00:26,280 --> 00:00:29,480
然后我去让它调整了以后

631
00:00:00,000 --> 00:00:07,839
整个模型变得更好,但是我在这一步里面看到了两步,一个是底层的模型和它的这个reinforcement learning这一步,

632
00:00:07,839 --> 00:00:18,480
呃,就是所谓的in context learning的激活这一步,嗯,就是就是这个人可能还是同样一个人,我不是确定这里应该打比方,anyway还是说模型吧,

633
00:00:18,480 --> 00:00:27,920
就是他的模型权重仍然是这些,然后你的这个in context learning,你把它,就是其实就是你那天说的这个nonparametric,parametric两步,

634
00:00:00,000 --> 00:00:01,919
你把它看成一个系统的话

635
00:00:01,919 --> 00:00:03,520
整个系统是发生了变化

636
00:00:03,520 --> 00:00:05,719
但是你回到这个底层大模型的话

637
00:00:05,719 --> 00:00:07,799
底层大模型是不是全中一定发生变化

638
00:00:07,799 --> 00:00:08,480
我不确定

639
00:00:08,480 --> 00:00:11,960
因为有可能底层大模型的全中不发生变化的情况下

640
00:00:11,960 --> 00:00:15,480
你的in-context learning的能力是强的

641
00:00:15,480 --> 00:00:20,920
你去追逐你in-context learning这些例子回答的更好

642
00:00:20,920 --> 00:00:23,120
去overfit这些例子的情况下

643
00:00:23,120 --> 00:00:25,120
反而它的模型的能力

644
00:00:25,120 --> 00:00:27,559
in-context learning的能力有可能会下降

645
00:00:00,000 --> 00:00:03,560
这个我们可以展开到时候以后我们可以接着聊

646
00:00:03,560 --> 00:00:04,280
对

647
00:00:04,280 --> 00:00:09,480
所以其实现在业界大家并不知道在prechain之后还有什么办

648
00:00:09,480 --> 00:00:12,640
prechain之后做的那些事情有没有可能改变这个

649
00:00:12,640 --> 00:00:14,160
foundation model的这个模型选种

650
00:00:14,160 --> 00:00:15,160
我们并不知道

651
00:00:15,160 --> 00:00:17,399
在今天之前我的理解都是没有改变

652
00:00:17,760 --> 00:00:19,920
今天我会打一个问号

653
00:00:19,920 --> 00:00:21,320
也希望观众如果

654
00:00:21,320 --> 00:00:22,640
对我们到时候可以再讨论

655
00:00:22,640 --> 00:00:23,239
对

656
00:00:23,239 --> 00:00:25,199
这个可能是我理解上的一些偏差

657
00:00:25,199 --> 00:00:25,640
我觉得

658
00:00:25,640 --> 00:00:26,039
对

659
00:00:26,039 --> 00:00:27,719
就是拆开再继续

660
00:00:27,719 --> 00:00:28,359
拆开讨论

661
00:00:00,000 --> 00:00:03,799
我们可以在群里也可以请到时候观众们来确定知道的告诉我们

662
00:00:03,799 --> 00:00:06,040
但是这个问题再复述一下的话

663
00:00:06,040 --> 00:00:10,119
就是pre-trained大模型在in-context learning之后

664
00:00:10,640 --> 00:00:14,400
第一,in-context learning有没有改变权重

665
00:00:14,400 --> 00:00:15,439
大概率是没有

666
00:00:15,439 --> 00:00:18,199
但是第二,为了让它的performance更好

667
00:00:18,199 --> 00:00:21,519
它需不需要或者说应不应该去改变模型的权重

668
00:00:21,519 --> 00:00:25,199
在强化学习的过程当中有没有改变模型的权重

669
00:00:25,199 --> 00:00:27,039
我的观点是是你的观点是

670
00:00:00,000 --> 00:00:03,759
我的观点是 distill 是better distillation

671
00:00:03,759 --> 00:00:05,719
然后你的观点是要改变模型的参数

672
00:00:05,719 --> 00:00:08,039
我觉得它的模型的参数会发生一些变化

673
00:00:08,039 --> 00:00:08,320
嗯

674
00:00:08,320 --> 00:00:08,839
嗯

675
00:00:08,839 --> 00:00:11,960
其实就是因为前面这个柯代表做了一个这个

676
00:00:11,960 --> 00:00:14,560
clarification 就是你对于这个fine tune的这个定义

677
00:00:14,560 --> 00:00:16,239
可能我就可能是因为

678
00:00:16,239 --> 00:00:18,359
因为大家现在如果大家讲fine tune的时候

679
00:00:18,359 --> 00:00:21,160
可能更多讲的是SFT的那个环节

680
00:00:21,160 --> 00:00:24,440
但是你这样就quick clarify这个是不是因为在

681
00:00:24,440 --> 00:00:26,239
传统的machinery里边

682
00:00:26,239 --> 00:00:28,559
当你讲到fine tune的时候就默认你其实改变了

683
00:00:28,559 --> 00:00:29,800
这个模型的这个权重

684
00:00:00,000 --> 00:00:02,680
你觉得如果我们用Fenton这个词就可能就默认了

685
00:00:02,680 --> 00:00:06,879
我们在,呃,其实SFT这个这个这个过程是不改变这个

686
00:00:06,879 --> 00:00:10,199
对,这个就是我在写另一个文章的第二个问题时候

687
00:00:10,199 --> 00:00:14,599
我反复跟人沟通,然后发现跟所有的传统的机器学习的人

688
00:00:14,599 --> 00:00:17,399
但是没有接触过,或者说没有对大模型了解非常深

689
00:00:17,399 --> 00:00:19,559
或者非常细的人的时候,总是会出现的一个

690
00:00:19,879 --> 00:00:23,839
呃,争执,就是Fenton,尤其是对这个词的definition的区别

691
00:00:23,839 --> 00:00:28,239
他们心中一想到Fenton,第一想的就是我去改变模型的权重

692
00:00:00,000 --> 00:00:02,319
但是我说不对啊,你看这有incount task learning,

693
00:00:02,319 --> 00:00:05,559
它不需要改变模型权这种情况下也能给你输出更好的结果,

694
00:00:05,559 --> 00:00:09,519
然后我就会发现,哦,是大家对这个词的定义已经产生了很大的变化。

695
00:00:09,519 --> 00:00:13,400
对,其实刚才有一点,我觉得可以我想来继续问一下,

696
00:00:13,400 --> 00:00:16,120
因为其实我们刚才提到这个这个话题,

697
00:00:16,120 --> 00:00:25,160
其实是因为你聊到说,如果等到OpenAI开放了这个RHF的这一些更多的这个权限以后,

698
00:00:25,160 --> 00:00:27,160
我们有很多这个可以enable的东西。

699
00:00:00,000 --> 00:00:06,480
那也许有些人会就是那我想可能有一些朋友会想说那到底

700
00:00:06,480 --> 00:00:09,320
开放这个有什么是我们现在不能做的

701
00:00:09,320 --> 00:00:12,519
而你觉得说等下开放这个全场我们就可以做的

702
00:00:12,519 --> 00:00:14,560
因为现在其实我们是可以做一些

703
00:00:14,560 --> 00:00:18,559
呃一定程度上的这个呃sft对吧

704
00:00:18,559 --> 00:00:22,719
然后还有包括这个呃拉玛的话其实也可以做一些嘛

705
00:00:22,719 --> 00:00:25,239
就是你觉得开放后有哪些都可以做的

706
00:00:25,239 --> 00:00:26,000
我先直接回答

707
00:00:26,000 --> 00:00:27,519
然后再prompt 卢毅来回答啊

708
00:00:27,519 --> 00:00:28,920
就是我我先我先不用

709
00:00:00,000 --> 00:00:03,839
然后这个这个这个可以我帮他就是我们当时聊到这个话题

710
00:00:03,839 --> 00:00:06,719
就是因为他现在在做的这个业务

711
00:00:06,719 --> 00:00:10,480
就是去做一个更垂直的模型

712
00:00:10,480 --> 00:00:13,599
就是他们两条一步一个是应用市场上

713
00:00:13,599 --> 00:00:14,880
一个是自己开发

714
00:00:14,880 --> 00:00:18,079
然后我们当时说如果JPT

715
00:00:18,079 --> 00:00:20,239
如果OpenAI开放了这个layer的话

716
00:00:20,239 --> 00:00:22,640
会不会所有的垂直模型都没有意义了

717
00:00:22,640 --> 00:00:24,440
然后也很有可能是这样子的

718
00:00:24,440 --> 00:00:26,760
因为它模型performance好太多

719
00:00:26,760 --> 00:00:29,239
它能给你开放这样一个定制的环节的话

720
00:00:00,000 --> 00:00:02,200
完全就把你们所有人全都淹死了

721
00:00:02,200 --> 00:00:05,639
然后我举一个例子就是它开放能达到什么程度

722
00:00:05,639 --> 00:00:07,919
我举的例子现实中不会存在

723
00:00:07,919 --> 00:00:10,599
但是它可以帮助大家思考这件事

724
00:00:10,599 --> 00:00:14,119
就是我们现在人是无法学会狗说话的

725
00:00:14,119 --> 00:00:17,719
假设狗真的能给GPT提供一个reward function

726
00:00:17,719 --> 00:00:21,320
那你让GPT去学狗叫

727
00:00:21,320 --> 00:00:23,440
GPT很有可能就会学会狗的语言

728
00:00:23,960 --> 00:00:24,519
不存在

729
00:00:24,519 --> 00:00:26,079
但是或者说就是代码

730
00:00:26,079 --> 00:00:27,640
或者说是no

731
00:00:27,640 --> 00:00:29,000
是一个更实际的例子

732
00:00:00,000 --> 00:00:02,799
就比如说我们的医生是很难训练的

733
00:00:02,799 --> 00:00:06,320
因为你很多知识不是是否问题

734
00:00:06,320 --> 00:00:09,080
它不是一个rule based这个function

735
00:00:09,080 --> 00:00:10,400
你看到了一些东西

736
00:00:10,400 --> 00:00:12,320
你怎么会得到这样的诊断

737
00:00:12,320 --> 00:00:14,519
其实不同的医生他给你的

738
00:00:14,519 --> 00:00:16,800
根据他的经验直觉什么的都不一样

739
00:00:17,160 --> 00:00:20,000
但是假设我们有一个高质量的

740
00:00:20,000 --> 00:00:22,120
这样的一个数据集

741
00:00:22,120 --> 00:00:24,480
就是说病例所有医生能看到的

742
00:00:24,480 --> 00:00:25,640
收集到的所有信息

743
00:00:25,640 --> 00:00:27,199
和他做出来的判断

744
00:00:27,199 --> 00:00:29,440
和也许他写下来了一些判断的依据

745
00:00:00,000 --> 00:00:01,000
也許沒有寫

746
00:00:01,199 --> 00:00:03,000
我們也許就可以拿這樣一個東西

747
00:00:03,000 --> 00:00:06,799
去構建一個這個reward的數據集

748
00:00:06,799 --> 00:00:11,000
然後讓GPT學會怎麼樣子去進行專家級的

749
00:00:11,000 --> 00:00:13,000
就真正專家級的醫療診斷

750
00:00:13,000 --> 00:00:16,920
對 這個知識不是在現有的那個所謂的

751
00:00:16,920 --> 00:00:19,000
案例庫裡面直接存在的

752
00:00:19,000 --> 00:00:20,800
GPT現有的能力

753
00:00:20,800 --> 00:00:24,399
也許能去模仿一些人在這個案例庫裡的對話

754
00:00:24,399 --> 00:00:28,000
但是他得不到那個醫生下診斷的那個知識

755
00:00:00,000 --> 00:00:03,960
可是你如果有了这个reward数据集的话

756
00:00:03,960 --> 00:00:07,679
GPT有可能就能得到一个真正医生的那个

757
00:00:07,679 --> 00:00:08,560
呃 知识

758
00:00:08,560 --> 00:00:10,039
嗯 明白你的意思

759
00:00:10,039 --> 00:00:12,880
我其实我就想先退一步说这个问题

760
00:00:12,880 --> 00:00:15,279
就是我们首先我们想说

761
00:00:15,279 --> 00:00:18,839
OpenAI给开放一个可以定制化或者

762
00:00:18,839 --> 00:00:20,879
个性化这个模型的这个服务

763
00:00:20,879 --> 00:00:22,640
到底像我们要解决什么样的问题

764
00:00:22,640 --> 00:00:25,079
我们可能有几类

765
00:00:25,079 --> 00:00:28,960
几个东西是我们需要去做个性化的

766
00:00:00,000 --> 00:00:02,560
一个是它就是知识

767
00:00:02,560 --> 00:00:05,280
你GPT学习的知识就是

768
00:00:05,280 --> 00:00:07,200
比如说我们简单的说就是Wikipedia

769
00:00:07,200 --> 00:00:08,800
或者说我们互联网上的文字

770
00:00:08,800 --> 00:00:10,279
你上面获得的这些知识

771
00:00:10,279 --> 00:00:13,599
那有一些实际上是专有的一些知识

772
00:00:13,599 --> 00:00:16,600
比如说是就像你说的医疗方面的知识

773
00:00:17,480 --> 00:00:19,199
或者是法律方面的知识

774
00:00:19,199 --> 00:00:22,839
如何让这个模型去专门去复习也好

775
00:00:22,839 --> 00:00:24,519
或者是专门去强调这方面的知识

776
00:00:24,519 --> 00:00:26,719
就把这些知识放到它的参数当中

777
00:00:00,000 --> 00:00:03,680
那可能是一个我们想去做个性化的部分

778
00:00:03,680 --> 00:00:05,240
这另外一个其实呢

779
00:00:05,240 --> 00:00:08,199
更多的是你已经拥有了这个知识

780
00:00:08,199 --> 00:00:10,080
拥有了这种推理能力

781
00:00:10,080 --> 00:00:14,359
那如何我去用一种方式去把你的这个能力

782
00:00:14,359 --> 00:00:16,320
应用在我的这种任务上

783
00:00:16,320 --> 00:00:19,679
也是一种个性化的思路

784
00:00:19,679 --> 00:00:22,640
那么这其实往往就是我们可以说一下

785
00:00:22,640 --> 00:00:23,480
传统来讲

786
00:00:23,480 --> 00:00:24,920
如果要去达到这两个目标

787
00:00:24,920 --> 00:00:27,679
我有一个比如说是白盒模型

788
00:00:00,000 --> 00:00:03,240
我们如果是就像GPT现在我们可以认为是

789
00:00:03,240 --> 00:00:04,919
对于我们个人来讲我们是黑盒模型

790
00:00:05,360 --> 00:00:07,679
那如果假如说我们像Lemma或者说对OpenAI

791
00:00:07,679 --> 00:00:09,359
他们自己来讲这就是一个白盒模型

792
00:00:09,359 --> 00:00:11,359
对白盒模型想去完成这个目标

793
00:00:11,359 --> 00:00:13,480
可能需要做的任务稍微有一些不一

794
00:00:13,480 --> 00:00:15,480
就是方法稍微有一点不同

795
00:00:16,039 --> 00:00:18,559
这也是我经常收到的一些问题

796
00:00:18,679 --> 00:00:22,640
就是如何去把一个大语言模型

797
00:00:22,640 --> 00:00:24,440
应用在一个特定的领域当中

798
00:00:25,480 --> 00:00:26,879
那其实我往往就想说

799
00:00:26,879 --> 00:00:28,440
如果你要去学到知识

800
00:00:00,000 --> 00:00:03,200
其实现在学校知识最好的方法就是类似

801
00:00:03,200 --> 00:00:05,360
language modeling task一样的方法

802
00:00:05,360 --> 00:00:08,400
或者是instruction finding

803
00:00:08,400 --> 00:00:09,599
所谓instruction finding

804
00:00:09,599 --> 00:00:11,800
就比如说像Flynn他们的做法

805
00:00:11,800 --> 00:00:14,199
就是我有这么一个问题或者是有一个fixed problem

806
00:00:14,199 --> 00:00:15,279
那最后出来一个结果

807
00:00:15,279 --> 00:00:17,120
这种方式也可以学到知识

808
00:00:17,120 --> 00:00:20,320
那么我给了一 textbook的话

809
00:00:20,320 --> 00:00:21,719
也是可以学到知识的

810
00:00:21,719 --> 00:00:24,160
我自己之前自己玩的时候也去做这种东西

811
00:00:24,160 --> 00:00:26,559
就是说你给他这个textbook做这种

812
00:00:26,559 --> 00:00:28,600
causal language modeling task

813
00:00:00,000 --> 00:00:02,480
也就是next token production这种task

814
00:00:02,480 --> 00:00:03,640
它就确实会

815
00:00:03,640 --> 00:00:07,160
等等你说你自己玩和你刚做这个是

816
00:00:07,160 --> 00:00:08,960
就相当于是我自己的一些探索

817
00:00:08,960 --> 00:00:10,439
是在GPT之后还是之前

818
00:00:10,439 --> 00:00:12,839
在GPT之后做这些探索

819
00:00:12,839 --> 00:00:15,640
然后用的一些也是开源的这种

820
00:00:15,640 --> 00:00:18,800
对 decoder这样的模型去做这种事情

821
00:00:18,800 --> 00:00:19,920
然后我就会去

822
00:00:19,920 --> 00:00:23,679
比如说我对房地产这方面的东西感兴趣

823
00:00:23,679 --> 00:00:27,760
然后我就把他们的教科书教材考试的题

824
00:00:00,000 --> 00:00:03,120
全都变成那个语料

825
00:00:03,120 --> 00:00:04,280
收集下来以后

826
00:00:04,280 --> 00:00:05,679
我去训练这个模型

827
00:00:05,679 --> 00:00:07,160
原来我同样问这个问题

828
00:00:07,160 --> 00:00:08,199
他会回答一个答案

829
00:00:08,199 --> 00:00:09,599
也许他有一点合理

830
00:00:09,599 --> 00:00:12,080
但是我把这个教科书给他了以后

831
00:00:12,080 --> 00:00:13,519
同样这个问题他回答答案

832
00:00:13,519 --> 00:00:16,399
他就会更像教科书里面讲的那样的答案

833
00:00:16,399 --> 00:00:16,839
对

834
00:00:16,839 --> 00:00:19,079
他就是这就是相当于学到了一些知识

835
00:00:19,079 --> 00:00:21,960
那这种其实就是我觉得像Cosmos

836
00:00:21,960 --> 00:00:23,640
Languages 会更合适的东西

837
00:00:23,640 --> 00:00:25,600
那还有一种

838
00:00:25,600 --> 00:00:27,280
我这加个pin回头再讨论

839
00:00:27,280 --> 00:00:27,920
你先继续

840
00:00:27,920 --> 00:00:28,399
好的

841
00:00:00,000 --> 00:00:03,279
然后还有就是如何去format最后的结果

842
00:00:03,279 --> 00:00:06,519
也就是我有一个也是非常overloaded term

843
00:00:06,519 --> 00:00:07,599
就是alignment

844
00:00:07,599 --> 00:00:10,119
如何把这个GPT的结果

845
00:00:10,119 --> 00:00:12,839
它也许GPT已经拥有这样的能力

846
00:00:12,839 --> 00:00:15,240
但是我希望你去完成的一件事情

847
00:00:15,240 --> 00:00:18,800
是按照我的这个任务的format去完成

848
00:00:18,800 --> 00:00:20,399
或者说这些步骤去完成

849
00:00:20,399 --> 00:00:23,320
那这个时候如果我能够提供一些

850
00:00:23,320 --> 00:00:26,600
比如说训练数据的话

851
00:00:26,600 --> 00:00:29,239
那其实可以达到的就是两个效果

852
00:00:00,000 --> 00:00:04,200
一个就是我不需要再通过in context learning的方式去达到这个效果

853
00:00:04,200 --> 00:00:06,360
因为in context learning我提供了一些例子

854
00:00:06,360 --> 00:00:08,359
那它就可能prompt的更长

855
00:00:08,359 --> 00:00:09,800
然后更慢

856
00:00:09,800 --> 00:00:10,720
更加的贵

857
00:00:10,720 --> 00:00:13,119
那如果在之前训练的过程当中

858
00:00:13,119 --> 00:00:14,480
我就已经提供了这些例子

859
00:00:14,480 --> 00:00:15,839
我可以提供更多的例子

860
00:00:15,839 --> 00:00:20,199
我在去做inference或者说去做生成的时候

861
00:00:20,199 --> 00:00:23,320
我就不需要提供额外的例子去做这件事情

862
00:00:23,320 --> 00:00:25,039
这也是它的一个好处吧

863
00:00:25,039 --> 00:00:27,320
就是我觉得也不能

864
00:00:00,000 --> 00:00:02,960
这也是就我们回到那个customization的好处

865
00:00:02,960 --> 00:00:05,559
那我 我们这就又说到

866
00:00:05,559 --> 00:00:07,839
呃我们不得不说就是强化学习

867
00:00:07,839 --> 00:00:09,519
它到底开放的是什么东西

868
00:00:09,519 --> 00:00:13,119
我个人的观点可能和你稍微有一点点不一样

869
00:00:13,119 --> 00:00:15,640
就是我自己觉得

870
00:00:15,640 --> 00:00:18,160
你去开放你要完成的

871
00:00:18,160 --> 00:00:19,440
如果是学习知识的话

872
00:00:19,440 --> 00:00:20,480
那是一个思路

873
00:00:20,480 --> 00:00:23,120
如果开放的是format alignment的话

874
00:00:23,120 --> 00:00:24,480
是一种不同的开放

875
00:00:24,480 --> 00:00:26,000
如果你要开放强化学习

876
00:00:26,000 --> 00:00:29,760
它是一种第三种我觉得不一样的类型的任务

877
00:00:00,000 --> 00:00:02,439
什么时候我觉得用强化学习

878
00:00:02,439 --> 00:00:04,799
我自己之前就在想有几个可能性

879
00:00:04,799 --> 00:00:08,000
一个是说你需要完成一系列

880
00:00:08,000 --> 00:00:10,320
就a sequence of decision

881
00:00:10,320 --> 00:00:11,839
而不是一个decision

882
00:00:11,839 --> 00:00:13,279
然后直接出一个结果的

883
00:00:14,759 --> 00:00:17,039
比如说我们下棋更像是一个

884
00:00:17,280 --> 00:00:22,039
一系列决定最后获得的一个结果

885
00:00:22,760 --> 00:00:25,239
这种东西我觉得更适合于强化学习

886
00:00:25,359 --> 00:00:28,519
如果只是说我说一个东西回一个东西

887
00:00:00,000 --> 00:00:02,120
然后这个任务就结束了

888
00:00:02,120 --> 00:00:04,719
永远都是这种就是一回合的

889
00:00:04,719 --> 00:00:09,279
其实并不见得都是适合强化学习的

890
00:00:09,279 --> 00:00:10,359
不见得是强化学习

891
00:00:10,359 --> 00:00:15,119
也许就是我们所谓说的这种传统的方式去微调模型

892
00:00:15,119 --> 00:00:17,839
或者是更加方便直接的一个方法

893
00:00:17,839 --> 00:00:19,519
因为你会直接告诉他的结果

894
00:00:19,519 --> 00:00:21,600
那如果是一个sequence of decision

895
00:00:21,600 --> 00:00:23,199
那你其实就比如说conversation

896
00:00:23,199 --> 00:00:24,960
就是一个sequence of decision

897
00:00:24,960 --> 00:00:26,239
你回答一个东西

898
00:00:26,239 --> 00:00:26,960
你说了一个东西

899
00:00:26,960 --> 00:00:28,280
你他别人又说了一个

900
00:00:00,000 --> 00:00:02,799
这个时候他们别人再根据你的回答

901
00:00:02,799 --> 00:00:04,320
可能会说完全不一样的话

902
00:00:04,320 --> 00:00:04,719
对吗

903
00:00:04,719 --> 00:00:07,360
你完全猜不出别人可能会怎么回答

904
00:00:07,360 --> 00:00:08,800
在你说了这句话之前

905
00:00:09,359 --> 00:00:15,679
像这种一系列互相相depend在一起的问题

906
00:00:15,679 --> 00:00:17,839
就更适合于强化学习这个问题

