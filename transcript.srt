1
00:00:00,000 --> 00:00:04,480
其实在GPT的大模型里面的知识是非常大的

2
00:00:04,480 --> 00:00:07,799
我们在现在只不过是activated其中一点点

3
00:00:07,799 --> 00:00:11,880
然后是通过这种instruct或者说是chat的方式

4
00:00:11,880 --> 00:00:16,199
去让这个模型的输出变得更palatable to human

5
00:00:16,199 --> 00:00:17,679
就是我们更容易理解

6
00:00:17,679 --> 00:00:21,079
它的思维模式可能远远在我们所能理解之上

7
00:00:21,079 --> 00:00:23,239
其实有一个问题就是你的prompt

8
00:00:23,239 --> 00:00:26,359
你要去让chat GPT完成各种各样的任务

9
00:00:26,359 --> 00:00:30,399
那到底是一个工程技巧还是一个PM技巧

10
00:00:30,399 --> 00:00:33,119
就是你是要教给拆GPT怎么用呢

11
00:00:33,119 --> 00:00:35,799
还是你是要告诉拆GPT做什么

12
00:00:35,799 --> 00:00:38,359
再说一些这个过往的历史

13
00:00:38,520 --> 00:00:41,200
就是拆GPT这件事不是一个真的prediction

14
00:00:41,200 --> 00:00:44,759
它只不过是历史很多颠覆科技的重复

15
00:00:44,759 --> 00:00:45,600
Hello大家好之前我不是写过一个文章吗关是历史很多颠覆科技的重复哈喽大家好

16
00:00:45,600 --> 00:00:46,920
之前我不是写过一个文章吗

17
00:00:46,920 --> 00:00:48,600
关于TGPT的5个问题

18
00:00:48,960 --> 00:00:50,719
我发现我其实写了一个slides

19
00:00:50,719 --> 00:00:51,920
然后在去年3月份

20
00:00:52,119 --> 00:00:55,520
今天我就会结合这个slides里的内容

21
00:00:55,759 --> 00:00:58,280
然后和我们这一年的观察

22
00:00:58,479 --> 00:01:00,320
给大家重温一下这5个问题

23
00:01:00,320 --> 00:01:02,320
然后带来一些新的见解

24
00:01:02,600 --> 00:01:04,200
像大家看到的

25
00:01:04,200 --> 00:01:06,000
我现在其实是在一个狗公园旁边

26
00:01:06,000 --> 00:01:10,000
然后我就坐在狗公园旁边

27
00:01:10,000 --> 00:01:11,000
去给大家看

28
00:01:11,000 --> 00:01:13,000
我们可以先看一下它的

29
00:01:14,000 --> 00:01:16,000
一个immersive mode

30
00:01:16,000 --> 00:01:17,000
在狗公园里边

31
00:01:17,000 --> 00:01:20,000
我瞬间就来到了

32
00:01:21,000 --> 00:01:23,000
乔布斯发布会的现场

33
00:01:23,000 --> 00:01:29,319
然后我在这里就可以去给大家讲我的PPT

34
00:01:29,319 --> 00:01:30,319
Lights Up

35
00:01:30,319 --> 00:01:32,120
Oh

36
00:01:32,120 --> 00:01:33,120
Lights Down

37
00:01:39,799 --> 00:01:40,799
第一个问题

38
00:01:40,799 --> 00:01:42,159
第一个问题

39
00:01:42,159 --> 00:01:44,400
好像这样还是不太好玩

40
00:01:44,400 --> 00:01:47,120
我们还是回到我们原來的模式吧

41
00:01:53,560 --> 00:01:57,000
這樣的話就給大家一邊看我們做的這個

42
00:01:57,000 --> 00:02:00,599
一邊看狗狗在那裡玩

43
00:02:03,319 --> 00:02:04,120
是哪五個問題

44
00:02:04,120 --> 00:02:05,959
然後為什麼這五個问题都很重要呢

45
00:02:06,319 --> 00:02:07,719
第一个就是

46
00:02:07,719 --> 00:02:09,199
拆GPD到底是什么

47
00:02:09,199 --> 00:02:11,240
它是一个范式突破吗

48
00:02:11,560 --> 00:02:14,599
我们在这个Vision Pro出来的时候

49
00:02:14,599 --> 00:02:15,879
在iPhone出来的时候

50
00:02:15,879 --> 00:02:17,400
在一个新科技出来的时候

51
00:02:17,520 --> 00:02:20,159
这都是一个我们最需要问的问题

52
00:02:20,360 --> 00:02:21,759
为什么这个问题重要

53
00:02:21,759 --> 00:02:23,120
因为它只有范式突破

54
00:02:23,120 --> 00:02:25,439
才能带来一个十倍百倍的机会

55
00:02:25,439 --> 00:02:30,400
如果你在原有的基础之上做一个小范围的突破

56
00:02:30,400 --> 00:02:31,840
而不是一个范式突破的话

57
00:02:31,840 --> 00:02:34,319
它其实能带来的机会是有限的

58
00:02:34,319 --> 00:02:37,919
因为范式突破才可以让你做到之前完全不可能做到的事情

59
00:02:37,919 --> 00:02:40,560
而不是在原有的基础之上降本增效

60
00:02:40,560 --> 00:02:46,000
第二个就是我们怎么样子去了解大圃远模型的impact到底是什么

61
00:02:46,000 --> 00:02:49,000
第三个就是我们如何去制造一个大跃远模型

62
00:02:49,000 --> 00:02:53,000
第四个问题是我们如何使用大跃远模型

63
00:02:53,000 --> 00:02:57,000
第五个是我们人类和大跃远模型有什么不一样

64
00:02:57,000 --> 00:02:59,000
这四个问题都是顺接的

65
00:02:59,000 --> 00:03:02,000
就是我们了解了它到底能产生什么样的影响

66
00:03:02,000 --> 00:03:05,400
尤其是站在这个技术刚刚出现的时候

67
00:03:05,400 --> 00:03:08,319
我们要去想象它5年后10年后的影响

68
00:03:08,319 --> 00:03:11,000
这样的话我们才可以知道

69
00:03:11,000 --> 00:03:14,199
它给我们带来的机会在哪里

70
00:03:14,199 --> 00:03:17,599
带来有影响之后我们想抓到这个机会

71
00:03:17,599 --> 00:03:19,680
接下来就是说它的难度是如何

72
00:03:19,680 --> 00:03:22,759
这样一方面我们可以了解它的发展速度

73
00:03:22,759 --> 00:03:25,360
另外一方面也能了解我们在这里边应该做什么

74
00:03:25,360 --> 00:03:29,879
现在说假设我们不是作为大模型的生产方的话

75
00:03:29,879 --> 00:03:33,719
我们应该怎么去使用它才能更好的让它去改变我们的生活

76
00:03:33,719 --> 00:03:35,039
抓住这里边的机会

77
00:03:35,039 --> 00:03:37,759
最后其实是一个究极问题了

78
00:03:37,759 --> 00:03:43,159
就是我们站到技术已经发展到完全非常成熟的时候

79
00:03:43,159 --> 00:03:45,400
人类和技术还有什么不一样

80
00:03:45,400 --> 00:03:47,599
就是说我们还应该去做什么

81
00:03:47,599 --> 00:03:49,800
才是不会被这个技术所改变的

82
00:03:49,800 --> 00:03:52,500
就是为什么这五个问题是这样的顺序

83
00:03:52,500 --> 00:03:54,400
和为什么这五个问题这么重要

84
00:03:54,400 --> 00:03:56,199
互联网上很多东西都是noise

85
00:03:56,199 --> 00:03:57,300
那在这个noise之外

86
00:03:57,300 --> 00:03:59,400
到底还有什么东西是重要的

87
00:04:02,800 --> 00:04:04,900
有一些技术细节是必须要知道的

88
00:04:04,900 --> 00:04:05,000
我们这就最进行一些最最最简单的技术细节是必须要知道的

89
00:04:05,000 --> 00:04:08,639
我们这就最进行一些最最最简单的技术普及

90
00:04:08,639 --> 00:04:10,280
最简单的就是

91
00:04:10,280 --> 00:04:13,199
Chai GPT它其实是一个几个关键词

92
00:04:13,199 --> 00:04:14,560
我把它放大一点

93
00:04:14,560 --> 00:04:16,639
这样还可以再远一点吗

94
00:04:16,639 --> 00:04:18,399
再来一

95
00:04:18,399 --> 00:04:21,480
好像也就这么远了

96
00:04:21,480 --> 00:04:25,279
Chai GPT它是一个叫做

97
00:04:25,279 --> 00:04:28,079
Generative Autoregressive Large Language Models

98
00:04:28,079 --> 00:04:29,120
它是一个代元模型

99
00:04:29,120 --> 00:04:32,920
然后这个代元模型它的本质是一个生成式的

100
00:04:32,920 --> 00:04:36,319
然后它是一个Autoregressive就是自回归式的

101
00:04:36,319 --> 00:04:38,800
那这个具体怎么理解

102
00:04:38,800 --> 00:04:44,759
就是它是这个是一个就是Stefan Wolfram的一个图

103
00:04:44,759 --> 00:04:45,839
我觉得他讲的很好

104
00:04:45,839 --> 00:04:48,079
它的底层的模型GPT

105
00:04:48,079 --> 00:04:51,920
它要做的事情就是你去generate the next world

106
00:04:51,920 --> 00:04:54,279
这有一个好处就是

107
00:04:54,279 --> 00:04:57,759
它有人类有海量的文本去训练这个模型

108
00:04:57,759 --> 00:04:59,920
就是它在想方设法生成下一个次

109
00:04:59,920 --> 00:05:02,439
然后你这样自然而然就可以把人类已知的文本

110
00:05:02,439 --> 00:05:04,040
和它去进行一个匹配

111
00:05:04,040 --> 00:05:05,439
看它生成了对不对

112
00:05:05,439 --> 00:05:08,079
这样的话你就有海量的标签好的数据

113
00:05:08,079 --> 00:05:10,519
去帮助你这个模型去学习

114
00:05:10,519 --> 00:05:13,759
之后就是他们非常重要的

115
00:05:13,759 --> 00:05:15,439
scaling law的observation

116
00:05:15,439 --> 00:05:16,839
我在这稍微再多解释

117
00:05:16,839 --> 00:05:18,160
就在这里就多解释一下

118
00:05:19,279 --> 00:05:22,120
为什么generative这个任务如此之重要

119
00:05:22,120 --> 00:05:23,120
有两个点

120
00:05:23,120 --> 00:05:25,000
第一个就是过去的machinery model它都不general它只能做一个非常简单的任务如此之重要有两个点第一个就是过去的Models

121
00:05:25,000 --> 00:05:26,000
它都不general

122
00:05:26,000 --> 00:05:28,000
它只能做一个非常简单的任务

123
00:05:28,000 --> 00:05:30,000
那它做一个简单的任务就是

124
00:05:30,000 --> 00:05:31,000
你告诉它一个目标

125
00:05:31,000 --> 00:05:33,000
然后你能告诉它做得好还是不好

126
00:05:33,000 --> 00:05:35,000
那它就可以去学习

127
00:05:35,000 --> 00:05:36,000
在这方面可以做得很好

128
00:05:36,000 --> 00:05:40,000
但是问题是你所有的告诉它做得好还是不好的

129
00:05:40,000 --> 00:05:41,000
这些数据都必须要标注

130
00:05:41,000 --> 00:05:43,000
都是所谓的labeled

131
00:05:43,000 --> 00:05:46,079
包括你的就是unsupervised learning其实你在一定程度上也必须要标注都是所谓的labeled包括你的unsupervised learning

132
00:05:46,079 --> 00:05:49,680
其实你在一定程度上也必须要告诉这个模型的做得好还是不好

133
00:05:49,680 --> 00:05:52,879
这样的数据其实是有限的和人需要去生成的

134
00:05:52,879 --> 00:05:55,920
也就是说你不能让模型去举一反三

135
00:05:55,920 --> 00:06:00,240
你只能让模型去做一个已经限定好的任务下去进行优化

136
00:06:00,240 --> 00:06:03,680
可是generative这个事情就变得非常的

137
00:06:03,680 --> 00:06:07,680
在文本上的generation就变得非常的普适

138
00:06:07,680 --> 00:06:09,560
因为人类有海量的文本

139
00:06:09,560 --> 00:06:11,959
那你只要告诉你就什么都不用想

140
00:06:11,959 --> 00:06:13,279
你甚至不用去定义这个人物

141
00:06:13,279 --> 00:06:15,360
你只要摆这个海量的文本

142
00:06:15,360 --> 00:06:17,000
只要你的质量还是可以的

143
00:06:17,000 --> 00:06:19,519
所以说你的这个数据量一下就可以多了很多

144
00:06:19,519 --> 00:06:22,480
那接下来你需要去做的就是去想方设法

145
00:06:22,480 --> 00:06:23,600
来把这个模型变大

146
00:06:23,600 --> 00:06:25,279
让他可以多学到这里面的知识

147
00:06:25,279 --> 00:06:27,560
和多给模型为高质量的文本

148
00:06:27,560 --> 00:06:29,160
接下来就是一个leap of faith

149
00:06:29,160 --> 00:06:32,600
就是你会相信这么一个大模型能产出好的结果

150
00:06:32,600 --> 00:06:34,279
这个leap of faith非常的重要

151
00:06:34,279 --> 00:06:37,439
但是OpenAI的人很明显是有这样的一个信仰的

152
00:06:37,439 --> 00:06:38,519
他们也做到了

153
00:06:39,959 --> 00:06:41,800
这些技术细节我们就不多聊了

154
00:06:42,000 --> 00:06:46,000
就是接下来一个技术很重要的点就是in context learning

155
00:06:46,000 --> 00:06:48,240
然后这一点其实是非常重要的

156
00:06:48,240 --> 00:06:50,959
就是fantuning和in context learning的区别是什么

157
00:06:50,959 --> 00:06:53,319
尤其是fantuning和few shot的区别是什么

158
00:06:53,319 --> 00:06:56,079
因为乍一看他们其实可能是差不多的

159
00:06:57,720 --> 00:06:59,839
这也是很多过去的machinery scientist

160
00:06:59,839 --> 00:07:01,600
在这方面会出现问题的地方

161
00:07:01,600 --> 00:07:02,879
在这个概念上会出现问题

162
00:07:02,879 --> 00:07:05,560
因为其实你说in context learning

163
00:07:05,560 --> 00:07:06,600
是不是fantuning

164
00:07:06,600 --> 00:07:09,040
从一定程度上也是可以这么说的

165
00:07:09,040 --> 00:07:12,000
因为fantuning这个词并不是特别的精确

166
00:07:12,199 --> 00:07:14,519
所以呢我们这儿就发现fantuning其实是

167
00:07:14,519 --> 00:07:15,439
一个过度

168
00:07:15,639 --> 00:07:17,560
broad过度宽泛的一个词

169
00:07:17,759 --> 00:07:19,639
那我们怎么样子去区别呢

170
00:07:19,639 --> 00:07:20,800
我们就区别过去的

171
00:07:20,800 --> 00:07:21,600
machinery model

172
00:07:21,600 --> 00:07:23,879
是需要change the weight of the base model

173
00:07:24,040 --> 00:07:25,519
但是in context learning呢他其实是activ是需要change the weight of the base model但是in context learning

174
00:07:25,519 --> 00:07:28,720
它其实是activate了different weights in the base model

175
00:07:28,720 --> 00:07:30,199
它不需要change the base

176
00:07:30,199 --> 00:07:32,480
change the weight of the base model

177
00:07:32,480 --> 00:07:35,360
就是你用GPT出来的东西

178
00:07:35,360 --> 00:07:37,319
你其实是不改变的GPT的

179
00:07:37,319 --> 00:07:38,600
你是拿着GPT这个模型

180
00:07:38,600 --> 00:07:40,560
然后去提炼出来了一个不一样的东西

181
00:07:40,560 --> 00:07:43,680
是通过alignment这一层去提炼出来的不同的东西

182
00:07:43,680 --> 00:07:45,319
activate了不同的weight但是你没有回去做的不同的东西activate不同的位置

183
00:07:45,319 --> 00:07:47,639
但是你没有回去做一个不同的任务

184
00:07:47,639 --> 00:07:49,600
就要去把GPT给改变一次

185
00:07:49,600 --> 00:07:50,720
这个是很重要的

186
00:07:50,720 --> 00:07:51,759
这就是区别了

187
00:07:51,759 --> 00:07:52,439
就traditional

188
00:07:52,439 --> 00:07:53,920
如果说一个high level的解释的话

189
00:07:53,920 --> 00:07:56,839
traditional ML是new task需要new model

190
00:07:56,839 --> 00:07:59,680
in context learning可以让你做到new task

191
00:07:59,680 --> 00:08:02,000
same model different alignment

192
00:08:02,000 --> 00:08:04,600
然后第三个重要的点就是emergence

193
00:08:04,600 --> 00:08:06,000
就是涌现大模型的涌整然后第三个重要的点就是Emergence就是涌现

194
00:08:06,000 --> 00:08:08,000
大模型的涌现是一个非常重要的点

195
00:08:08,000 --> 00:08:10,000
就是它不是线性的一点一点变好的

196
00:08:10,000 --> 00:08:12,000
而是过了一个节点突然就变得很好了

197
00:08:12,000 --> 00:08:14,000
这个点是很重要的

198
00:08:14,000 --> 00:08:18,000
就是它会让你的工程变得不是那么可控

199
00:08:18,000 --> 00:08:20,000
我们不知道为什么我们也不能partake

200
00:08:20,000 --> 00:08:23,000
但是我们知道它好像你给它足够多的数据

201
00:08:23,000 --> 00:08:25,120
足够大的模型他就出现了

202
00:08:25,759 --> 00:08:28,240
这些我们就一个一个过我们就不多说了

203
00:08:28,240 --> 00:08:29,720
你如果想看的话可以了解

204
00:08:29,720 --> 00:08:32,879
所以这个时候你就是这个图虽然很可怕

205
00:08:32,879 --> 00:08:34,720
但是我觉得其实讲的还是有点道理的

206
00:08:34,720 --> 00:08:38,159
就是我们在背后是整个就是GPT的

207
00:08:38,159 --> 00:08:40,080
这个所谓的unsupervised learning

208
00:08:40,080 --> 00:08:42,440
当然这个词也不是完全准

209
00:08:42,440 --> 00:08:46,840
我觉得更准的是把unsupervised learning 变成 GPT

210
00:08:46,840 --> 00:08:48,960
就是它的 base model

211
00:08:48,960 --> 00:08:52,799
然后 supervised fine tuning 我们把它叫做 alignment

212
00:08:52,799 --> 00:08:55,679
然后 reinforcement learning with human feedback

213
00:08:55,679 --> 00:08:57,879
这个可以是 alignment 中间的一环

214
00:08:57,879 --> 00:09:00,360
是 alignment 的其中一种方法

215
00:09:00,360 --> 00:09:03,519
我就不太多去详细解释它了

216
00:09:03,519 --> 00:09:09,360
但是总之其实在 GPT 的大模型里面的知识是非常大的

217
00:09:09,360 --> 00:09:12,559
我们在现在只不过是activate了它的其中一点点

218
00:09:12,559 --> 00:09:16,639
然后是通过这种instruct或者说是chat的方式

219
00:09:16,639 --> 00:09:21,120
去让这个模型的输出变得更palatable to human

220
00:09:21,120 --> 00:09:24,480
就是我们更容易理解或者说我们更容易appreciate

221
00:09:24,480 --> 00:09:28,399
其实它的思维模式可能远远在我们所能理解之上

222
00:09:28,399 --> 00:09:31,519
只不过我们没有办法理解模型在想什么

223
00:09:31,519 --> 00:09:34,799
我们需要用chat的方式去理解模型在想什么

224
00:09:34,799 --> 00:09:38,559
最后就是reinforcement learning with human feedback

225
00:09:38,559 --> 00:09:42,080
其实如果大家去看extract gpt和chat gpt的paper的话

226
00:09:42,080 --> 00:09:45,399
你会发现他们的整个范式是非常接近的

227
00:09:45,399 --> 00:09:47,639
基本上就是蓝色和绿色的区别

228
00:09:47,639 --> 00:09:51,240
但是其他的就是你怎么样子去用reward function

229
00:09:51,240 --> 00:09:52,440
你怎么去用policy

230
00:09:52,440 --> 00:09:55,720
然后怎么样子去做reward model

231
00:09:55,720 --> 00:09:58,480
都是几乎是一样的方式

232
00:09:58,480 --> 00:10:00,559
就是其他的所有的点都是一样的

233
00:10:00,559 --> 00:10:04,320
只不过是他到底是用instruct去做align

234
00:10:04,320 --> 00:10:05,759
还是去用chat做align还是去用chart做align

235
00:10:05,759 --> 00:10:08,000
这就是未来其他很多模型

236
00:10:08,000 --> 00:10:10,799
就是openAI会出现很多模型的方法

237
00:10:10,799 --> 00:10:12,600
就是它不一定要用instruct或者chart

238
00:10:12,600 --> 00:10:15,039
它可以有别的方式去align模型

239
00:10:15,039 --> 00:10:17,320
然后去输出对应的效果

240
00:10:17,320 --> 00:10:19,279
来适配不同的任务种类

241
00:10:19,279 --> 00:10:23,519
最后一个就是我们把所有刚才说的东西给总结到一起

242
00:10:23,519 --> 00:10:27,120
我们在最底层会有一个pre-trained这个foundation model

243
00:10:27,120 --> 00:10:30,159
然后我们可以去funtune来change weight的话

244
00:10:30,159 --> 00:10:32,879
就是你拿到一个GPT你训练好了一个GPT

245
00:10:32,879 --> 00:10:35,679
然后你需要根据新的任务去funtune

246
00:10:35,679 --> 00:10:38,639
然后把整个GPT的foundation model都改了话

247
00:10:38,639 --> 00:10:40,320
那就是traditional ML的做法

248
00:10:40,320 --> 00:10:42,080
那不是我们现在的做法

249
00:10:42,080 --> 00:10:43,840
我们要在左边那条路径上画个X

250
00:10:43,840 --> 00:10:47,799
接下来我们是funtune to activate different weights

251
00:10:47,799 --> 00:10:51,919
其实我们就把这一层应该改成叫alignment

252
00:10:51,919 --> 00:10:56,519
因为GPT有了in-context learning的这样的一个涌现出来的能力

253
00:10:56,519 --> 00:11:00,360
导致我们可以通过alignment去activate不同的weight

254
00:11:00,360 --> 00:11:03,320
在这个之下我们再用不同的方式

255
00:11:03,320 --> 00:11:06,519
我们接下来会说就是我们现在所看到的

256
00:11:06,519 --> 00:11:07,919
不管是GPT Store也好

257
00:11:07,919 --> 00:11:09,399
还是这个agency也好

258
00:11:09,399 --> 00:11:11,720
都是在ChainGPT之下的一个方式

259
00:11:11,720 --> 00:11:14,039
我们回头再展开就好

260
00:11:14,039 --> 00:11:15,720
我打一个比方

261
00:11:15,720 --> 00:11:16,919
就是我们在了解了以后

262
00:11:16,919 --> 00:11:18,200
我们可以用一个比方再去理解

263
00:11:18,200 --> 00:11:19,399
我不喜欢用比方来开头

264
00:11:19,399 --> 00:11:21,200
因为那个时候会带来一些误解

265
00:11:21,200 --> 00:11:24,399
就是我们整个GPT的PrintTrend

266
00:11:24,399 --> 00:11:26,000
这个Base Foundation Model

267
00:11:26,000 --> 00:11:29,080
其实是钢铁侠的那个反应核心

268
00:11:29,080 --> 00:11:30,879
就是他那个Arc

269
00:11:30,879 --> 00:11:33,000
他中间这个反应核心有了以后

270
00:11:33,000 --> 00:11:34,360
你才有可能做出钢铁侠

271
00:11:34,360 --> 00:11:37,159
其实上面的一些武器人类都已经有了

272
00:11:37,159 --> 00:11:38,480
或者说Tony Stark

273
00:11:38,480 --> 00:11:40,200
就Stark Industry已经有了

274
00:11:40,200 --> 00:11:41,639
你没有中间的那个核心呢

275
00:11:41,639 --> 00:11:43,360
那就像下面这张图一样

276
00:11:43,360 --> 00:11:45,120
是那个什么Hemley Industries

277
00:11:45,120 --> 00:11:47,279
就是他试图做出来一堆

278
00:11:47,279 --> 00:11:48,960
长得像Ironman的东西

279
00:11:48,960 --> 00:11:50,480
可是他没有核心

280
00:11:51,279 --> 00:11:52,320
你有了核心以后

281
00:11:52,320 --> 00:11:53,519
你通过不同的Line问题

282
00:11:53,519 --> 00:11:55,039
能把它调教成不同的任务

283
00:11:55,039 --> 00:11:57,039
这里边我们看到钢铁侠3里边的

284
00:11:57,039 --> 00:11:58,000
不同的钢铁侠

285
00:11:58,000 --> 00:11:59,360
它可以适配不同的任务

286
00:11:59,360 --> 00:12:01,600
但是前提都是要有核心

287
00:12:01,600 --> 00:12:03,759
核心的质量是最重要的

288
00:12:03,759 --> 00:12:08,360
这个时候很多时候大家忽视的大家会用各种各样开源的模型去做出来一个

289
00:12:08,840 --> 00:12:14,840
看上去也不错的核心 但是其实它的核心的质量比钢铁侠差很远 其实就是钢铁侠2里边的

290
00:12:15,679 --> 00:12:21,320
那个鞭侠对吧 用鞭子的那个人 他也可以用 你可以把他理解成一个开源模型

291
00:12:21,679 --> 00:12:24,159
所以他也能做出来一个核心 然后他做的这个核心

292
00:12:21,399 --> 00:12:23,000
把它理解成一个太原模型它也能做出来一个核心

293
00:12:23,000 --> 00:12:26,840
然后它做的核心的效果也能似是而非

294
00:12:26,840 --> 00:12:29,039
但是它不是钢铁侠的核心

295
00:12:29,039 --> 00:12:32,639
它也不能像钢铁侠在接下来的漫威里边

296
00:12:32,639 --> 00:12:35,200
迭代出来那么多的tech

297
00:12:35,200 --> 00:12:37,879
因为它的核心是不够强的

298
00:12:37,879 --> 00:12:40,039
然后它的研发能力也是不够强的

299
00:12:40,039 --> 00:12:42,120
这个我在会员视频里边有讲过

300
00:12:42,120 --> 00:12:46,100
就是为什么OpenAI现在是站在一个指数增长的环节上

301
00:12:46,100 --> 00:12:49,799
这个LM的历史我就不多说了

302
00:12:49,799 --> 00:12:52,100
就是他到底是过去的发展

303
00:12:52,100 --> 00:12:53,899
Transformers的他到底是什么

304
00:12:53,899 --> 00:12:55,799
Transformers的这个发展

305
00:12:55,799 --> 00:12:58,399
我这之前都有兴趣的话

306
00:12:58,399 --> 00:13:00,799
其实应该去看俊林老师的那个文章

307
00:13:00,799 --> 00:13:02,200
就是我绝大多数的

308
00:13:02,200 --> 00:13:05,500
对这个知识是通过他的文章传起来的

309
00:13:05,500 --> 00:13:08,000
包括t5怎么样子把所有的task

310
00:13:08,000 --> 00:13:09,500
line成generative task

311
00:13:09,500 --> 00:13:11,500
这是一个非常重要的点

312
00:13:11,500 --> 00:13:14,500
还有一些其他的quick takeover的东西

313
00:13:14,500 --> 00:13:16,500
比如说GPT是怎么开始的对吧

314
00:13:16,500 --> 00:13:20,500
然后这个就是GPT和Birds在

315
00:13:20,500 --> 00:13:23,500
PKGPT完败的情况下

316
00:13:23,500 --> 00:13:26,000
大家是怎么样子去stick with GPT的

317
00:13:26,000 --> 00:13:28,000
然后ChunFuzi这边的作用

318
00:13:28,000 --> 00:13:32,080
和Gates在这里边扮演的关键角色

319
00:13:32,080 --> 00:13:35,600
就是在GPT3到3.5到拆GPT的时候

320
00:13:35,600 --> 00:13:38,080
Bergeis在这里边其实扮演了非常重要的角色

321
00:13:38,080 --> 00:13:39,279
他在里边也有说

322
00:13:39,279 --> 00:13:40,159
我也不多说了

323
00:13:41,600 --> 00:13:43,840
接下来就进入我们的正式环节

324
00:13:43,840 --> 00:13:45,039
就是这五个问题

325
00:13:45,039 --> 00:13:47,159
第一个就是它是不是一个

326
00:13:47,159 --> 00:13:48,759
只是一个更好的代理模型

327
00:13:48,759 --> 00:13:49,879
这个其实是乐库的观点

328
00:13:49,879 --> 00:13:51,679
就是GPT其实没有什么不一样的

329
00:13:51,679 --> 00:13:53,159
它是一个更好的模型

330
00:13:53,159 --> 00:13:54,200
它没有凡事突破

331
00:13:54,200 --> 00:13:56,159
那我接下来要问的问题就是

332
00:13:56,159 --> 00:13:58,440
我们人类是不是只是一个更聪明的猴子

333
00:13:58,840 --> 00:14:00,000
答案肯定不是

334
00:14:00,000 --> 00:14:02,320
但我们就要抱着这个观点

335
00:14:02,320 --> 00:14:05,360
就是说你不能因为我们看不出来

336
00:14:05,360 --> 00:14:07,080
人类和猴子的大脑有什么不一样

337
00:14:07,080 --> 00:14:09,159
从而去否定人类和猴子不一样

338
00:14:09,399 --> 00:14:12,360
Chai GPT似乎从原理上和其他大学模型不一样

339
00:14:12,360 --> 00:14:15,919
但是我们要接受Chai GPT有可能不一样的可能性

340
00:14:16,279 --> 00:14:17,879
过去的Models是什么

341
00:14:17,879 --> 00:14:18,919
就是我们要知道不一样的话

342
00:14:18,919 --> 00:14:21,200
我们就要知道过去的Models是什么

343
00:14:21,200 --> 00:14:25,000
过去的Models它其实就是Find Correspondence

344
00:14:25,000 --> 00:14:28,000
它是在数据里边寻找一些规律

345
00:14:28,000 --> 00:14:31,000
那你可以告诉它你这个规律寻找的对不对

346
00:14:31,000 --> 00:14:34,000
最简单的就是你给它一对X给它一对Y

347
00:14:34,000 --> 00:14:35,000
然后把X给map到Y

348
00:14:35,000 --> 00:14:39,000
那接下来它选好了这个X和Y之间的关系以后

349
00:14:39,000 --> 00:14:42,000
你再给它一对新的X和一对新的Y看看能不能map的好

350
00:14:42,000 --> 00:14:45,000
你可以在这里面有各种各样的方式

351
00:14:45,000 --> 00:14:47,000
设计各种各样的这个loss function

352
00:14:47,000 --> 00:14:50,799
去帮助他和你的机制去帮助他去

353
00:14:50,799 --> 00:14:53,600
把这个correspondence找的比较好

354
00:14:53,600 --> 00:14:55,000
找到你想要的correspondence

355
00:14:55,000 --> 00:14:58,000
而不要找那些over fitting或者说是under fitting的东西

356
00:14:58,000 --> 00:15:03,100
同时呢你再把他和其他的计算机的功能给结合起来

357
00:15:03,100 --> 00:15:04,799
你就可以让他做很多的事情

358
00:15:04,799 --> 00:15:06,039
这是过去的 machine learning

359
00:15:06,480 --> 00:15:09,000
但是 machine learning 它有一个问题

360
00:15:09,000 --> 00:15:10,399
就是它只会鹦鹉学舌

361
00:15:10,399 --> 00:15:11,360
它不能理解

362
00:15:11,360 --> 00:15:13,440
当然这两个词都非常的重

363
00:15:13,440 --> 00:15:15,080
所以说我们接下来就要去讲

364
00:15:15,159 --> 00:15:16,559
什么是理解

365
00:15:16,559 --> 00:15:18,399
在这里边有一个 window grade schema

366
00:15:18,399 --> 00:15:21,600
它非常好的去讲了这里边的理解

367
00:15:21,879 --> 00:15:23,679
我觉得最好的就是

368
00:15:23,679 --> 00:15:27,799
朱松成老师所说的乌鸦和鹦鹉的区别

369
00:15:27,799 --> 00:15:30,399
就是在一个城市里边呢

370
00:15:30,399 --> 00:15:34,200
乌鸦可以通过观察车观察人

371
00:15:34,200 --> 00:15:38,000
它想要达到一个打开一个坚果的这样的一个任务

372
00:15:38,000 --> 00:15:42,000
那它就发现车是可以在红绿灯前面停下来的

373
00:15:42,000 --> 00:15:44,399
车可以压碎它的坚果

374
00:15:44,399 --> 00:15:46,200
然后它就会把坚果drop到这个红绿灯前面停下来的车可以压碎他的坚果然后他就会把坚果

375
00:15:46,200 --> 00:15:48,200
drop到红绿灯前面

376
00:15:48,200 --> 00:15:49,799
让车去把它压碎

377
00:15:49,799 --> 00:15:50,799
然后等到红灯的时候

378
00:15:50,799 --> 00:15:52,799
再去把坚果给pick up起来

379
00:15:52,799 --> 00:15:55,200
在这里边他只有一次任务

380
00:15:55,200 --> 00:15:57,399
就是他如果被车撞了就被撞死了

381
00:15:57,399 --> 00:15:59,000
所以说他所有这些东西

382
00:15:59,000 --> 00:16:00,399
都是通过他的inference

383
00:16:00,399 --> 00:16:01,399
都是通过他的理解

384
00:16:01,399 --> 00:16:03,200
都是通过他的comprehend

385
00:16:03,200 --> 00:16:06,279
然后他自己在脑子里边run了一个simulation

386
00:16:06,279 --> 00:16:08,399
然后发现他觉得这样可以他才去做的

387
00:16:08,399 --> 00:16:12,240
而不是通过试错来试出来的

388
00:16:12,240 --> 00:16:14,440
大家如果说是一个merchant learning scientist

389
00:16:14,440 --> 00:16:16,720
你就可以代入到merchant learning的工作方式

390
00:16:16,720 --> 00:16:19,679
你可以知道过去的merchant learning是不可能做这种东西的

391
00:16:19,679 --> 00:16:21,559
因为他没有办法得到这样的数据

392
00:16:21,559 --> 00:16:24,600
过去的merchant learning你只能通过1000个乌鸦

393
00:16:24,600 --> 00:16:25,279
1万个乌鸦1万个乌鸦

394
00:16:25,279 --> 00:16:26,159
100万个乌鸦

395
00:16:26,159 --> 00:16:27,200
反复的去

396
00:16:27,480 --> 00:16:28,600
randomly尝试

397
00:16:28,600 --> 00:16:29,399
各种各样的pattern

398
00:16:29,399 --> 00:16:30,159
找到了以后

399
00:16:30,159 --> 00:16:31,240
然后再进化

400
00:16:31,240 --> 00:16:32,159
可是这样的代价

401
00:16:32,159 --> 00:16:32,679
就是必须要

402
00:16:32,679 --> 00:16:34,200
你要死这么多个乌鸦

403
00:16:34,240 --> 00:16:35,679
你是没有办法去

404
00:16:36,080 --> 00:16:37,000
在脑子里边

405
00:16:37,000 --> 00:16:38,399
simulate出来不同的效果

406
00:16:38,399 --> 00:16:39,159
最后得到

407
00:16:39,159 --> 00:16:40,679
达成这个任务的

408
00:16:40,679 --> 00:16:43,120
就所谓的Deduce and Inference

409
00:16:43,440 --> 00:16:45,879
拆GPT是可以做到这件事情的

410
00:16:45,879 --> 00:16:48,600
而且他拆GPT就是这个

411
00:16:48,600 --> 00:16:51,200
GPT3.5吧和GPT3和以前的

412
00:16:51,200 --> 00:16:52,919
我们训练的model有个很不一样的

413
00:16:52,919 --> 00:16:54,279
就是以前的大语言模型

414
00:16:54,679 --> 00:16:56,759
和很多哪怕现在的开源的大语言模型

415
00:16:56,759 --> 00:16:57,440
有很大的区别

416
00:16:57,440 --> 00:16:59,639
就是他可以做in context correction

417
00:16:59,840 --> 00:17:01,840
就是说你他跟你说了一个东西

418
00:17:01,840 --> 00:17:03,240
然后你跟他说你这说的不对

419
00:17:03,240 --> 00:17:05,599
他马上说我说的不对我来改

420
00:17:05,599 --> 00:17:07,839
这是一个非常强的推理能力

421
00:17:07,839 --> 00:17:11,640
这在这GPT3.5之前是没有一个模型

422
00:17:11,640 --> 00:17:15,440
哪怕GPT2和3都没有展现出来的能力

423
00:17:15,440 --> 00:17:16,680
展现出来了这个以后

424
00:17:16,680 --> 00:17:19,440
我们觉得GPT确实和过去是不一样的

425
00:17:19,440 --> 00:17:23,240
他有了一个真正的理解的能力在

426
00:17:24,319 --> 00:17:28,680
那接下来就遇到了就是去年我在出这个的时候的一个巨大的debate

427
00:17:28,680 --> 00:17:31,319
当然是乐坤为首和一大堆数据科学家

428
00:17:31,319 --> 00:17:36,000
一大堆这个machinery scientist都持这一派观点

429
00:17:36,000 --> 00:17:38,480
现在大家听到的这个争论越来越少了

430
00:17:38,480 --> 00:17:39,920
乐坤似乎也不这么想了

431
00:17:39,920 --> 00:17:42,680
但是是不是大家真的被说服了呢

432
00:17:42,680 --> 00:17:46,119
就是我们回来就去想GGBT是不是真的有意识

433
00:17:46,480 --> 00:17:47,799
这又是一个很重要的问题

434
00:17:48,000 --> 00:17:49,400
如果GBT能理解的话

435
00:17:49,599 --> 00:17:50,640
GBT具备意识吗

436
00:17:50,759 --> 00:17:53,599
如果GBT能理解的话又具备意识

437
00:17:53,839 --> 00:17:55,920
它的模型我们又知道

438
00:17:56,400 --> 00:17:57,559
你在这样的一个

439
00:17:57,559 --> 00:18:00,119
Auto Regressive Generative

440
00:18:00,119 --> 00:18:02,240
Latent Language Models的范式之下

441
00:18:02,240 --> 00:18:03,920
能不能培养出来一个意识呢

442
00:18:04,440 --> 00:18:07,279
这就回到了一个哲学问题就是意识到底是什么

443
00:18:07,279 --> 00:18:09,039
其实我们不知道意识是什么

444
00:18:09,039 --> 00:18:12,359
这个是哲学的哲学给我们的启示

445
00:18:12,359 --> 00:18:16,759
哲学是说我们并不知道人类到底是否有意识

446
00:18:16,759 --> 00:18:19,519
然后我这里边有链接

447
00:18:19,519 --> 00:18:22,839
就是20世纪最重要的几个科学问题

448
00:18:22,839 --> 00:18:24,039
就是意识到底是什么

449
00:18:24,039 --> 00:18:25,519
人类到底是不是真的有意识

450
00:18:25,519 --> 00:18:28,160
但是在Cognitive Science里边

451
00:18:28,480 --> 00:18:30,400
我们就会发现了一个观点

452
00:18:30,880 --> 00:18:34,880
人其实和猴子和其他动物

453
00:18:34,880 --> 00:18:35,920
Biological类

454
00:18:36,119 --> 00:18:39,119
其实不是特别的不一样

455
00:18:39,119 --> 00:18:40,160
就是说我们

456
00:18:40,519 --> 00:18:42,799
你如果光看我们和猴子的大脑的区别

457
00:18:42,799 --> 00:18:43,440
你不会觉得

458
00:18:43,440 --> 00:18:44,440
你不会得出一个结论

459
00:18:44,440 --> 00:18:46,599
说人类就是和猴子完全不一样的东西

460
00:18:46,599 --> 00:18:50,000
那这样看起来我们的意识其实是

461
00:18:50,000 --> 00:18:50,880
emerged的

462
00:18:50,880 --> 00:18:53,079
这个是一个cognitive science的观点

463
00:18:53,079 --> 00:18:54,920
这个词大家就很熟悉了

464
00:18:54,920 --> 00:18:58,799
就是GPT虽然它的结构和其他的

465
00:18:58,799 --> 00:19:00,160
Machine Learning没有那么不一样

466
00:19:00,160 --> 00:19:01,920
或者其他大语言模型那么不一样

467
00:19:01,920 --> 00:19:04,519
但是它似乎emerged出来了一些不一样的东西

468
00:19:04,519 --> 00:19:07,880
我们现在不要去深究就是为什么GPT不一样我们也不要去深究言模型那么不一样但是它似乎emerge出来了一些不一样的东西我们现在不要去深究为什么GPT不一样

469
00:19:07,880 --> 00:19:09,599
我们也不要去深究人类为什么不一样

470
00:19:09,599 --> 00:19:13,000
我们只要去关注它有可能是不一样的就好了

471
00:19:13,000 --> 00:19:18,599
第二个就是我们说GPT和以前是一个完全不一样的范式更新

472
00:19:18,599 --> 00:19:21,559
GPT到底是一个什么样的东西

473
00:19:21,759 --> 00:19:26,000
这个其实我在我的视频里面总结Gates和Ultimate的说法

474
00:19:26,000 --> 00:19:28,299
就是GUI加Morris law for everything

475
00:19:29,200 --> 00:19:30,480
第一个就是它是一个

476
00:19:30,480 --> 00:19:32,680
near perfect abstraction of

477
00:19:32,880 --> 00:19:34,539
internet technologies

478
00:19:34,539 --> 00:19:36,579
就是我们其实在过往的

479
00:19:36,940 --> 00:19:38,240
呃这个计算机呢

480
00:19:38,240 --> 00:19:40,980
就是我们是在算力和storage和存储上

481
00:19:40,980 --> 00:19:42,279
进行了很多的

482
00:19:42,740 --> 00:19:43,740
颠覆和

483
00:19:44,279 --> 00:19:45,319
advancement我们在数据的generation和利用上进行了很多的颠覆和advancement

484
00:19:45,319 --> 00:19:48,200
我们在数据的generation和利用上

485
00:19:48,200 --> 00:19:49,400
进行了很多进步

486
00:19:49,400 --> 00:19:52,599
然后我们是把这两个事给结合的很好

487
00:19:52,599 --> 00:19:56,000
那其实GUI它就解决了很多

488
00:19:56,000 --> 00:19:58,119
specific purpose的东西

489
00:19:58,119 --> 00:20:00,319
比如说我现在在看的这些东西

490
00:20:00,319 --> 00:20:01,920
都是跟GUI相关的

491
00:20:01,920 --> 00:20:03,119
我去刷抖音

492
00:20:03,119 --> 00:20:06,200
抖音是一个GUI去解决我的

493
00:20:06,200 --> 00:20:08,000
就是产生视频

494
00:20:08,000 --> 00:20:09,599
然后产生我的喜好

495
00:20:09,599 --> 00:20:12,000
把该我喜欢的视频推送到我面前

496
00:20:12,000 --> 00:20:13,500
然后高效率的去刷

497
00:20:13,500 --> 00:20:15,799
它都被GUI解决的很好

498
00:20:15,799 --> 00:20:16,900
但是拆GPT呢

499
00:20:16,900 --> 00:20:19,200
它可以解决一个general purpose的东西

500
00:20:19,200 --> 00:20:21,700
就是说我之前我抖音想的很好

501
00:20:21,700 --> 00:20:22,700
但是我跟他说

502
00:20:22,700 --> 00:20:25,000
我想用抖音的这个能力去干一些别的事情

503
00:20:25,200 --> 00:20:26,640
那我就需要去编程

504
00:20:26,640 --> 00:20:29,160
但是这个编程的门槛是非常非常之高的

505
00:20:29,359 --> 00:20:31,440
现在Chai GPT呢是可以让我

506
00:20:31,680 --> 00:20:34,519
去用自然语言去调动一个编程

507
00:20:34,799 --> 00:20:36,359
这就是我左上角的这个例子

508
00:20:36,359 --> 00:20:38,480
就是你来做一个Linux terminal

509
00:20:38,599 --> 00:20:40,119
之后我们可能就跟Chai GPT说

510
00:20:40,119 --> 00:20:41,279
我要做一个什么的

511
00:20:41,359 --> 00:20:44,440
GPT就可以帮助你调用算力存储数据

512
00:20:44,720 --> 00:20:46,400
来去实现你的目的

513
00:20:46,400 --> 00:20:49,799
我有另外一个比喻就是GUI是山

514
00:20:49,799 --> 00:20:51,599
然后GPT是水

515
00:20:51,599 --> 00:20:54,599
水涨船高迟早会淹没很多山的

516
00:20:54,599 --> 00:20:59,500
那我们接下来就是说就是intellectual

517
00:20:59,500 --> 00:21:04,700
就是就是我们智能智能的分发过去是非常非常之贵的

518
00:21:04,700 --> 00:21:07,400
因为智能是通过人

519
00:21:07,400 --> 00:21:09,440
然后人能面对的人是有限的

520
00:21:09,440 --> 00:21:11,720
可是我们接下来就可以进行智能的分发

521
00:21:11,720 --> 00:21:13,839
把智能分发的编制成本降为0

522
00:21:13,839 --> 00:21:16,319
那一个医生我就在过去也说了

523
00:21:16,319 --> 00:21:18,160
就是医生律师等等的这些人

524
00:21:18,160 --> 00:21:20,559
他可以同时服务无限度的人

525
00:21:20,559 --> 00:21:23,000
而不只需要服务他面前的用户

526
00:21:23,000 --> 00:21:25,480
我希望在一两年之后

527
00:21:25,680 --> 00:21:28,599
我的GPT是可以做一个非常非常好的

528
00:21:28,720 --> 00:21:30,599
数据分析师或者数据科学家的

529
00:21:30,759 --> 00:21:31,880
这就是刚刚说的

530
00:21:31,880 --> 00:21:33,559
就是你做出来一个Everman之后

531
00:21:33,559 --> 00:21:35,519
你给他再加一个差不多的大脑

532
00:21:35,680 --> 00:21:37,839
那他其实自动的就可以去

533
00:21:38,079 --> 00:21:39,240
做很多很多的任务

534
00:21:39,240 --> 00:21:40,480
这个就是那个

535
00:21:40,640 --> 00:21:41,400
outrun对吧

536
00:21:41,400 --> 00:21:43,839
但是这是一个好版的outrun

537
00:21:43,839 --> 00:21:46,720
就是就是曾经如果说是完美的情况下

538
00:21:46,720 --> 00:21:48,720
奥创所能干的是什么事情

539
00:21:48,720 --> 00:21:49,920
就是这样的一个事情

540
00:21:49,920 --> 00:21:51,920
就是一下子出现了

541
00:21:51,920 --> 00:21:54,559
Unlimited Element

542
00:21:54,559 --> 00:21:57,359
Element自己一个人能干的事情是有限的

543
00:21:57,359 --> 00:21:59,359
我相信有了奥创大军以后

544
00:21:59,359 --> 00:22:01,359
Thanos是不可能打过他们的

545
00:22:01,359 --> 00:22:03,680
就像在飞猴女巫的宇宙里

546
00:22:03,680 --> 00:22:05,400
就是有光明会的那个宇宙一样

547
00:22:05,400 --> 00:22:06,960
那接下来就是如何

548
00:22:07,400 --> 00:22:09,400
这个它有多难制造

549
00:22:09,599 --> 00:22:11,720
这个我就简单的跳过吧

550
00:22:11,720 --> 00:22:13,640
因为其实我已经说过很多遍了

551
00:22:14,000 --> 00:22:16,240
第四个就是我们应该怎么去使用它

552
00:22:16,519 --> 00:22:18,119
其实就是说你在

553
00:22:18,559 --> 00:22:19,519
浏览器刚出来的时候

554
00:22:19,519 --> 00:22:20,680
不要去再造一个浏览器

555
00:22:20,680 --> 00:22:22,559
而是应该去做网页

556
00:22:22,839 --> 00:22:24,119
这是回到刚才的那个文章

557
00:22:24,119 --> 00:22:25,440
我把最下面打开

558
00:22:25,440 --> 00:22:28,359
其实我们就是一年前看的

559
00:22:28,359 --> 00:22:31,759
今这一年的所有的变化和这个几乎是

560
00:22:31,759 --> 00:22:33,119
完全走在剧本上

561
00:22:33,119 --> 00:22:34,920
就是没有什么超出意想的东西

562
00:22:34,920 --> 00:22:35,319
为什么呢

563
00:22:35,319 --> 00:22:37,160
因为当时new bing和microsoft

564
00:22:37,160 --> 00:22:39,519
copilot其实已经把这个路线

565
00:22:39,519 --> 00:22:40,799
探索的很好了

566
00:22:40,799 --> 00:22:42,519
就是你能access to tools

567
00:22:42,519 --> 00:22:44,960
然后这个prompt as a configuration

568
00:22:45,359 --> 00:22:46,960
你在开放这一层就是你能access to tools然后这个prompt as a configuration呃你在开放这一层

569
00:22:47,160 --> 00:22:48,279
就是你开放

570
00:22:48,640 --> 00:22:50,559
chai gpt可以去调用别的东西

571
00:22:50,559 --> 00:22:52,519
你可以写代码你可以去改变系统

572
00:22:52,839 --> 00:22:54,759
然后你在开放给一些

573
00:22:55,039 --> 00:22:56,839
指定的指令给chai gpt

574
00:22:56,839 --> 00:22:58,000
那他就能干很多事

575
00:22:58,000 --> 00:22:59,920
那再往上呢是开放你的这个

576
00:22:59,920 --> 00:23:01,200
element这个接口

577
00:23:02,119 --> 00:23:04,079
这我就不再多说了

578
00:23:04,079 --> 00:23:05,279
这个我们就直接看呃这一年的发布就好了不再多说了这个我们就直接看

579
00:23:05,279 --> 00:23:07,279
这一年的发布就好了

580
00:23:07,279 --> 00:23:10,319
相信都已经说了很就是这一年的发布

581
00:23:10,319 --> 00:23:14,559
非常好的验证了我一年前的这些预测

582
00:23:14,559 --> 00:23:15,599
这里有一个问题了

583
00:23:15,599 --> 00:23:20,000
就是我们如果说调用Tri-GPT的能力很重要的话

584
00:23:20,000 --> 00:23:21,920
其实有一个问题就是你的prompt

585
00:23:21,920 --> 00:23:25,079
你要去让Tri-GPT完成各种各样的任务

586
00:23:25,079 --> 00:23:29,119
那到底是一个工程技巧还是一个PM技巧

587
00:23:29,119 --> 00:23:31,799
就是你是要教给Chai GPT怎么用呢

588
00:23:31,799 --> 00:23:34,599
还是你是要告诉Chai GPT做什么

589
00:23:34,599 --> 00:23:38,400
我的想法是在一开始你一定要有很强的工程能力

590
00:23:38,400 --> 00:23:40,039
去告诉Chai GPT怎么用

591
00:23:40,039 --> 00:23:44,680
然后在未来越来越重要的是告诉Chai GPT做什么

592
00:23:49,119 --> 00:23:50,720
我其实在这一年用ChaiGBT做了很多尤其是我的数据分析

593
00:23:50,720 --> 00:23:55,559
我发现就是像我的一个实习生一样去使用ChaiGBT

594
00:23:55,559 --> 00:23:56,960
你先提出问题

595
00:23:56,960 --> 00:23:58,759
然后跟他一起去来按思路

596
00:23:58,759 --> 00:23:59,880
然后看他的思路对不对

597
00:23:59,880 --> 00:24:01,799
如果他的思路和你的思路差不多的时候

598
00:24:01,799 --> 00:24:03,079
跟他说你现在去做

599
00:24:03,079 --> 00:24:05,599
做完了以后再跟他不断的反馈提高

600
00:24:05,599 --> 00:24:08,160
这里边有非常多的要告诉他GPT做什么

601
00:24:08,160 --> 00:24:09,599
他才能把这个东西做好

602
00:24:09,599 --> 00:24:12,640
那之后我相信随着我的调教变多的话

603
00:24:12,640 --> 00:24:15,559
如果说我已经把他非常好的调教了

604
00:24:15,559 --> 00:24:17,200
告诉他如何做了

605
00:24:17,200 --> 00:24:19,680
那我之后再告诉他你要去做什么就行了

606
00:24:20,160 --> 00:24:22,000
所以说总结一下的话

607
00:24:22,000 --> 00:24:23,960
就是一开始是有很多

608
00:24:23,960 --> 00:24:26,359
你要告诉这个大�学模型怎么做

609
00:24:26,359 --> 00:24:28,440
在未来更多的是告诉他做什么

610
00:24:28,440 --> 00:24:31,000
再说一些过往的历史吧

611
00:24:31,000 --> 00:24:33,799
就是XGBT这件事不是一个真的prediction

612
00:24:33,799 --> 00:24:37,400
它只不过是历史很多颠覆科技的重复

613
00:24:37,400 --> 00:24:41,960
这个颠覆科技的重复就是当你电脑出现的时候

614
00:24:41,960 --> 00:24:43,599
当互联网出现的时候

615
00:24:43,599 --> 00:24:45,440
当移动互联网出现的时候

616
00:24:45,640 --> 00:24:48,279
其实第一波都是把现有的东西

617
00:24:48,279 --> 00:24:49,559
放到新的科技上

618
00:24:49,559 --> 00:24:50,480
这个是Selecon Valley

619
00:24:50,480 --> 00:24:51,880
那个把radio on internet

620
00:24:51,880 --> 00:24:53,559
那个变成billionaire的

621
00:24:53,559 --> 00:24:55,000
那个asshole对吧

622
00:24:55,000 --> 00:24:57,039
呃就是他只不过把现有的东西

623
00:24:57,039 --> 00:24:57,720
放到了互联网

624
00:24:57,720 --> 00:24:59,400
然后就得到了巨大的财富

625
00:24:59,519 --> 00:25:00,759
但是在那之后

626
00:25:00,759 --> 00:25:03,160
最重要的就是10倍百倍的机会

627
00:25:03,200 --> 00:25:07,319
一定是新的科技做了过去科技所做不到的事情的

628
00:25:07,319 --> 00:25:10,799
就比如说Google在一开始的互联网上是没有用的

629
00:25:10,799 --> 00:25:13,039
因为那个时候互联网上都没有什么信息

630
00:25:13,039 --> 00:25:16,759
但是Google它作为一个internet的fundamental的科技

631
00:25:16,759 --> 00:25:18,759
它是随着互联网增加的

632
00:25:18,759 --> 00:25:22,400
就是你互联网的信息指数增加了以后

633
00:25:22,400 --> 00:25:25,039
Google的有用程度也变得指数重要了

634
00:25:25,039 --> 00:25:28,200
所以说一开始我们会看到很多

635
00:25:28,200 --> 00:25:30,440
把现有的东西用代语模型去实现

636
00:25:30,440 --> 00:25:32,480
但是未来一定是代语模型去实现

637
00:25:32,480 --> 00:25:34,359
现在完全实现不了的东西

638
00:25:34,359 --> 00:25:36,359
然后这里边最重要的东西

639
00:25:36,359 --> 00:25:38,799
就是start with build simple things

640
00:25:38,799 --> 00:25:40,079
that people really want

641
00:25:40,079 --> 00:25:42,039
Instagram其实之前就是一个future app

642
00:25:42,039 --> 00:25:43,920
它不是一个social media或者怎么样

643
00:25:43,920 --> 00:25:46,079
现在的这些东西都是他做了一个

644
00:25:46,079 --> 00:25:48,400
人们真正需要的东西之后才出来的

645
00:25:48,640 --> 00:25:50,599
你没有人们需要的那个东西的话

646
00:25:50,599 --> 00:25:51,400
其他都是白谈

647
00:25:51,400 --> 00:25:52,799
你说那么多概念也没有用的

648
00:25:53,000 --> 00:25:54,480
然后这就是我的一下opinion了

649
00:25:54,480 --> 00:25:56,480
就是他到底是一个2b2c的机会

650
00:25:56,759 --> 00:25:58,039
它是Scannet吗

651
00:25:58,039 --> 00:25:58,759
我觉得不是

652
00:25:58,759 --> 00:26:00,079
但是它有可能是可以

653
00:26:00,319 --> 00:26:00,720
crack

654
00:26:00,720 --> 00:26:03,000
encryption的

655
00:26:03,000 --> 00:26:07,000
GPG native application application到底是什么东西

656
00:26:07,000 --> 00:26:11,000
然后这里边这个personalized private search有多么的重要

657
00:26:11,000 --> 00:26:14,000
就是现在我们所说的retrieved

658
00:26:14,000 --> 00:26:16,000
argumented generation

659
00:26:16,000 --> 00:26:17,000
对吧 RAG

660
00:26:17,000 --> 00:26:20,000
其实personalized private search

661
00:26:20,000 --> 00:26:22,000
就是这个retrieved这个环节非常重要的

662
00:26:22,000 --> 00:26:24,000
其实到现在这GPT都没有做得很好

663
00:26:24,000 --> 00:26:26,119
它也是一个未来非常重要的方向

664
00:26:26,119 --> 00:26:29,480
那个big tech在这里边的会benefit什么

665
00:26:29,480 --> 00:26:30,079
其实是的

666
00:26:30,079 --> 00:26:31,599
因为他们已经有了现有的场景

667
00:26:31,599 --> 00:26:33,839
他们其实可以把拆GPT在一开始用的很好

668
00:26:34,119 --> 00:26:37,680
最后一个问题就是人类和拆GPT的区别是什么

669
00:26:37,680 --> 00:26:40,960
就是拆GPT其实这个是乐坤去讲的一个

670
00:26:40,960 --> 00:26:42,640
我觉得他这点反而讲的很对

671
00:26:42,640 --> 00:26:45,839
就是拆GPT他其实可以知道很多东西

672
00:26:45,839 --> 00:26:48,200
但是问题他不知道什么东西是对的

673
00:26:48,200 --> 00:26:50,599
所以说需要人去告诉他什么东西是对的

674
00:26:50,599 --> 00:26:52,960
而且这个时候其实没有一个绝对的正确

675
00:26:52,960 --> 00:26:55,319
而只不过是什么东西对我更有用而已

676
00:26:55,319 --> 00:26:58,359
所以说他一直需要人去告诉他什么东西对我更有用

677
00:26:59,559 --> 00:27:03,559
那就是人类在这里边是需要知道什么东西真正有用的

678
00:27:03,799 --> 00:27:07,039
然后去知道这个世界上缺的是什么

679
00:27:07,400 --> 00:27:08,720
有一词叫做Eureka

680
00:27:08,720 --> 00:27:11,319
就是阿基米德发明福利定律的那件

681
00:27:11,319 --> 00:27:13,200
或者发现福利定律的事情

682
00:27:13,400 --> 00:27:15,480
其实Eureka就是人类独特的能力

683
00:27:15,480 --> 00:27:17,759
然后这里边大家仔细去想一下Eureka的话

684
00:27:17,759 --> 00:27:19,519
会发现有两个步骤

685
00:27:19,519 --> 00:27:22,160
第一个就是你要能找到答案

686
00:27:22,519 --> 00:27:26,440
但是第二个是你要发现答案是重要的

687
00:27:26,799 --> 00:27:28,400
猜猜比提拉很有可能

688
00:27:28,400 --> 00:27:31,240
他通过自己的特别强大的算力

689
00:27:31,240 --> 00:27:32,319
他能找到答案

690
00:27:32,319 --> 00:27:34,839
问题是他只不过是他找到的

691
00:27:34,839 --> 00:27:36,599
所有答案中的其中一个

692
00:27:36,960 --> 00:27:39,640
你要能知道这个答案是多么重要的话

693
00:27:39,640 --> 00:27:43,519
那还是需要人的这种直觉去知道

694
00:27:43,960 --> 00:27:45,599
我在这1000个答案里边

695
00:27:45,599 --> 00:27:47,000
这个答案是真正重要的

696
00:27:47,000 --> 00:27:47,799
那个答案

697
00:27:47,799 --> 00:27:49,200
我们回到乔布斯

698
00:27:49,200 --> 00:27:51,920
其实乔布斯说的这些东西就是这些crazy ones

699
00:27:51,920 --> 00:27:53,440
他们就是知道这个答案

700
00:27:53,440 --> 00:27:56,200
并且能把这个答案通过自己的conviction

701
00:27:56,200 --> 00:27:58,799
然后去通过自己的行动去把它实现出来

702
00:27:58,799 --> 00:28:00,799
他们是真正change the world

703
00:28:00,799 --> 00:28:03,559
所以说你如果只是停留在一些表面的思考的话

704
00:28:03,559 --> 00:28:05,279
那确实可以被拆式背景取代的

705
00:28:05,279 --> 00:28:07,039
然后这就是最后一个点了

706
00:28:07,039 --> 00:28:09,920
其实just make something people want

707
00:28:09,920 --> 00:28:11,599
是这里边所有的答案

708
00:28:11,599 --> 00:28:15,920
你在代语言模型中的代语言时代怎么样子去抓住机会

709
00:28:15,920 --> 00:28:17,759
just make something people want

710
00:28:17,759 --> 00:28:22,960
好的 这是一些appendix去讲

711
00:28:22,960 --> 00:28:26,000
这里边就是

712
00:28:26,000 --> 00:28:28,000
技术的演进到底是什么样子的

713
00:28:28,000 --> 00:28:30,000
这就是我给大家画的一些东西

714
00:28:30,000 --> 00:28:32,000
不在这里面讲了 如果大家感兴趣的话可以去看

715
00:28:32,000 --> 00:28:34,000
好的

716
00:28:34,000 --> 00:28:36,000
这里就是我们

717
00:28:36,000 --> 00:28:38,000
Vision Pro的

718
00:28:38,000 --> 00:28:40,000
一个

719
00:28:40,000 --> 00:28:42,000
看看刚才东西有没有录上

720
00:28:46,839 --> 00:28:49,240
希望大家喜欢

721
00:28:49,240 --> 00:28:51,839
或者说觉得这是一个有用的东西

722
00:28:51,839 --> 00:28:53,319
我的视频到那边去了

723
00:28:53,599 --> 00:28:54,880
那就先这样吧

724
00:28:56,640 --> 00:28:58,319
好了 我们下次见

725
00:28:58,319 --> 00:28:59,000
拜拜

