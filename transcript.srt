1
00:00:00,000 --> 00:00:01,399
Hello大家好欢迎回到课代表雷震

2
00:00:01,399 --> 00:00:02,439
我是门德的课代表

3
00:00:02,439 --> 00:00:05,040
这期视频我们继续课代表聊数据

4
00:00:05,040 --> 00:00:08,000
我们继续聊数据科学家和决策的关系

5
00:00:08,000 --> 00:00:09,439
在这我来问一个问题

6
00:00:09,800 --> 00:00:12,839
如果屏幕前的同学是数据科学家

7
00:00:12,839 --> 00:00:15,320
或者你们跟数据科学家打过交道

8
00:00:15,359 --> 00:00:17,280
有多少人认为

9
00:00:17,719 --> 00:00:21,359
你在有很确定的cultural evidence的时候

10
00:00:21,399 --> 00:00:23,199
敢对决策发表意见

11
00:00:23,280 --> 00:00:24,519
有的话扣1

12
00:00:24,800 --> 00:00:26,039
如果不敢的话扣2

13
00:00:26,199 --> 00:00:26,559
好

14
00:00:26,559 --> 00:00:27,239
接下来

15
00:00:27,280 --> 00:00:29,359
如果你没有cultural evidence

16
00:00:00,000 --> 00:00:02,120
这个时候你有一些correlation

17
00:00:02,120 --> 00:00:04,120
然后你有一些其他的analysis

18
00:00:04,120 --> 00:00:07,320
然后你对你的insights觉得很确定

19
00:00:07,320 --> 00:00:10,199
你觉得就是A虽然没有causal evidence

20
00:00:10,199 --> 00:00:11,240
但是能导致B

21
00:00:11,240 --> 00:00:13,000
或者说你觉得做A是对的

22
00:00:13,119 --> 00:00:14,839
哪怕在没有A B实验

23
00:00:14,839 --> 00:00:17,039
或者各种各样causal analysis的

24
00:00:17,160 --> 00:00:18,120
基础之上

25
00:00:18,120 --> 00:00:19,519
你觉得做A是对的

26
00:00:19,679 --> 00:00:23,239
这个时候你敢不敢对决策发表意见

27
00:00:23,239 --> 00:00:25,199
如果敢的话请打1

28
00:00:25,199 --> 00:00:26,920
如果不敢的话请打2

29
00:00:26,920 --> 00:00:27,800
第三种情况

30
00:00:00,000 --> 00:00:02,359
你只有一些基本的data或者facts

31
00:00:02,359 --> 00:00:05,759
可是你对这件事情有一定的理解

32
00:00:05,759 --> 00:00:09,439
所以说你虽然你的analysis

33
00:00:09,439 --> 00:00:11,599
不能证明作为A是对的

34
00:00:11,599 --> 00:00:13,640
可是你觉得作为A是对的

35
00:00:13,640 --> 00:00:17,679
有多少人敢在这个时候对这个决策发表意见

36
00:00:17,679 --> 00:00:19,879
敢的请打一不敢的请打二

37
00:00:19,879 --> 00:00:22,559
然后我这个也是在Facebook跟那个大佬

38
00:00:22,559 --> 00:00:23,719
上那门课的时候

39
00:00:23,719 --> 00:00:25,559
他给我的一个特别大的启发

40
00:00:25,559 --> 00:00:29,719
他认为数据科学家在情况三的时候

41
00:00:00,000 --> 00:00:01,439
也应该敢说话

42
00:00:01,480 --> 00:00:03,080
因为在情况三的时候

43
00:00:03,080 --> 00:00:05,160
其他的所有的职业都敢说话

44
00:00:05,200 --> 00:00:07,960
尤其是作为一个PM是吧

45
00:00:08,000 --> 00:00:10,199
然后各种各样的leadership

46
00:00:10,320 --> 00:00:12,039
他们的工作就是在

47
00:00:12,039 --> 00:00:14,720
充满uncertainty的情况下去做一个决定

48
00:00:14,720 --> 00:00:15,960
这个决定可能是对的

49
00:00:15,960 --> 00:00:16,879
可能是错的

50
00:00:16,920 --> 00:00:19,039
有的时候是需要你的intuition的

51
00:00:19,039 --> 00:00:21,440
但是大家敢做这个决定

52
00:00:21,440 --> 00:00:24,480
起码大家敢对这个决定发表意见

53
00:00:24,480 --> 00:00:26,800
就是我们应不应该做这个决定发表意见

54
00:00:27,000 --> 00:00:29,399
但是很多数据科学家

55
00:00:00,000 --> 00:00:01,840
因为职业受到的training

56
00:00:01,840 --> 00:00:02,919
我们就会觉得

57
00:00:02,919 --> 00:00:05,240
我只有在最concrete

58
00:00:05,240 --> 00:00:08,039
最concise最无懈可击的证据之下

59
00:00:08,039 --> 00:00:09,199
我才能发表意见

60
00:00:09,199 --> 00:00:11,800
如果我没有这些数据或者这些analysis

61
00:00:11,800 --> 00:00:13,519
去backup我的观点的话

62
00:00:13,519 --> 00:00:15,519
那我就不应该去发表意见

63
00:00:15,519 --> 00:00:19,039
我应该把发表意见的责任和权利

64
00:00:19,039 --> 00:00:20,199
交给其他人

65
00:00:20,280 --> 00:00:22,920
这样做在我看来是不对的

66
00:00:22,920 --> 00:00:23,719
为什么呢

67
00:00:23,719 --> 00:00:25,559
就是因为第一

68
00:00:25,559 --> 00:00:27,640
PM比我们都长了个脑袋吗

69
00:00:27,640 --> 00:00:28,199
没有啊

70
00:00:00,000 --> 00:00:03,279
就是他们受到了很多他们独有的training

71
00:00:03,279 --> 00:00:07,799
能帮助他们对这个决策产生有价值的input

72
00:00:07,799 --> 00:00:10,119
可是我们也受到了很多的training

73
00:00:10,119 --> 00:00:12,119
我们有这个scientific的training

74
00:00:12,119 --> 00:00:13,759
我们对数据更敏感

75
00:00:13,759 --> 00:00:16,000
我们有很多logical的training

76
00:00:16,000 --> 00:00:19,719
所以说我们一样可以对这个决策

77
00:00:19,719 --> 00:00:21,359
在没有数据的情况下

78
00:00:21,359 --> 00:00:24,800
在我们没有一个analysis来证明这个决策正确的情况下

79
00:00:24,800 --> 00:00:27,079
然后对这个决策产生了正向的贡献

80
00:00:27,079 --> 00:00:28,519
这是事情的一面

81
00:00:00,000 --> 00:00:02,240
就是我们不要把自己的决策

82
00:00:02,240 --> 00:00:04,919
因为我们受到的training更rigorous

83
00:00:04,919 --> 00:00:09,080
而局限在了只能得到rigorous answer的这一部分

84
00:00:09,080 --> 00:00:13,320
而是你应该把你的注意力放在更大范围

85
00:00:13,320 --> 00:00:14,439
更重要的决策上

86
00:00:14,439 --> 00:00:17,719
这些重要的决策很多时候是有很多模糊性的

87
00:00:17,719 --> 00:00:20,039
但是作为一个数据科学家

88
00:00:20,039 --> 00:00:24,120
你也应该敢于对这些决策发表意见

89
00:00:24,120 --> 00:00:29,719
但是事情的反面就是我们要对一个决策中

90
00:00:00,000 --> 00:00:02,960
到底有多少是能被数据measure的

91
00:00:02,960 --> 00:00:04,679
有一个清醒的认知

92
00:00:04,679 --> 00:00:07,879
前面的一部分是在你没有数据的情况下

93
00:00:07,879 --> 00:00:09,000
也要敢做决策

94
00:00:09,000 --> 00:00:10,960
后面这部分就是不要过分

95
00:00:10,960 --> 00:00:13,839
quadrate数据在决策中的作用

96
00:00:13,839 --> 00:00:15,720
我发现后面这种情况

97
00:00:15,720 --> 00:00:17,600
就是我之前的一个视频讲的

98
00:00:17,600 --> 00:00:21,920
很多企业会依赖模型做决策而犯下的错误

99
00:00:21,920 --> 00:00:24,839
但是我其实后来有一个苏刚同学

100
00:00:24,839 --> 00:00:26,960
在跟我讨论的时候跟我说了

101
00:00:00,000 --> 00:00:03,200
這個 McNamara fallacy

102
00:00:03,200 --> 00:00:05,000
我才發現

103
00:00:05,000 --> 00:00:07,480
原來這件事情不是什麼

104
00:00:07,480 --> 00:00:10,000
現代的互聯網大企業才發生的

105
00:00:10,000 --> 00:00:12,960
這是一個講的美國越戰的故事

106
00:00:12,960 --> 00:00:16,879
當時美國的國防部長叫這個 McNamara

107
00:00:16,879 --> 00:00:19,120
他當時在這個越戰中

108
00:00:19,120 --> 00:00:22,000
就犯下了一個很致命的錯誤

109
00:00:22,000 --> 00:00:23,800
這個致命的錯誤是什麼呢

110
00:00:23,800 --> 00:00:26,480
他這裡邊說有四步對吧

111
00:00:26,480 --> 00:00:27,640
第一步呢

112
00:00:00,000 --> 00:00:03,000
是去measure whatever can be easily measured

113
00:00:03,480 --> 00:00:05,280
比如說在越戰的時候

114
00:00:05,280 --> 00:00:09,400
他們去measure的就是我們消滅了多少敵軍

115
00:00:09,400 --> 00:00:10,880
我們的損失是多少

116
00:00:10,919 --> 00:00:13,400
他們覺得如果我們消滅了足夠多的敵軍

117
00:00:13,400 --> 00:00:16,199
我們的這個戰爭就會走向勝利

118
00:00:16,600 --> 00:00:18,480
聽起來好像是有點道理的

119
00:00:19,000 --> 00:00:22,600
但是這第二部分就是disregard

120
00:00:22,640 --> 00:00:25,079
that which can't be easily measured

121
00:00:25,440 --> 00:00:27,960
就是你不能去measure的東西

122
00:00:27,960 --> 00:00:29,480
你就直接不管了

123
00:00:00,000 --> 00:00:02,000
第三步就是去assume

124
00:00:02,000 --> 00:00:04,240
What cannot be measured

125
00:00:04,240 --> 00:00:05,839
really isn't important

126
00:00:05,839 --> 00:00:07,519
然後第四步是

127
00:00:07,519 --> 00:00:08,960
What can be easily measured

128
00:00:08,960 --> 00:00:10,240
really doesn't exist

129
00:00:10,240 --> 00:00:12,240
就是開始追上眼睛說

130
00:00:12,240 --> 00:00:13,679
我不能measure的東西

131
00:00:13,679 --> 00:00:15,439
其實對我的決策不重要

132
00:00:15,439 --> 00:00:16,879
或者說根本就不存在

133
00:00:16,879 --> 00:00:21,120
在這裡邊就是越南人民對美國的憎恨

134
00:00:21,120 --> 00:00:23,199
越南人民的抵抗決心

135
00:00:23,199 --> 00:00:27,039
他比如說去轟炸更多的地方

136
00:00:27,039 --> 00:00:29,120
或者說是把軍事介入

137
00:00:00,000 --> 00:00:02,120
投入到一个更大的范围

138
00:00:02,120 --> 00:00:06,879
以及他去屠杀了更多的越南的士兵

139
00:00:06,879 --> 00:00:08,039
或者民众也好

140
00:00:08,039 --> 00:00:10,160
激起了越南人民的反抗决心

141
00:00:10,199 --> 00:00:13,439
其实是让他离他战争胜利的目标

142
00:00:13,439 --> 00:00:14,400
会越来越远

143
00:00:14,400 --> 00:00:16,719
从而他消灭多少敌军的

144
00:00:16,719 --> 00:00:18,640
那个可以被easily measure的

145
00:00:18,640 --> 00:00:20,839
那个goal变得不是那么重要的

146
00:00:20,879 --> 00:00:24,079
因为他激起了整个国家的反抗意志

147
00:00:24,120 --> 00:00:27,000
他陷入了人民战争的汪洋大海

148
00:00:27,039 --> 00:00:28,120
尸道寡住

149
00:00:28,160 --> 00:00:28,640
对吧

150
00:00:00,000 --> 00:00:03,240
抽離來看的話是一個很明顯的事情

151
00:00:03,240 --> 00:00:07,679
可是當時美國的國防部就犯了一個

152
00:00:07,679 --> 00:00:10,599
試圖scientifically來track

153
00:00:10,599 --> 00:00:12,119
這個world progress的東西

154
00:00:12,119 --> 00:00:14,839
因為他們是已經形成了一個匯報體系

155
00:00:14,839 --> 00:00:17,559
坐在辦公室裡的國防部長

156
00:00:17,559 --> 00:00:20,559
他說我需要用各種各樣的一套metrics

157
00:00:20,559 --> 00:00:22,679
來track我的戰爭的進展

158
00:00:22,679 --> 00:00:23,879
我是不是要贏了

159
00:00:23,879 --> 00:00:24,920
我可以去跟人匯報

160
00:00:24,920 --> 00:00:25,839
我可以去拿預算

161
00:00:25,839 --> 00:00:27,519
變成了challenge的一件事情以後

162
00:00:00,000 --> 00:00:02,879
他们就忽略了很多常识

163
00:00:02,879 --> 00:00:06,120
而去用这种所谓的data driven decisions

164
00:00:06,120 --> 00:00:08,599
最后导致决策的失败

165
00:00:09,599 --> 00:00:10,160
好

166
00:00:10,199 --> 00:00:12,679
这就是事情的另外一部分

167
00:00:12,720 --> 00:00:15,400
我们一定要知道这个决策里边

168
00:00:15,439 --> 00:00:19,000
到底有多少东西是可以被easily measure的

169
00:00:19,000 --> 00:00:22,320
然后也要respect那些不能被easily measure的

170
00:00:22,320 --> 00:00:24,800
并且给他们足够多的attention

171
00:00:24,800 --> 00:00:26,280
我们要运用我们的常识

172
00:00:26,280 --> 00:00:27,559
我们要运用我们的

173
00:00:27,559 --> 00:00:29,480
像那些视频说的knowhow的能力

174
00:00:00,000 --> 00:00:01,600
去把这些地方搞懂

175
00:00:01,600 --> 00:00:04,040
最后才能做出一个好的决策

176
00:00:04,040 --> 00:00:04,719
好的

177
00:00:04,719 --> 00:00:06,719
这期视频我们就到这

178
00:00:06,719 --> 00:00:09,480
我就是想跟各位数据科学家们

179
00:00:09,480 --> 00:00:12,119
去讲清楚这一个道理

180
00:00:12,119 --> 00:00:13,560
就是第一

181
00:00:13,560 --> 00:00:16,239
你要勇于在没有数据

182
00:00:16,239 --> 00:00:17,960
backup你的观点的情况下

183
00:00:17,960 --> 00:00:20,280
去对重要的决策发表意见

184
00:00:20,280 --> 00:00:24,079
并且你要去积极的寻找那些重要的决策

185
00:00:24,079 --> 00:00:25,559
而不只是把你的目光

186
00:00:25,559 --> 00:00:28,199
局限在可以被数据回答的决策

187
00:00:00,000 --> 00:00:02,480
第二对数据科学家也好

188
00:00:02,480 --> 00:00:03,359
对所有人也好

189
00:00:03,359 --> 00:00:06,639
尤其我觉得对数据科学家有义务去告诉所有人

190
00:00:06,719 --> 00:00:08,320
就是在这个决策中

191
00:00:08,320 --> 00:00:10,400
数据到底占了一个多大的比重

192
00:00:10,400 --> 00:00:12,599
有的决策可能就是一个很简单的

193
00:00:12,599 --> 00:00:14,519
用数据基本上就可以做的决策

194
00:00:14,560 --> 00:00:16,000
但是很多决策不是

195
00:00:16,000 --> 00:00:18,160
那这个时候你有必要跟大家说

196
00:00:18,199 --> 00:00:21,719
虽然大家对uncertainty是uncomfortable的

197
00:00:21,760 --> 00:00:23,839
可是我们数据只能measure30%

198
00:00:23,879 --> 00:00:25,800
剩下的70%就是uncertain

199
00:00:25,800 --> 00:00:27,079
就是数据回答不了的

200
00:00:00,000 --> 00:00:03,000
我们就是不应该去用数据去框定这部分

201
00:00:03,000 --> 00:00:05,160
但是我们要足够重视这一部分

202
00:00:05,160 --> 00:00:06,759
好 这期视频就到这

203
00:00:06,759 --> 00:00:08,400
谢谢大家 我们下期再见 拜拜

