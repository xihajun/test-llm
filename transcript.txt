[00:00:00,000 -> 00:00:01,340] 這不光是我
[00:00:01,340 -> 00:00:02,839] 我其實剛剛開始的時候
[00:00:02,839 -> 00:00:04,719] 我聊過很多就是這方面的研究
[00:00:04,719 -> 00:00:06,540] 然後甚至做大模型的人
[00:00:06,540 -> 00:00:07,679] 大家都非常的意外
[00:00:07,679 -> 00:00:09,140] 原來我們之前做的東西
[00:00:09,140 -> 00:00:11,380] 並不是在一個錯誤的方向移動
[00:00:11,380 -> 00:00:13,460] 我一直聽這個業界的朋友說
[00:00:13,460 -> 00:00:14,720] 大家也在想說
[00:00:14,720 -> 00:00:16,140] 下一步我怎麼樣讓
[00:00:16,140 -> 00:00:18,879] 讓降臨派來的更快的一種是
[00:00:18,879 -> 00:00:20,219] 對 你這個標準要求人
[00:00:20,219 -> 00:00:21,219] 你就想打那個神
[00:00:21,219 -> 00:00:22,339] 說不能打那麼多神
[00:00:22,339 -> 00:00:23,379] 反正我跟你
[00:00:23,379 -> 00:00:25,140] 天天寫代碼寫這麼好了
[00:00:25,140 -> 00:00:26,640] 還要要求我有音樂素養
[00:00:26,640 -> 00:00:29,140] 結果發現其實還是有音樂素養的
[00:00:00,000 -> 00:00:06,799] 对你想想我觉得这部剧很有意思的事情
[00:00:06,799 -> 00:00:09,039] 就以前我们为什么要有产品经理
[00:00:09,039 -> 00:00:11,000] 产品经理要写这个PRD
[00:00:11,000 -> 00:00:14,599] 不就是因为你所有设计出来的工具都是应该是
[00:00:14,599 -> 00:00:17,280] 按照你设定好的去工作的
[00:00:17,280 -> 00:00:21,600] 结果突然有一天会发现我们并不知道自己设计出来的工具
[00:00:21,600 -> 00:00:23,839] 它有哪一些可为不可为的
[00:00:23,839 -> 00:00:26,719] 我们自己完全没有一个不理解它的这个scope
[00:00:26,719 -> 00:00:29,760] 但这个回过头来说就是一方面
[00:00:00,000 -> 00:00:03,560] 我们可以说我们就应该从AGI的角度
[00:00:03,560 -> 00:00:05,519] 可能我们就应该接受Hydrogenation
[00:00:05,519 -> 00:00:08,439] 就是AGI的一部分
[00:00:08,439 -> 00:00:10,679] 但是我想可能从如意这边
[00:00:10,679 -> 00:00:16,440] 你们在实际我们能够实际的应用的场景中
[00:00:16,440 -> 00:00:19,719] 可以了解到Hydrogenation对你们的影响
[00:00:19,719 -> 00:00:21,120] 可能一个短期一个长期
[00:00:21,120 -> 00:00:22,480] 尤其是又在跟
[00:00:22,480 -> 00:00:24,800] 就好像我们前面提到的
[00:00:24,800 -> 00:00:28,239] Yes就是你把Hydrogenation尽可能地去掉了以后
[00:00:00,000 -> 00:00:02,600] 可能你牺牲了就是他的这个饭话呀
[00:00:02,600 -> 00:00:04,000] 这个creativity的能力
[00:00:04,000 -> 00:00:06,839] 那在具体的这个应用中
[00:00:06,839 -> 00:00:08,640] 你们是怎么做这样的一个平衡呢
[00:00:09,400 -> 00:00:10,800] 我觉得首先肯定应用里面是
[00:00:11,240 -> 00:00:12,880] 大家都不能接受和洛斯那事
[00:00:13,000 -> 00:00:14,759] 就是大家都觉得哦这是一个工具
[00:00:14,759 -> 00:00:16,000] 或者说这是一个bot
[00:00:16,000 -> 00:00:18,839] 然后你你如果去做一些很奇怪的事情
[00:00:19,120 -> 00:00:21,039] 看起来就像是这个
[00:00:21,120 -> 00:00:22,079] 比如说如果是一个客服
[00:00:22,079 -> 00:00:24,160] 就像是这个公司做了一件很奇怪的事情
[00:00:24,160 -> 00:00:24,640] 他们
[00:00:24,679 -> 00:00:26,559] 这是我们当时的话题三
[00:00:26,559 -> 00:00:28,120] 就是苹果怎么掉队的
[00:00:00,000 -> 00:00:02,500] 这是因为苹果一点不确定性都不能接受
[00:00:02,500 -> 00:00:03,060] 它不能接受
[00:00:03,060 -> 00:00:08,779] 那从应用上来讲肯定是大部分人或者现在我相信大部分人都是不太能接受
[00:00:09,099 -> 00:00:10,820] Hallucination的出现
[00:00:10,820 -> 00:00:13,019] 只是说这个程度不一样
[00:00:13,019 -> 00:00:15,820] 有些人说我能接受10%
[00:00:15,820 -> 00:00:17,300] 有些人就说我只能接受1%
[00:00:17,300 -> 00:00:18,899] 甚至有些人说我完全不能接受
[00:00:18,899 -> 00:00:19,899] 但是程度不一样
[00:00:19,899 -> 00:00:21,859] 但是这个问题大家不会有疑问
[00:00:21,859 -> 00:00:23,219] 就是说这件事情是好的
[00:00:23,219 -> 00:00:25,780] 大家都觉得这个东西是在应用上是一个不好的
[00:00:25,780 -> 00:00:27,940] 要去要解决的问题越少越好
[00:00:00,000 -> 00:00:03,399] 那我们就可以说如果解决它的一些思路
[00:00:03,399 -> 00:00:07,200] 就是我自己认为解决这个东西的思路
[00:00:07,200 -> 00:00:13,519] 就是你一把这个问题拆分成一些两个思路
[00:00:13,519 -> 00:00:14,720] 一个是拆分问题
[00:00:14,720 -> 00:00:18,440] 还一个是让提供一些context
[00:00:18,440 -> 00:00:20,280] 让他直接去reference这个context
[00:00:20,920 -> 00:00:21,960] 那我先说拆分
[00:00:21,960 -> 00:00:23,679] 就是前面有篇文章讲的
[00:00:23,679 -> 00:00:25,679] 就是说比如说你问GPT
[00:00:25,679 -> 00:00:29,239] 比如说9231到底是不是一个质数
[00:00:00,000 -> 00:00:04,019] 他可能会说是因为他可以拆分成x乘以y
[00:00:04,019 -> 00:00:07,000] 但是你真正你要问他x乘以y等于几
[00:00:07,000 -> 00:00:08,320] 是不是等于9231
[00:00:08,320 -> 00:00:10,199] 他会说不是他不等于9231
[00:00:10,199 -> 00:00:11,279] 是不是一个合数
[00:00:11,279 -> 00:00:15,080] 你可以反过来说他有可能也说是
[00:00:15,080 -> 00:00:16,320] 但这就是他们在认为
[00:00:16,320 -> 00:00:18,559] 就是说他在做这种推断的时候
[00:00:18,559 -> 00:00:21,039] 是一种Helucinogen来源
[00:00:21,039 -> 00:00:24,640] 更多的就是说因为他先生成了一个词是yes
[00:00:24,640 -> 00:00:28,640] 所以他之后所有生成的东西全都会向这个yes去移动
[00:00:00,000 -> 00:00:02,640] 因为这样整句话才通畅
[00:00:02,640 -> 00:00:03,759] 就跟人很像
[00:00:03,759 -> 00:00:04,440] 对吧
[00:00:04,440 -> 00:00:05,480] 你做了一个论点之后
[00:00:05,480 -> 00:00:07,200] 你最后很多时候你都会
[00:00:07,200 -> 00:00:09,320] 找方式方法把他收圆
[00:00:09,320 -> 00:00:10,640] 对找方法把他圆回来
[00:00:10,640 -> 00:00:12,359] 但是和人稍微不一样的地方就是
[00:00:12,359 -> 00:00:14,679] 人们是虽然我先说了结论
[00:00:14,679 -> 00:00:16,839] 但是我实际上先想了我的东西
[00:00:16,839 -> 00:00:18,760] 只是先想后说
[00:00:18,760 -> 00:00:21,079] 他更说是先说后想
[00:00:21,079 -> 00:00:22,719] 这个是一个我觉得一个区别
[00:00:22,719 -> 00:00:25,640] 那解决这个问题其实就是说明什么呢
[00:00:25,640 -> 00:00:28,800] 说明你可以把这个问题给它拆解开
[00:00:00,000 -> 00:00:02,000] 先让他去做一些想
[00:00:02,000 -> 00:00:05,599] 然后就像是引导他去做一些思考
[00:00:05,599 -> 00:00:05,879] 比如说
[00:00:05,879 -> 00:00:07,559] 然后再让他做结论
[00:00:07,559 -> 00:00:07,799] 对
[00:00:07,799 -> 00:00:10,839] 这我觉得这种是一种减少hallucination的一种方法
[00:00:10,839 -> 00:00:11,800] 是很有道理
[00:00:11,800 -> 00:00:14,119] 因为你比如说我把这个问题拆开
[00:00:14,119 -> 00:00:16,800] 拆成9231
[00:00:16,800 -> 00:00:20,079] 他比如说可以被哪个
[00:00:20,079 -> 00:00:21,679] 那他怎么拆分
[00:00:21,679 -> 00:00:23,519] 他怎么被成或者是不能被成
[00:00:23,519 -> 00:00:24,920] 然后成完了以后你问他
[00:00:24,920 -> 00:00:26,760] 你确不确定他可以被成
[00:00:26,760 -> 00:00:29,640] 你可以让他自己去批判你自己
[00:00:00,000 -> 00:00:01,480] 然后最后你得到一个结论
[00:00:01,480 -> 00:00:03,919] 它是不是一个质数还是一个合数
[00:00:03,919 -> 00:00:07,639] 它有可能就会获得一个更正确的答案
[00:00:07,639 -> 00:00:09,279] 这就是减少它的Holocene
[00:00:09,279 -> 00:00:10,800] 把它这个问题拆开
[00:00:10,800 -> 00:00:16,280] 让这也是另外一个就是整个现在大圆模型的一个发展
[00:00:16,280 -> 00:00:17,640] 就是说叫Chain of thought
[00:00:17,640 -> 00:00:19,800] 先让你去做思考
[00:00:19,800 -> 00:00:20,879] 先去做思考
[00:00:20,879 -> 00:00:21,760] 最后得到答案
[00:00:21,760 -> 00:00:24,640] 我觉得这种是一种解决Holocene的一个思路
[00:00:24,640 -> 00:00:28,480] 另外一个思路
[00:00:00,000 -> 00:00:01,919] 就是说我我这插一句吧
[00:00:01,919 -> 00:00:02,640] 就是我在
[00:00:02,640 -> 00:00:04,759] 就刚刚你的例子让我想起来
[00:00:04,759 -> 00:00:06,839] 我其实前段时间辅导我没数学
[00:00:06,839 -> 00:00:08,160] 我就是觉得就是
[00:00:08,160 -> 00:00:10,439] 人的习惯并不是step by step
[00:00:10,439 -> 00:00:12,640] 然后我们是经过了非常严格的训练以后
[00:00:12,640 -> 00:00:13,640] 才去做这件事情
[00:00:13,640 -> 00:00:15,359] 就是什么是我已知的条件
[00:00:15,720 -> 00:00:16,839] 什么是问题
[00:00:16,839 -> 00:00:18,039] 然后根据已知的条件
[00:00:18,039 -> 00:00:19,440] 如何一步一步的推导
[00:00:19,440 -> 00:00:20,359] 一步一步推导
[00:00:20,359 -> 00:00:22,320] 就是说已知的推导出
[00:00:22,320 -> 00:00:24,079] 加上一些东西推导出来已知的
[00:00:24,079 -> 00:00:26,960] 就是每一步我们都能确定这一步一定正确
[00:00:26,960 -> 00:00:28,239] 然后才会继续往下
[00:00:00,000 -> 00:00:03,580] 但是這不是一個天生的思考習慣
[00:00:03,580 -> 00:00:05,000] 它是培養出來的
[00:00:05,000 -> 00:00:07,700] 而和大模型可能需要這樣的培養
[00:00:07,700 -> 00:00:09,119] 從此也說出來
[00:00:09,119 -> 00:00:11,500] 就是很多時候人類覺得簡單的問題
[00:00:11,500 -> 00:00:12,619] 其實不那麼簡單
[00:00:12,619 -> 00:00:15,699] 包括怎麼樣子確定一個信息的真實
[00:00:15,699 -> 00:00:17,699] 這個其實是一個挺難的事情
[00:00:17,699 -> 00:00:20,579] 對 我們經過了背後的很多訓練
[00:00:20,579 -> 00:00:23,300] 大圓模型我相信它的這個
[00:00:23,300 -> 00:00:24,960] 思考能力是在的
[00:00:24,960 -> 00:00:26,300] 但是它需要這些人
[00:00:26,300 -> 00:00:28,079] 需要人去教它或調整好
[00:00:28,079 -> 00:00:29,539] 需要引導的一個過程
[00:00:00,000 -> 00:00:09,560] 其实你刚才讲到那个例子,就让我引申想到你刚才说你教你妹妹学数学,我觉得是一个很好的例子。
[00:00:09,560 -> 00:00:22,600] 我觉得其实你想想我们人类大部分的所谓的一些best practice,我们所谓的这种professional的这种训练本质都是把一些,你想所有professional service,什么咨询,对吧?
[00:00:00,000 -> 00:00:04,559] 什么律师审计所有这些本质不就是把一个抽象问题
[00:00:04,559 -> 00:00:07,200] 但是他用这种但是用一些这些
[00:00:07,200 -> 00:00:11,039] best practice的方法把他给固化下来
[00:00:11,039 -> 00:00:12,279] 比方说像这个咨询
[00:00:12,279 -> 00:00:13,720] 你怎么定一个这个strategy的
[00:00:13,720 -> 00:00:15,599] 为什么搞那么多framework什么的
[00:00:15,599 -> 00:00:17,760] 其实这个跟我比方说我最近
[00:00:17,760 -> 00:00:20,679] 因为接触很多这个startup
[00:00:20,679 -> 00:00:22,559] 我就很明显感觉其实在
[00:00:22,559 -> 00:00:25,280] 因为在硅谷我觉得这个所谓的创业的
[00:00:00,000 -> 00:00:05,519] 这个所谓方法论本身已经被教育的挺完善了
[00:00:05,519 -> 00:00:06,719] 而什么是一个创意方法
[00:00:06,719 -> 00:00:08,480] 你想我们这样听到了什么
[00:00:08,480 -> 00:00:09,839] 你要先有一个MVP
[00:00:09,839 -> 00:00:10,080] 对吧
[00:00:10,080 -> 00:00:13,759] 然后你要去测试你的这个product market fit等等的
[00:00:13,759 -> 00:00:16,160] 不也就是一个拆解的一个过程
[00:00:16,160 -> 00:00:17,440] 而你会发现其实
[00:00:18,000 -> 00:00:21,280] 其实人学这个其实并不是那么容易的
[00:00:21,280 -> 00:00:23,120] 因为很多时候它是比较反直觉的
[00:00:23,120 -> 00:00:25,160] 有时候人是很想说
[00:00:25,160 -> 00:00:29,239] 根据我的直觉一下就冲到那个去把这公司给scale up的程度
[00:00:00,000 -> 00:00:02,759] 这也让我想到最近我应该是听一些
[00:00:02,759 -> 00:00:05,280] 我一直听业界朋友说
[00:00:05,280 -> 00:00:08,000] 大家也在想说下一步我怎么样让
[00:00:08,000 -> 00:00:10,560] 让佳林派来的更快的一种
[00:00:10,560 -> 00:00:15,839] 是说我怎么样把全世界这些专家的这些能力
[00:00:15,839 -> 00:00:16,879] 不只是他们的知识
[00:00:16,879 -> 00:00:17,800] 因为我们讲到很多时候
[00:00:17,800 -> 00:00:19,079] 讲到information retrieval的时候
[00:00:19,079 -> 00:00:19,920] 讲都是知识
[00:00:19,920 -> 00:00:22,359] 其实更他们更重要的是
[00:00:22,359 -> 00:00:23,920] Einstein是怎么思考的
[00:00:23,920 -> 00:00:24,480] 对吧
[00:00:24,480 -> 00:00:26,160] Elia是怎么思考的
[00:00:26,160 -> 00:00:29,120] 就是他怎么把这些专家的思考方式
[00:00:00,000 -> 00:00:01,600] 如何教给大陆人
[00:00:01,600 -> 00:00:03,560] 我觉得真的如果有了这个的话
[00:00:03,560 -> 00:00:04,719] 这个真是非常的
[00:00:04,719 -> 00:00:05,559] 就更可怕
[00:00:05,559 -> 00:00:06,679] 就每个人
[00:00:06,679 -> 00:00:07,839] 我就是在等他开放了
[00:00:07,839 -> 00:00:08,720] 上面那一步的时候
[00:00:08,720 -> 00:00:09,640] 去认真教教他
[00:00:09,640 -> 00:00:11,039] 然后我就可以降临了
[00:00:11,039 -> 00:00:12,320] 这也就是人类老师
[00:00:12,320 -> 00:00:14,279] 经常最应该教你的东西
[00:00:14,279 -> 00:00:15,400] 不是教你知识
[00:00:15,400 -> 00:00:16,920] 而是教你这些方法
[00:00:16,920 -> 00:00:18,079] 和思维的方法
[00:00:18,079 -> 00:00:19,800] 这个方法论怎么教呢
[00:00:19,800 -> 00:00:21,239] 关注我的视频
[00:00:21,239 -> 00:00:22,879] 很多
[00:00:22,879 -> 00:00:24,760] 真的
[00:00:24,760 -> 00:00:25,839] 现在我们看到就是
[00:00:25,839 -> 00:00:28,879] 教这个方法论有什么
[00:00:00,000 -> 00:00:02,720] 从最简单的可能就一些prompt这个方法
[00:00:02,720 -> 00:00:04,799] 到可能更深层方有什么
[00:00:04,799 -> 00:00:07,440] 现在有什么一些比较好的这个做法
[00:00:07,440 -> 00:00:09,880] 我觉得还是回到那个就是chain of thought
[00:00:09,880 -> 00:00:12,839] 就比如说他们去做的时候就是给了你一个问题
[00:00:12,839 -> 00:00:16,960] 然后让人去写一步一步你是怎么得到最后答案的
[00:00:16,960 -> 00:00:18,359] 然后让模型生成的时候
[00:00:18,359 -> 00:00:21,399] 你也去生成一步一步是怎么得到最后答案的
[00:00:21,399 -> 00:00:24,519] 其实这就像因为我相信XHPT像这种模型
[00:00:24,519 -> 00:00:27,719] 他们肯定用了很多这样的训练方法
[00:00:27,719 -> 00:00:28,920] 已经在当中了
[00:00:00,000 -> 00:00:02,439] 那我们只是说可能需要更多的专家
[00:00:02,439 -> 00:00:06,080] 更多领域去做更复杂的一些思考的方式
[00:00:06,080 -> 00:00:07,480] 所以这种东西
[00:00:07,480 -> 00:00:10,880] 甚至有些人就是说你甚至可以使得
[00:00:10,880 -> 00:00:12,519] XHPT去做这种
[00:00:12,839 -> 00:00:14,720] ChainMsort这种思考
[00:00:14,720 -> 00:00:16,879] 你就直接就在你的prompt最后加一个
[00:00:17,480 -> 00:00:19,199] think out loud step by step
[00:00:19,480 -> 00:00:21,399] 他就会去做我们做
[00:00:21,559 -> 00:00:24,199] 然后你可以跟他甚至说
[00:00:24,320 -> 00:00:26,559] think out loud step by step then answer
[00:00:26,960 -> 00:00:28,800] 他就会去做这样的事情
[00:00:00,000 -> 00:00:02,439] 那这也是这又回到刚那个就是说
[00:00:02,439 -> 00:00:06,360] 这也是一种帮助解决hallucination的一种方法
[00:00:06,360 -> 00:00:09,199] 但是实际上虽然解决了hallucination
[00:00:09,199 -> 00:00:10,839] 可能也会让它效果更好
[00:00:10,839 -> 00:00:13,000] 但是实际上同时我们也生成了更多东西
[00:00:13,000 -> 00:00:14,880] 同时也变得更贵
[00:00:14,880 -> 00:00:16,760] 这也是它的一些其他的问题吧
[00:00:16,760 -> 00:00:19,359] 我们肯定希望他最开始一下就能告诉你
[00:00:19,359 -> 00:00:20,719] 正确的答案是最好的
[00:00:20,719 -> 00:00:23,719] 对我刚刚那句话听上去是玩笑
[00:00:23,719 -> 00:00:24,559] 其实不是玩笑
[00:00:24,559 -> 00:00:28,000] 就是我的视频里面很多东西都是可以直接给大家
[00:00:00,000 -> 00:00:02,560] 结论的 甚至可以包装一下结论
[00:00:02,560 -> 00:00:03,759] 把这个结论变得很动听
[00:00:03,759 -> 00:00:05,919] 然后可以被更多人的一下子理解
[00:00:05,919 -> 00:00:07,719] 其实大家也想的就是要那个结论
[00:00:07,719 -> 00:00:10,480] 但是我会花更长的时间去解释我的思考过程
[00:00:10,480 -> 00:00:12,199] 推理过程什么东西是事实
[00:00:12,199 -> 00:00:13,560] 什么东西是我的假设
[00:00:13,560 -> 00:00:16,079] 然后什么东西是我如何得到这个结论的
[00:00:16,079 -> 00:00:18,960] 我觉得这个习惯挺重要
[00:00:18,960 -> 00:00:20,679] 包括在布置工作的时候
[00:00:20,679 -> 00:00:23,160] 我也会尽量把context目标
[00:00:23,160 -> 00:00:24,600] 然后我是怎么想的
[00:00:24,600 -> 00:00:26,320] 但是我的思考只是一个举例
[00:00:26,320 -> 00:00:28,320] 你应该去在什么角度上去
[00:00:28,320 -> 00:00:29,839] perspective很重要
[00:00:00,000 -> 00:00:01,199] 去思考这个问题
[00:00:01,199 -> 00:00:03,040] 我都会尽可能多的说
[00:00:03,040 -> 00:00:04,480] 但是对
[00:00:04,480 -> 00:00:06,360] 就是不是所有人都appreciate这一点
[00:00:06,360 -> 00:00:09,039] 我觉得包括我在使用GPT的时候
[00:00:09,039 -> 00:00:11,439] 就很明显能感觉到所谓的prompt
[00:00:11,439 -> 00:00:13,599] 就是你怎么样子跟人把一件事情
[00:00:13,599 -> 00:00:14,960] 讲得越来越清楚也好
[00:00:14,960 -> 00:00:17,039] 我就觉得很多人分享的例子
[00:00:17,039 -> 00:00:19,000] 就是说你给我干一件A
[00:00:19,000 -> 00:00:21,199] 我说这种老板本身就是最容易烦的
[00:00:21,199 -> 00:00:22,839] 就是他其实根本就没有讲
[00:00:22,839 -> 00:00:23,640] 他到底要干嘛
[00:00:23,640 -> 00:00:24,920] 他问的问题都问不清楚
[00:00:24,920 -> 00:00:26,719] 他讲的东西讲不清楚的情况下
[00:00:26,719 -> 00:00:28,519] 他说大模型给他的答案不好
[00:00:28,519 -> 00:00:28,960] 对
[00:00:00,000 -> 00:00:03,040] 然后我说那我给你演示一下我怎么问
[00:00:03,040 -> 00:00:06,080] 然后果然就简单多加几句话背景
[00:00:06,080 -> 00:00:07,000] 我是一个什么样的人
[00:00:07,000 -> 00:00:08,279] 我需要得到一个什么样的答案
[00:00:08,279 -> 00:00:09,720] 我要拿这个问题干什么
[00:00:09,720 -> 00:00:13,759] 模型马上就给你一个非常好的这个可用的answer
[00:00:13,759 -> 00:00:16,440] 嗯对我在使用的过程中也会发现
[00:00:16,440 -> 00:00:19,239] 就是我会先跟他去讲
[00:00:19,239 -> 00:00:20,440] 尤其code interpreter对吧
[00:00:20,440 -> 00:00:22,239] 我让他做分析的时候
[00:00:22,239 -> 00:00:24,239] 分析完了以后会发现
[00:00:24,239 -> 00:00:27,480] 哦其实我更好的解决这个问题的思路是什么
[00:00:27,480 -> 00:00:29,039] 然后重开一个chat
[00:00:00,000 -> 00:00:02,879] 然后我再把这个按照这个思路再去做
[00:00:02,879 -> 00:00:04,200] 然后再重开一个chat
[00:00:04,200 -> 00:00:05,799] 然后再去refine我的思路
[00:00:05,799 -> 00:00:08,439] 我发现用它去refine我的思路以后再去
[00:00:08,439 -> 00:00:10,080] 一步一步和它一起做
[00:00:10,080 -> 00:00:11,880] 每次效果都会提升很多很多
[00:00:14,359 -> 00:00:15,880] 那在就是
[00:00:15,880 -> 00:00:18,839] 我觉得刚刚其实还聊到就是你们所做的
[00:00:18,839 -> 00:00:22,960] 这一个就是在大实际这个落地的这个过程中
[00:00:22,960 -> 00:00:25,359] 就是刚才也提到就是说你怎么把这个
[00:00:25,359 -> 00:00:28,239] 就是怎么把它我们所期望的
[00:00:00,000 -> 00:00:06,280] 它的这种泛化能力这个推理能力跟它在就是你要落地时候避免它的
[00:00:06,280 -> 00:00:11,119] 你要控制它的一个可靠性中间怎么达到一个这个balance
[00:00:11,119 -> 00:00:12,880] 但是我在想这个问题的时候
[00:00:12,880 -> 00:00:17,280] 我在想当当我已经有了一个那么具体的场景和任务的时候
[00:00:17,280 -> 00:00:19,760] 可能本身这个东西就对吧
[00:00:19,760 -> 00:00:26,399] 它就不是一个就也许我们就不应该期待用代模型的这种这种方式来去来来去给它解决
[00:00:26,399 -> 00:00:29,199] 所以说是不是就像我们刚才讨论的
[00:00:00,000 -> 00:00:04,160] 任何我们现在能已经能够很好的去定义的一个场景
[00:00:05,040 -> 00:00:09,720] 就是都不应该是我们去第一个想到用应用大模型的这个场景
[00:00:09,720 -> 00:00:12,439] 可能更多的都是在我们现在还没有看到的这个地方
[00:00:13,119 -> 00:00:15,599] 我觉得也是几个思路
[00:00:15,599 -> 00:00:19,160] 就是一方面就是如果你有一个固定的
[00:00:19,359 -> 00:00:23,000] 像decision tree这样的一个设定的话
[00:00:23,199 -> 00:00:25,679] 那我相信这个东西一定是更可好的
[00:00:25,960 -> 00:00:27,239] 就是没什么好说的
[00:00:27,239 -> 00:00:28,519] 就一定是更可好的
[00:00:00,000 -> 00:00:02,319] 然后你一定能得到你想要的结果
[00:00:02,319 -> 00:00:04,919] 如果你的问题就适用于这种情况下
[00:00:04,919 -> 00:00:05,879] 你就应该去这么做
[00:00:05,879 -> 00:00:08,199] 而不是应该去为了使用大模型而使用大模型
[00:00:08,519 -> 00:00:13,080] 但这并不代表说所有的这种
[00:00:13,080 -> 00:00:16,719] 就是说Task都是可以被用这种
[00:00:16,719 -> 00:00:19,280] 这种这种decision tree的方式去表达的
[00:00:19,280 -> 00:00:23,160] 甚至就是说你认为这个东西是按一种decision tree的方式去表达
[00:00:23,160 -> 00:00:25,160] 但也许它在执行过程当中
[00:00:25,160 -> 00:00:29,800] 并不一定要去完全按照你给它提供的这个tree去完成
[00:00:00,000 -> 00:00:04,160] 你允许他去做一些推断或者去做一些判断
[00:00:04,160 -> 00:00:08,880] 比如说你帮助别人去reset password
[00:00:09,199 -> 00:00:11,199] 你可能就是说如果是这样的话做这个
[00:00:11,199 -> 00:00:12,359] 如果是这样的话做这个
[00:00:12,359 -> 00:00:13,880] 并且我可以提供一篇文章
[00:00:13,880 -> 00:00:16,120] 就讲我的password到底会是什么样的
[00:00:16,120 -> 00:00:18,320] 或者我提供一段code这篇文章是什么样的
[00:00:18,320 -> 00:00:20,399] 可能用户在问的时候
[00:00:20,399 -> 00:00:21,839] 可能交流的过程当中
[00:00:21,839 -> 00:00:24,440] 他遇见的一些情况不是你在写的e-files当中
[00:00:24,440 -> 00:00:26,519] 或者是他提供了一些额外的信息
[00:00:26,519 -> 00:00:28,000] 你就不需要去专门做一些模型
[00:00:00,000 -> 00:00:04,679] 就是说我要提取这个信息放在这个地方去做这件事情
[00:00:05,040 -> 00:00:05,759] 然后去完成
[00:00:05,759 -> 00:00:08,000] 我觉得这还是两个好处
[00:00:08,000 -> 00:00:09,160] 第一个好处就是
[00:00:09,439 -> 00:00:12,039] 你的问题如果描述的很清楚
[00:00:12,039 -> 00:00:13,880] 给了他一些解决方法的例子的话
[00:00:13,880 -> 00:00:15,800] 也许他解决的是整个问题
[00:00:15,800 -> 00:00:18,280] 而并不是你去设定的这么几个例子
[00:00:18,559 -> 00:00:22,679] 另外一个就是你可能让整个设置这套workflow
[00:00:22,800 -> 00:00:25,600] 或者这个东西变得更简化了
[00:00:25,800 -> 00:00:27,399] 原来你可能要设计的东西是
[00:00:27,600 -> 00:00:29,280] 我要不光要设计这个workflow
[00:00:00,000 -> 00:00:04,000] 我要设计一些比如说name entity的extraction
[00:00:04,000 -> 00:00:07,080] 我要设置一些东西如何把东西
[00:00:07,080 -> 00:00:09,439] 这个entity和这个entitylink在一起
[00:00:09,439 -> 00:00:11,240] 有很多这样的components
[00:00:11,240 -> 00:00:12,839] 可能这些东西就会被简化
[00:00:12,839 -> 00:00:18,719] 也会降低一些automation的entry barrier
[00:00:18,719 -> 00:00:22,600] 有什么补充
[00:00:22,600 -> 00:00:24,800] 补充或者总结两个点
[00:00:24,800 -> 00:00:25,760] 就是一个
[00:00:25,760 -> 00:00:29,199] 我觉得两个非常在我在使用的过程中
[00:00:00,000 -> 00:00:01,120] 非常有用的点
[00:00:01,120 -> 00:00:05,000] 一个就是去帮助我完成那些TDS work
[00:00:05,120 -> 00:00:07,280] 就是我说我要干这个
[00:00:07,280 -> 00:00:08,320] 然后写多少SQL
[00:00:08,320 -> 00:00:09,000] 然后给我写了
[00:00:09,000 -> 00:00:10,199] 然后改一改就好了
[00:00:10,320 -> 00:00:11,240] 虽然可能有问题
[00:00:11,240 -> 00:00:13,039] 但是它极大地很要我的效率
[00:00:13,119 -> 00:00:14,880] 但是更大的惊喜
[00:00:14,880 -> 00:00:17,800] 或者serendipity的时刻
[00:00:17,839 -> 00:00:18,960] 都是在于它告诉了我
[00:00:18,960 -> 00:00:20,079] 完全不知道的东西
[00:00:20,519 -> 00:00:23,079] 比如说我当时做一个分析
[00:00:23,079 -> 00:00:25,160] 要分析网站的用户的
[00:00:25,160 -> 00:00:27,399] 这个session journey
[00:00:27,480 -> 00:00:29,519] 然后他就说我们做这个东西
[00:00:00,000 -> 00:00:01,600] 应该用3K diagram
[00:00:01,600 -> 00:00:03,480] 然后我不知道3K diagram是什么
[00:00:03,480 -> 00:00:04,240] 然后我一查
[00:00:04,240 -> 00:00:06,040] 然后发现果然就是我需要的一个东西
[00:00:06,360 -> 00:00:07,639] 对就不使用它的话
[00:00:07,639 -> 00:00:11,919] 就是我自己的认知范围是不会想到那个东西的
[00:00:11,919 -> 00:00:16,440] 就是你刚刚说的真正的值得做的
[00:00:16,440 -> 00:00:18,679] 在一个确定性的东西下
[00:00:18,679 -> 00:00:19,960] 它一定有一些重复的工作
[00:00:19,960 -> 00:00:22,839] 那因为大学模型它有这个re-signing的能力
[00:00:22,839 -> 00:00:24,960] 再加上它能调用算力是吧
[00:00:24,960 -> 00:00:29,320] 现在直接用拆JPG的这个方式未必能特别好的做
[00:00:00,000 -> 00:00:02,240] 但是API加上一些东西
[00:00:02,240 -> 00:00:06,200] 我相信能把很多TDS work给很好地解决掉
[00:00:06,200 -> 00:00:07,799] 但是另外就是他
[00:00:07,799 -> 00:00:09,720] 因为他的背后的智能
[00:00:09,720 -> 00:00:12,160] 我相信他能告诉我们很多不知道的东西
[00:00:12,160 -> 00:00:14,439] 那我好奇卢毅有没有
[00:00:14,439 -> 00:00:15,800] 你有没有经历过那一个
[00:00:15,800 -> 00:00:17,760] 就是刚说的serendipity的这个时刻
[00:00:17,760 -> 00:00:20,559] 就大模型就经验到你的这个时候
[00:00:20,559 -> 00:00:22,440] 因为你在这个行做了那么多年
[00:00:22,440 -> 00:00:24,199] 我想经验你可能更难一些
[00:00:24,199 -> 00:00:26,760] 我这个其实说起来
[00:00:26,760 -> 00:00:28,079] 我当时跟你们吃完饭的时候
[00:00:00,000 -> 00:00:02,520] 我在想在聊这个问题
[00:00:02,520 -> 00:00:06,519] 就是其实也跟我们的模型评估啊什么什么有相关
[00:00:06,519 -> 00:00:08,800] 因为我一开始接触模型的时候更多的是
[00:00:08,800 -> 00:00:11,800] 最开始是LSTM类似这样的模型
[00:00:11,800 -> 00:00:15,640] 然后是BERT类型的模型
[00:00:15,640 -> 00:00:16,800] 然后BERT变得更好了
[00:00:16,800 -> 00:00:18,120] ROBERT类型的模型
[00:00:18,120 -> 00:00:18,960] T5
[00:00:18,960 -> 00:00:21,440] 然后后面是OPT就变得更大
[00:00:21,440 -> 00:00:25,359] 就是说我们当时觉得一个B点已经很大了
[00:00:25,359 -> 00:00:27,960] 然后一个B点很大了
[00:00:00,000 -> 00:00:02,060] 因为我如果没记错的话
[00:00:02,060 -> 00:00:04,980] 就可能是BirdBase就是500M个
[00:00:04,980 -> 00:00:07,019] 然后你就说两个那么大
[00:00:07,019 -> 00:00:07,620] 然后
[00:00:07,620 -> 00:00:08,820] 两个那么大
[00:00:08,820 -> 00:00:10,339] 然后后来又在想
[00:00:10,339 -> 00:00:11,019] 就是说
[00:00:11,019 -> 00:00:12,140] 那后来就说那个
[00:00:12,140 -> 00:00:13,619] 那有更大
[00:00:13,619 -> 00:00:15,060] 就比如说我们现在就还有
[00:00:15,060 -> 00:00:17,219] 就他们叫HuggingFace
[00:00:17,219 -> 00:00:18,500] 还让他们做的Bloom
[00:00:18,500 -> 00:00:20,219] 就觉得好大
[00:00:20,219 -> 00:00:21,820] 但是就在使用过程当中
[00:00:21,820 -> 00:00:24,699] 多少的还是会出现一些Glitch
[00:00:24,699 -> 00:00:25,620] 就会觉得
[00:00:25,620 -> 00:00:25,980] 哦
[00:00:25,980 -> 00:00:27,579] 我可以让你去完成这些事情
[00:00:27,579 -> 00:00:28,260] 可以做这件事情
[00:00:00,000 -> 00:00:04,839] 但是我并没有觉得很怎么说呢
[00:00:04,839 -> 00:00:08,400] 我能够把它作为一个人去对待
[00:00:08,919 -> 00:00:10,039] 去看这件事情
[00:00:10,039 -> 00:00:12,560] 然后真正开始使用GPT的时候
[00:00:12,560 -> 00:00:15,800] 一开始聊天其实并没有特别的那种感觉
[00:00:15,800 -> 00:00:16,640] 你说GPT3?
[00:00:16,640 -> 00:00:18,160] GPT3.5刚出来的时候
[00:00:18,160 -> 00:00:21,480] 就是刚刚跟他聊天就觉得很酷
[00:00:21,480 -> 00:00:23,559] 然后没有什么特别
[00:00:23,559 -> 00:00:25,280] 我真正用的时候就是感觉
[00:00:25,280 -> 00:00:29,320] 我当时也是我是非常怎么说呢
[00:00:00,000 -> 00:00:02,000] 我不能说是technology late adopter
[00:00:02,000 -> 00:00:04,400] 也是反正就是对这种东西有很多
[00:00:05,400 -> 00:00:08,000] 质疑怀疑
[00:00:08,000 -> 00:00:10,000] 有很多hold back
[00:00:10,000 -> 00:00:12,599] 因为我就想我看到了这么多大模型
[00:00:12,599 -> 00:00:14,000] 我用了这么多大模型
[00:00:14,400 -> 00:00:17,699] 为什么你出来这个东西也没说比别人怎么样
[00:00:17,699 -> 00:00:19,600] 为什么他就一定会更好呢
[00:00:19,600 -> 00:00:22,500] 然后但是我后面
[00:00:22,500 -> 00:00:25,100] 然后我想去做一些检索方面的工作
[00:00:25,100 -> 00:00:27,100] 然后就觉得确实不行
[00:00:27,100 -> 00:00:28,600] 我当时还跟朋友试了一个东西
[00:00:00,000 -> 00:00:02,720] 就是说你能不能把一个单词反过来拼
[00:00:02,720 -> 00:00:04,400] 对一开始不行
[00:00:04,400 -> 00:00:06,360] 是不可以的
[00:00:06,360 -> 00:00:08,759] 然后对现在又可以了
[00:00:08,759 -> 00:00:12,160] 一开始我就觉得你这样可能是因为你的Tokenizer有些问题
[00:00:12,160 -> 00:00:14,359] 然后类似这样东西
[00:00:14,359 -> 00:00:16,640] 我就觉得当时就有一种不屑的那种感觉
[00:00:16,640 -> 00:00:21,079] 到后来我觉得更多的是用它去做一些
[00:00:21,079 -> 00:00:23,679] 比如说跟coding相关的
[00:00:23,679 -> 00:00:25,640] 我说我需要做这样事情的时候
[00:00:25,640 -> 00:00:27,120] 我发现它竟然能够生成
[00:00:27,120 -> 00:00:28,280] 竟然比我想的还要好
[00:00:00,000 -> 00:00:01,879] 那时候我就觉得非常的意外
[00:00:01,879 -> 00:00:03,960] 我就前两天我还试用了一个东西
[00:00:03,960 -> 00:00:04,839] 我就一直在用
[00:00:04,839 -> 00:00:06,360] 我就觉得为什么我做这种事
[00:00:06,360 -> 00:00:07,679] 我不能用Google去搜索
[00:00:07,679 -> 00:00:09,359] 我相信Google一定是更好的
[00:00:09,359 -> 00:00:09,919] 就是能够
[00:00:09,919 -> 00:00:11,800] 你不要那种感觉吧
[00:00:11,800 -> 00:00:12,240] 对吧
[00:00:12,240 -> 00:00:13,560] 然后我就搜索了一段时间
[00:00:13,560 -> 00:00:15,039] 我当时做的事情也很简单
[00:00:15,039 -> 00:00:17,879] 就是我有一台Linux的机器
[00:00:17,879 -> 00:00:20,079] 我想从我的Mac上面Remote进去
[00:00:20,079 -> 00:00:21,480] 但是我不想SS进去
[00:00:21,480 -> 00:00:22,679] 但是我不想输密码
[00:00:22,960 -> 00:00:26,280] 然后如何能够设置这台电脑
[00:00:26,280 -> 00:00:27,800] 使得我SS进去的时候
[00:00:27,800 -> 00:00:28,920] 不用每次都输入密码
[00:00:00,000 -> 00:00:01,280] 然后我就搜
[00:00:01,280 -> 00:00:04,040] 他输了这个task
[00:00:04,040 -> 00:00:06,639] 下一个key就行
[00:00:06,639 -> 00:00:08,240] 下一个key
[00:00:08,240 -> 00:00:09,199] 然后再一个match
[00:00:09,199 -> 00:00:11,800] 你要生成一个sskey
[00:00:11,800 -> 00:00:12,800] 然后你就把你的publickey
[00:00:12,800 -> 00:00:14,240] copy到他的一个特别的位置
[00:00:14,240 -> 00:00:16,760] 用什么sscopyid什么之类的
[00:00:16,760 -> 00:00:18,280] 我当时我搜了大概一天
[00:00:18,280 -> 00:00:19,440] 对我搜了好长时间
[00:00:19,440 -> 00:00:20,600] 然后我就觉得非常的frustrated
[00:00:20,600 -> 00:00:21,440] 然后我就说
[00:00:21,440 -> 00:00:22,559] 我就用GPC试一下
[00:00:22,559 -> 00:00:24,640] 然后他就把step-by-step发给我了
[00:00:24,640 -> 00:00:26,199] 然后我一run完三个command
[00:00:26,199 -> 00:00:26,960] 然后就结束了
[00:00:26,960 -> 00:00:28,199] 我那个时候我就觉得
[00:00:00,000 -> 00:00:02,359] 就类似这当然不是第一次
[00:00:02,359 -> 00:00:04,120] 但是像类似这样的这种时刻
[00:00:04,120 -> 00:00:06,200] 就让人觉得非常的意外
[00:00:06,200 -> 00:00:07,080] 就是想到
[00:00:07,080 -> 00:00:07,639] 哦
[00:00:07,639 -> 00:00:11,560] 其实这种模型原来背后它只是为了生成下一个token
[00:00:11,560 -> 00:00:13,039] 它竟然能够做到这个程度
[00:00:13,039 -> 00:00:15,560] 就让人觉得实际上是很意外的
[00:00:15,560 -> 00:00:17,760] 我觉得很这不光是我
[00:00:17,760 -> 00:00:19,199] 我其实刚刚开始的时候
[00:00:19,199 -> 00:00:21,120] 我聊过很多就是这方面的研究
[00:00:21,120 -> 00:00:22,879] 然后甚至做大模型的人
[00:00:22,879 -> 00:00:24,039] 大家都非常的意外
[00:00:24,039 -> 00:00:26,679] 就说竟然能够做到这个程度
[00:00:26,679 -> 00:00:29,120] 原来我们做的这些事情
[00:00:00,000 -> 00:00:01,840] 原來我們之前做的東西
[00:00:01,840 -> 00:00:03,879] 並不是在一個錯誤的方向移動
[00:00:03,879 -> 00:00:07,540] 而是我們真的是不夠多 不夠大
[00:00:07,540 -> 00:00:08,580] 做的東西不夠細
[00:00:08,580 -> 00:00:10,380] 還有細節沒有夠好
[00:00:10,380 -> 00:00:11,539] 不夠有潛質
[00:00:11,539 -> 00:00:12,839] 我覺得就是有潛質
[00:00:12,839 -> 00:00:15,039] 就是你一定要相信這個總統
[00:00:15,039 -> 00:00:17,420] 就是人聰明了可以解決一些問題
[00:00:17,420 -> 00:00:18,420] 大家不相信這個
[00:00:18,420 -> 00:00:20,660] 大家說我必須要教他做炒這個
[00:00:20,660 -> 00:00:21,420] 他才能做好
[00:00:21,420 -> 00:00:23,120] 對 我覺得就是
[00:00:23,120 -> 00:00:25,620] 你其實你很難去責怪
[00:00:25,620 -> 00:00:26,500] 當時那一刻
[00:00:26,500 -> 00:00:27,879] 我不責怪大家
[00:00:00,000 -> 00:00:07,360] 因为真的很难相信我们只是通过这些
[00:00:07,360 -> 00:00:09,919] 我们所谓叫 boring tricks
[00:00:09,919 -> 00:00:11,519] 就可以完成这件事情
[00:00:11,519 -> 00:00:13,119] 我觉得是需要这样说
[00:00:13,119 -> 00:00:16,239] 很强的信仰去完成
[00:00:16,239 -> 00:00:18,000] 我相信他们里边肯定有哲学家
[00:00:18,000 -> 00:00:20,239] 就是这件事情从哲学上一定是make sense的
[00:00:20,239 -> 00:00:22,760] 但只是没有任何的神经科学
[00:00:22,760 -> 00:00:26,920] 人质科学能证明这个事情在人身上也是这样子的而已
[00:00:00,000 -> 00:00:07,080] 有一个Amazon过去openai的人,他就是researcher,他就是学哲学的
[00:00:07,080 -> 00:00:11,400] 其实我发现有不少Machine Learning的人以前是真是学哲学的
[00:00:11,400 -> 00:00:15,880] 其实刚才你们讲的就是这样,我想到即使是这样
[00:00:15,880 -> 00:00:20,600] 我想现在可能被他的能力经验的人已经越来越多了
[00:00:20,600 -> 00:00:22,800] 但是我们竟然还是会听到大家说
[00:00:22,800 -> 00:00:29,320] 他是可以很好,不一定,但是这个就真的代表他理解了嘛
[00:00:00,000 -> 00:00:03,200] 就是当时这个是否就代表intelligence
[00:00:03,200 -> 00:00:03,680] 对啊
[00:00:03,680 -> 00:00:04,599] 就很想我记得
[00:00:04,599 -> 00:00:07,480] 我相信这里边没有任何一个人能说得清楚理解是什么意思
[00:00:07,480 -> 00:00:08,480] 对
[00:00:08,480 -> 00:00:10,320] 我想我仔细想过
[00:00:10,320 -> 00:00:13,439] 就是我之前两三年前出的出了一系列关于模式
[00:00:13,439 -> 00:00:15,320] 那你为什么没有理解
[00:00:15,320 -> 00:00:17,199] 因为那个时候就是很简单嘛
[00:00:17,199 -> 00:00:18,239] 就是鹦鹉学舌
[00:00:18,239 -> 00:00:19,160] 然后大家就说
[00:00:19,160 -> 00:00:21,679] 那你如果真的能做到鹦鹉学舌的话
[00:00:21,679 -> 00:00:23,120] 那你其实也就理解了
[00:00:23,120 -> 00:00:24,199] 人可能也就是这样子的
[00:00:24,199 -> 00:00:29,120] 但是我认识到就是或者说我的belief就是这不是真正的理解
[00:00:00,000 -> 00:00:03,500] 然後這件事情最好的被總結
[00:00:03,500 -> 00:00:06,879] 是由David Hume和康德的討論
[00:00:06,879 -> 00:00:07,879] 就是純粹理性批判
[00:00:07,879 -> 00:00:10,500] David Hume是一個經驗主義
[00:00:10,500 -> 00:00:12,380] 然後就是純粹理性
[00:00:12,380 -> 00:00:14,460] 經驗主義他說只要你這個人
[00:00:14,460 -> 00:00:17,260] 就是我們現在所有的行為
[00:00:17,260 -> 00:00:19,539] 都是我們經驗的一個集合
[00:00:19,539 -> 00:00:20,879] 或者說就是我們對世界
[00:00:20,879 -> 00:00:22,219] 是知識的組織
[00:00:22,219 -> 00:00:24,219] 然後我們可以產出這個
[00:00:24,219 -> 00:00:25,300] 人不需要思考
[00:00:25,300 -> 00:00:28,219] 然後人就直接能產出鸚鵡學舌
[00:00:28,219 -> 00:00:29,879] 那就代表了人類的智能
[00:00:00,000 -> 00:00:01,800] 但是康德就說不是的
[00:00:01,800 -> 00:00:03,180] 人其實是有先驗認知的
[00:00:03,180 -> 00:00:04,459] 人在一開始就對這個事情
[00:00:04,459 -> 00:00:05,679] 有一個看法
[00:00:05,679 -> 00:00:06,719] 然後我們所有的
[00:00:06,719 -> 00:00:07,679] preception
[00:00:07,679 -> 00:00:08,339] 對
[00:00:08,339 -> 00:00:09,599] 它叫a priority
[00:00:09,599 -> 00:00:10,720] 對 就是一個prior
[00:00:10,720 -> 00:00:12,519] 對 就是我們只不過
[00:00:12,519 -> 00:00:13,839] 修正這個事情而已
[00:00:13,839 -> 00:00:14,919] 但是我們那個東西
[00:00:14,919 -> 00:00:16,420] 代表了人的理性和智慧
[00:00:16,420 -> 00:00:18,379] Anyway 就是
[00:00:18,379 -> 00:00:21,179] 剛才那個問題是個哲學問題
[00:00:21,179 -> 00:00:23,760] 但是我猜critique這件事情的人
[00:00:23,760 -> 00:00:24,920] 大多數人是沒有
[00:00:24,920 -> 00:00:26,600] 你真的去問他們
[00:00:26,600 -> 00:00:27,640] 什麼是真正的理解
[00:00:27,640 -> 00:00:28,500] 我猜他們說不出來
[00:00:00,000 -> 00:00:04,879] 那你觉得这个判断对于我们认为
[00:00:04,879 -> 00:00:08,279] 这个GBT是否是真正通往AGI的路径
[00:00:08,279 -> 00:00:09,519] 你觉得这个重要吗
[00:00:09,519 -> 00:00:12,919] 就是这个模型是不是真正在我们定的理解中
[00:00:12,919 -> 00:00:16,719] 理解这个东西对于通往AGI这个东西有多重要
[00:00:21,280 -> 00:00:23,879] 我觉得简单说
[00:00:23,879 -> 00:00:26,280] 首先我们到底对AGI有什么样的期待
[00:00:26,280 -> 00:00:26,559] 对
[00:00:26,559 -> 00:00:29,079] 就是有什么样的期待
[00:00:00,000 -> 00:00:03,120] 就Howie說的是 我其實同意他
[00:00:03,120 -> 00:00:05,620] 就是和那個張毅說的也是這樣
[00:00:05,620 -> 00:00:09,919] 就是他在人的能想像到的這方面的task
[00:00:09,919 -> 00:00:12,119] 都能達到高中生以上的水平
[00:00:12,119 -> 00:00:15,419] 他的那版的GPT不是我們的這版的GPT
[00:00:15,419 -> 00:00:16,420] 那就是了
[00:00:16,420 -> 00:00:18,920] 就是一個general intelligence
[00:00:18,920 -> 00:00:23,620] 因為這個general intelligence在學術界上是沒有一個定論的
[00:00:23,620 -> 00:00:26,420] 然後Spark of AGI那個paper一上來也說了
[00:00:00,000 -> 00:00:03,879] 各种各样的AGI的definition和他们觉得比较通用的definition
[00:00:03,879 -> 00:00:05,719] 那就是你和人一样聪明嘛
[00:00:05,719 -> 00:00:07,200] 然后现在就是嘛
[00:00:07,200 -> 00:00:08,480] 而且比大多数人都聪明
[00:00:08,480 -> 00:00:09,839] 那还不是AGI是啥
[00:00:09,839 -> 00:00:12,400] 当然Simon Timon自己说不是
[00:00:12,400 -> 00:00:13,519] Simon Timon自己说不是
[00:00:13,519 -> 00:00:18,000] 那你觉得跟比如说像需要通过什么图灵测试这样的东西吗
[00:00:18,000 -> 00:00:20,079] 它肯定能通过图灵测试啊
[00:00:20,079 -> 00:00:24,239] 而且我当时也有一个视频是说图灵测试是不完备的
[00:00:24,239 -> 00:00:26,640] 我们还要在上面去加上一些其他的
[00:00:26,640 -> 00:00:28,879] 比如说Winograd的schema
[00:00:00,000 -> 00:00:02,500] 然后去完善这个测试才能让他
[00:00:02,500 -> 00:00:04,299] 然后他现在也肯定能通过那些
[00:00:05,900 -> 00:00:08,599] 就是任何测人类智能的测试
[00:00:08,599 -> 00:00:10,199] 他都能通过在我看来
[00:00:10,900 -> 00:00:13,000] 起码比50%的人更能通过
[00:00:13,000 -> 00:00:17,100] 你觉得现在的GPT还是说你觉得GPT4
[00:00:17,100 -> 00:00:18,199] 就是我们面
[00:00:18,199 -> 00:00:19,500] 我们手里拿到这版
[00:00:19,500 -> 00:00:21,699] 我们不需要张毅手里那版
[00:00:21,699 -> 00:00:23,600] 我们只需要我们现在手里的这版
[00:00:23,600 -> 00:00:27,300] 我相信他比50%的人都更能通过智商测试
[00:00:27,300 -> 00:00:28,899] 关于人类智商的任何测试
[00:00:00,000 -> 00:00:02,000] 我覺得不是
[00:00:02,000 -> 00:00:05,000] 我一眼裡看到了猶豫
[00:00:05,000 -> 00:00:06,500] 沒有 我其實並不是猶豫
[00:00:06,500 -> 00:00:08,000] 我只是我自己在想
[00:00:08,000 -> 00:00:09,500] 就是說我也沒有對AGI
[00:00:09,500 -> 00:00:11,000] 有一個特別好的定義
[00:00:11,000 -> 00:00:12,000] 就是說什麼才是AGI
[00:00:12,000 -> 00:00:13,500] 但我相信他肯定就是說
[00:00:13,500 -> 00:00:15,000] 不管是完成測試
[00:00:15,000 -> 00:00:18,000] 或者聊都是可以被
[00:00:18,000 -> 00:00:20,500] 就是可能在做一些任務的時候
[00:00:20,500 -> 00:00:22,500] 都比大部分普通人要做得更好
[00:00:22,500 -> 00:00:24,500] 就有點像我們現在自動駕駛一樣
[00:00:24,500 -> 00:00:26,500] 你覺得其實自動駕駛
[00:00:26,500 -> 00:00:28,000] 它已經你可以把它認為
[00:00:28,000 -> 00:00:29,500] 已經解決的問題
[00:00:00,000 -> 00:00:04,040] 就是他已經比大部分的Average Driver要做得更好了
[00:00:04,040 -> 00:00:05,280] 那是不是就已經解決了
[00:00:05,280 -> 00:00:06,879] 他是不是就是一個已經是好的
[00:00:06,879 -> 00:00:07,559] 就兩件事
[00:00:07,559 -> 00:00:08,759] 一個是他是不是解決了
[00:00:08,759 -> 00:00:10,640] 和一個他是不是被社會接受
[00:00:10,640 -> 00:00:11,720] 社會不接受
[00:00:11,720 -> 00:00:12,919] 那是社會的傻逼
[00:00:12,919 -> 00:00:14,080] 那不是他的問題
[00:00:14,080 -> 00:00:15,720] 就是我有一天坐Lift
[00:00:15,720 -> 00:00:18,399] 然後那個司機就特別開得特別爛
[00:00:18,399 -> 00:00:19,199] 就特別危險
[00:00:19,199 -> 00:00:21,399] 然後隨便轉彎也根本不打燈
[00:00:21,399 -> 00:00:22,079] 我當時就想說
[00:00:22,079 -> 00:00:23,320] 靠這要是自動駕駛的話
[00:00:23,320 -> 00:00:24,800] 我就不用擔心受這個怕了
[00:00:24,800 -> 00:00:27,559] 但是很多人
[00:00:27,559 -> 00:00:29,640] 我相信很多人寧可坐這個人的車
[00:00:00,000 -> 00:00:01,800] 也不願意坐一輛自動駕駛的車
[00:00:01,800 -> 00:00:03,500] 那我正好問你問題
[00:00:03,500 -> 00:00:04,500] 我也在想就是說
[00:00:04,500 -> 00:00:06,339] 那你覺得一個好的AGI
[00:00:06,339 -> 00:00:07,679] 你需要
[00:00:07,679 -> 00:00:11,179] 比如說我如果能夠設計出一個問題
[00:00:11,179 -> 00:00:14,000] 讓它讓我能夠識別出
[00:00:14,000 -> 00:00:16,800] 這個是一個機器人
[00:00:16,800 -> 00:00:19,100] 你覺得它還是AGI嗎
[00:00:19,100 -> 00:00:21,420] 或者說你會覺得影響它成為AGI嗎
[00:00:21,420 -> 00:00:23,300] 我就不影響
[00:00:23,300 -> 00:00:26,960] 就是你有一個題目能讓
[00:00:26,960 -> 00:00:28,640] 就比如說我們現在的
[00:00:00,000 -> 00:00:02,060] 人类能做好的 机器肯定做不好的
[00:00:02,060 -> 00:00:05,259] 就是你的那个识别码是吧
[00:00:05,259 -> 00:00:06,379] 你的那个验证码
[00:00:06,379 -> 00:00:07,780] 那个是机器肯定做不好的
[00:00:07,780 -> 00:00:08,779] 那就能看出来
[00:00:08,779 -> 00:00:12,779] 但是我就不影响它是个AGI
[00:00:12,779 -> 00:00:14,380] 我不需要AGI和人一样
[00:00:14,380 -> 00:00:16,100] 对 这是我的definition
[00:00:16,100 -> 00:00:17,179] 可以可以
[00:00:17,179 -> 00:00:18,660] 我觉得我没有什么一个很好的答案
[00:00:18,660 -> 00:00:20,219] 但是我其实就是说
[00:00:20,219 -> 00:00:22,500] 这取决于我们对AGI的期待到底是什么
[00:00:22,500 -> 00:00:24,379] 每个人期待不一样
[00:00:24,379 -> 00:00:27,339] 然后你就可以理解成现在的GP到底是不是一个AGI
[00:00:27,339 -> 00:00:28,500] 或者说你认为
[00:00:00,000 -> 00:00:02,120] Sam就很明显对AGI的期待更高
[00:00:02,120 -> 00:00:02,680] 就更高
[00:00:02,680 -> 00:00:05,240] 然后你就可能会说往这个更高的方向去发展
[00:00:05,240 -> 00:00:06,080] 那也是好事
[00:00:06,080 -> 00:00:07,519] 我觉得我们现在达到了AGI
[00:00:07,519 -> 00:00:11,439] 但是就是AGI它那个词本身意义不大
[00:00:11,960 -> 00:00:16,440] 然后我相信GPT可以让我们变得特别厉害
[00:00:17,039 -> 00:00:20,160] 对 但是或者这方面的做法
[00:00:20,160 -> 00:00:21,839] 能让我们变得很厉害
[00:00:21,839 -> 00:00:25,519] 然后它不是AGI这个词所能囊括的
[00:00:26,640 -> 00:00:28,879] 也许需要一个更高的一个词去衡量
[00:00:28,879 -> 00:00:29,839] 更精确的词
[00:00:00,000 -> 00:00:01,600] 更精確的詞
[00:00:01,600 -> 00:00:04,799] 更高的標準去衡量這些所謂的
[00:00:04,799 -> 00:00:06,160] 對 更高的標準
[00:00:06,160 -> 00:00:07,240] AGI不是一個高標準
[00:00:07,240 -> 00:00:09,960] 對 就是因為我覺得你剛才提到的讓我想到
[00:00:09,960 -> 00:00:13,160] 我覺得為什麼大家一直不願承認它是AGI
[00:00:13,160 -> 00:00:15,560] 因為講到AGI這個詞就有點像
[00:00:15,560 -> 00:00:17,760] 人 我覺得它太偏向於intelligence了
[00:00:17,760 -> 00:00:19,399] 但是人之所以為人
[00:00:19,399 -> 00:00:21,879] 它是不只是intelligence
[00:00:21,879 -> 00:00:23,199] 我覺得很多人覺得它不像是
[00:00:23,199 -> 00:00:23,920] 意識
[00:00:23,920 -> 00:00:26,199] 對 就是可能覺得是不是沒有意識
[00:00:26,199 -> 00:00:28,320] 或者沒有一些所謂的humanity
[00:00:00,000 -> 00:00:03,000] 或者说人性的一些成分
[00:00:03,000 -> 00:00:04,960] 但是的确如果我们从一个很
[00:00:04,960 -> 00:00:05,879] practical的角度
[00:00:05,879 -> 00:00:07,280] 而且可能很不幸的
[00:00:07,280 -> 00:00:09,679] 在很多工业社会或者像说
[00:00:09,679 -> 00:00:13,119] 像工业社会一样去管理人的组织里边
[00:00:13,119 -> 00:00:15,000] 其实人其实并不需要有太多的
[00:00:15,000 -> 00:00:18,239] 比如最近的社会事件里边所看到的那些人
[00:00:18,239 -> 00:00:19,640] 就是完全没有人性
[00:00:19,640 -> 00:00:21,000] 对
[00:00:21,000 -> 00:00:25,879] 这个其实让我想到一个挺有意思的一个例子
[00:00:25,879 -> 00:00:28,559] 是去年的一个动画片
[00:00:00,000 -> 00:00:01,399] 叫那个万神殿
[00:00:01,399 -> 00:00:03,600] 其实是一个非常有意思的一个动画片
[00:00:03,600 -> 00:00:05,200] 虽然说画风极其差
[00:00:05,200 -> 00:00:07,599] 这个Netflix非常开脑洞的一个对
[00:00:07,599 -> 00:00:08,800] 他的这个idea
[00:00:08,800 -> 00:00:10,900] 但是用一个非常血腥的方式
[00:00:10,900 -> 00:00:13,099] 把人脑实际切片的这种方式
[00:00:13,099 -> 00:00:14,800] 然后去上传这个人
[00:00:14,800 -> 00:00:15,900] 其实有点像我们刚所说的
[00:00:15,900 -> 00:00:18,199] 上传专家知识的这个方式
[00:00:18,199 -> 00:00:19,699] 比如说他里面举个例子说
[00:00:19,699 -> 00:00:22,899] 这个有一个人特别会写发明IP
[00:00:22,899 -> 00:00:24,800] 那个IP的这种专利
[00:00:24,800 -> 00:00:27,100] OK他就把那个人给
[00:00:27,100 -> 00:00:28,800] 就数字孪生化
[00:00:00,000 -> 00:00:01,399] 很血腥方式的软身化
[00:00:01,399 -> 00:00:01,800] 然后呢
[00:00:01,800 -> 00:00:03,799] 所以在他那个Digital的空间里面
[00:00:03,799 -> 00:00:05,000] 那个人就每天在写专利
[00:00:05,000 -> 00:00:07,200] 他里面有一个特别小的插曲很有意思
[00:00:07,200 -> 00:00:11,199] 就是他在里面放入了说让这个人的一个
[00:00:11,199 -> 00:00:14,199] 他的妈妈每天给他在那个虚拟的空间里
[00:00:14,199 -> 00:00:16,000] 让他每天给他打个电话
[00:00:16,000 -> 00:00:16,800] 然后就有人问说
[00:00:16,800 -> 00:00:18,600] 为什么你要加上这么一个环节
[00:00:18,600 -> 00:00:20,199] 每天让他在写专利不就好了吗
[00:00:20,199 -> 00:00:24,399] 他说结果我发现有了这样的一些Emotional的Touch
[00:00:24,399 -> 00:00:24,600] 对吧
[00:00:24,600 -> 00:00:27,199] 这种情绪上的一种亲情的感知
[00:00:00,000 -> 00:00:04,080] 会提升它的创新力
[00:00:04,080 -> 00:00:05,280] 提升它的效率
[00:00:05,280 -> 00:00:06,879] 因为我就想到
[00:00:06,879 -> 00:00:10,359] 其实我们很多像这些大的物理学家
[00:00:10,359 -> 00:00:11,279] 数学家等等
[00:00:11,279 -> 00:00:13,279] 其实他们比如说他们还喜欢音乐
[00:00:13,279 -> 00:00:13,960] 喜欢艺术
[00:00:13,960 -> 00:00:17,800] 他们会提到这些东西给他的一些灵感
[00:00:17,800 -> 00:00:20,719] 有时候我们会想说是不是AGI
[00:00:20,719 -> 00:00:21,559] 就是大用心
[00:00:21,559 -> 00:00:24,559] 它还缺一些所谓原创能力这些
[00:00:24,559 -> 00:00:26,800] 我都想说是不是在我们人类所期待
[00:00:26,800 -> 00:00:27,719] 但其实我个人觉得
[00:00:00,000 -> 00:00:02,600] 这个真的已经是非常高标准的下一步
[00:00:02,600 -> 00:00:06,679] 就是他们还有这种创造力
[00:00:06,679 -> 00:00:07,879] 我就想说如果说
[00:00:07,879 -> 00:00:09,160] 我觉得我
[00:00:09,160 -> 00:00:13,400] 如果说现在很多人对于他这个平均的能力
[00:00:13,400 -> 00:00:15,839] 完成工作能力上已经OK的话
[00:00:15,839 -> 00:00:18,960] 我觉得可能我们能想象的下一步可能是
[00:00:19,280 -> 00:00:21,079] 这个总是对
[00:00:21,079 -> 00:00:22,120] 你这个标准要求人
[00:00:22,120 -> 00:00:24,199] 你就想打那个神说分他那么多事
[00:00:24,199 -> 00:00:27,039] 然后我跟你今天写代码写这么好了
[00:00:27,039 -> 00:00:28,559] 还要要求我有音乐素养
[00:00:00,000 -> 00:00:02,600] 结果发现其实还是有音乐素养的
[00:00:02,600 -> 00:00:03,080] 对吧
[00:00:04,400 -> 00:00:05,240] 对对对
[00:00:05,240 -> 00:00:06,960] 我觉得也许他现在已经能做到
[00:00:06,960 -> 00:00:10,400] 就是说我还有一个就是我们所谓说他的创造力
[00:00:10,400 -> 00:00:14,199] 更多的是说你创造这个东西是我们社会能认可的一个东西
[00:00:14,199 -> 00:00:16,039] 那也许这个标准甚至都在改变
[00:00:16,039 -> 00:00:18,280] 就是说是我作为一个裁判
[00:00:18,280 -> 00:00:19,079] 我去认可
[00:00:19,079 -> 00:00:21,399] 那你不能说他可能创造了一些东西
[00:00:21,399 -> 00:00:23,280] 然后如果对我来说没有意义
[00:00:23,280 -> 00:00:24,079] 我就不认可这个
[00:00:24,079 -> 00:00:25,399] 我就不认为是这个音乐
[00:00:00,000 -> 00:00:05,759] 甚至有很多人的那种音乐家或者画家
[00:00:05,759 -> 00:00:07,280] 他可能做了一些东西
[00:00:07,280 -> 00:00:08,800] 当时不被人认可
[00:00:08,800 -> 00:00:10,400] 但他不代表这个东西没有意义
[00:00:10,400 -> 00:00:12,039] 或者他没有表达他想要的东西
[00:00:12,039 -> 00:00:14,119] 那你能不能否定他那一刻
[00:00:14,119 -> 00:00:16,000] 就是说他没有创造力呢
[00:00:16,000 -> 00:00:18,760] 我觉得所以这个本身也很抽象
[00:00:18,760 -> 00:00:19,800] 那我们就是说
[00:00:19,800 -> 00:00:23,920] 还是只能说是让我们和机器的关系
[00:00:23,920 -> 00:00:26,480] 我觉得这是一个比较更深层次的一个讨论
[00:00:26,480 -> 00:00:29,120] 就是我们应该怎么去评价
[00:00:00,000 -> 00:00:02,200] 我的位置 机器的位置
[00:00:02,200 -> 00:00:04,120] 如果机器的位置就是为了去
[00:00:04,120 -> 00:00:07,320] 满足人类的这些需求和欲望的话
[00:00:07,320 -> 00:00:08,720] 那是一种东西
[00:00:08,720 -> 00:00:09,960] 那如果我们把它作为
[00:00:09,960 -> 00:00:11,359] 像你这种方式去要求的话
[00:00:11,359 -> 00:00:12,800] 我们可能要把这个机器的位置
[00:00:12,800 -> 00:00:13,439] 抬得更高一点
[00:00:13,439 -> 00:00:15,240] 就是我和机器是平等的
[00:00:15,240 -> 00:00:17,280] 我才能这么去要求机器
[00:00:17,280 -> 00:00:19,000] 或者说希望机器有
[00:00:19,000 -> 00:00:20,000] 这么样一种行为
