[00:00:00,000 -> 00:00:01,800] 这不是应该是你吗?这是你的节目
[00:00:01,800 -> 00:00:03,160] 你的节目啊
[00:00:03,160 -> 00:00:07,919] 所以说我觉得这些东西叠加起来就可能世界上只有5个人
[00:00:07,919 -> 00:00:10,080] 但是我在这一步里面看到了两步
[00:00:10,080 -> 00:00:14,599] 一个是底层的模型和他的这个reinforcement learning这一步
[00:00:14,599 -> 00:00:15,240] 欢迎啊
[00:00:15,240 -> 00:00:16,239] Hello
[00:00:16,239 -> 00:00:16,839] 好大家好
[00:00:16,839 -> 00:00:18,120] 欢迎来到onboard
[00:00:18,120 -> 00:00:18,920] 我是Monica
[00:00:18,920 -> 00:00:19,920] 快点把你整
[00:00:19,920 -> 00:00:20,879] 哈哈哈
[00:00:20,879 -> 00:00:21,760] 来继续继续
[00:00:21,760 -> 00:00:22,160] 对对对
[00:00:22,160 -> 00:00:23,839] 这加的好加的好
[00:00:23,839 -> 00:00:25,199] 对对大家可以感受到了
[00:00:25,199 -> 00:00:25,519] 没错
[00:00:25,519 -> 00:00:28,719] 我们这一次又是一次非常有意思的串台
[00:00:00,000 -> 00:00:02,240] 就是Monica在跟这个柯代表
[00:00:02,240 -> 00:00:07,000] 我们在西雅图终于又在美国第一次线下见面了
[00:00:07,000 -> 00:00:09,000] 对虽然都是前亚马逊同事
[00:00:09,000 -> 00:00:10,279] 但是那个时候没见过
[00:00:10,279 -> 00:00:11,000] 对对对
[00:00:11,000 -> 00:00:14,519] 然后我们上一次见面应该还是在深圳
[00:00:14,519 -> 00:00:15,720] 然后我也是刚回
[00:00:15,720 -> 00:00:17,719] 我们都是刚回国不久
[00:00:18,199 -> 00:00:21,199] 没有第一次是刚回不久
[00:00:21,199 -> 00:00:23,920] 第二次的深圳是我刚要回美国
[00:00:23,920 -> 00:00:24,679] 对对对
[00:00:24,679 -> 00:00:27,719] 就每次我们见面都是一个非常重要的时间节点
[00:00:00,000 -> 00:00:03,200] 而这一次特别的一个地方也是在于说
[00:00:03,200 -> 00:00:07,200] 我们之所以来有这么一次这个录制这个话题的这个
[00:00:07,200 -> 00:00:11,400] 机会也是非常的意外和惊喜
[00:00:11,400 -> 00:00:15,800] 也是我们那次最近我正好来到西雅图这边出差
[00:00:15,800 -> 00:00:17,800] 然后我们在一次这个聚餐上
[00:00:17,800 -> 00:00:21,800] 然后发现聚餐中有一位小哥哥特别的有insight
[00:00:21,800 -> 00:00:23,399] 跟我们这个火花不断
[00:00:23,399 -> 00:00:25,800] 我们就觉得说这个一次吃饭显然是不够的
[00:00:25,800 -> 00:00:29,399] 所以说就专门把这位小哥哥这个邀请到了
[00:00:00,000 -> 00:00:07,440] 我们的这个节目来跟大家一起去探讨很多他所自己经历的这一路以来
[00:00:07,440 -> 00:00:10,279] 这个Machine Learning他所看到的大语言模型
[00:00:10,279 -> 00:00:15,160] 以及他所在的公司所经历大语言模型的一些这个探索
[00:00:15,160 -> 00:00:17,719] 而且我相信这一次因为有柯代表的加持
[00:00:17,719 -> 00:00:23,800] 我们可以在很多技术历史甚至实的虚的思考上
[00:00:23,800 -> 00:00:25,879] 相信能够碰上出更多的这个火花
[00:00:25,879 -> 00:00:29,679] 那我们要不先由这位小哥来做自我介绍
[00:00:00,000 -> 00:00:02,600] 谢谢那个Monica的介绍
[00:00:02,600 -> 00:00:04,599] 还有欢迎谢谢克莱比奥
[00:00:04,599 -> 00:00:07,040] 邀请我一起做这期节目
[00:00:07,040 -> 00:00:09,400] 然后我可以简单做一下自我介绍
[00:00:09,400 -> 00:00:10,640] 然后我叫卢毅
[00:00:10,640 -> 00:00:14,199] 我现在在一家C轮的创业公司
[00:00:14,199 -> 00:00:16,679] 做智能客服的创业公司叫ForSot
[00:00:16,679 -> 00:00:18,559] 做他们的head of machine learning
[00:00:18,559 -> 00:00:20,920] 然后我同时还在University of Washington
[00:00:20,920 -> 00:00:22,519] 做他们的科做教授
[00:00:22,519 -> 00:00:26,679] 之前我是在苹果做一些机器学习方面的研究
[00:00:26,679 -> 00:00:28,839] 主要在theory里面做他们的
[00:00:00,000 -> 00:00:02,240] 比如说事实性问题的问答
[00:00:02,240 -> 00:00:04,320] 也做一些检索相关的工作
[00:00:04,320 -> 00:00:06,679] 所以我也是非常高兴
[00:00:06,679 -> 00:00:09,000] 这次能够和大家一起来分享一些我的观点
[00:00:09,000 -> 00:00:10,759] 然后和大家一起探讨学习
[00:00:12,160 -> 00:00:14,199] 好的 我叫孙宇峥
[00:00:14,199 -> 00:00:16,039] 然后我的网名叫柯代表立正
[00:00:16,039 -> 00:00:17,760] 是B站和油管的账号
[00:00:17,760 -> 00:00:19,440] 所以说叫我柯代表就行
[00:00:19,440 -> 00:00:21,239] 我是一个做数据的
[00:00:21,239 -> 00:00:22,480] 之前是经济学博士
[00:00:22,480 -> 00:00:23,480] 然后在亚马逊
[00:00:23,480 -> 00:00:24,160] Facebook
[00:00:24,160 -> 00:00:27,000] 腾讯都做数据方面的工作
[00:00:00,000 -> 00:00:02,919] 最近去加入了一家startup叫statseek
[00:00:02,919 -> 00:00:05,559] 是就是statistical significant的
[00:00:05,559 -> 00:00:06,200] 缩写
[00:00:06,200 -> 00:00:08,839] 它是做ABCN平台的公司
[00:00:08,839 -> 00:00:13,359] 我平时也对就是技术啊商业啊都非常感兴趣
[00:00:13,359 -> 00:00:17,000] 不管是工作中还是课外中都会有很多思考和产出相关的内容
[00:00:17,000 -> 00:00:19,879] 我因为在2月份的时候
[00:00:19,879 -> 00:00:21,679] 应该是去年10月份的时候吧
[00:00:21,679 -> 00:00:25,320] 跟Howie聊了之后就一直非常关注AGI的东西
[00:00:25,320 -> 00:00:27,160] 结果就跟他聊完了以后
[00:00:27,160 -> 00:00:28,280] Mid Journey出来了
[00:00:00,000 -> 00:00:05,080] 然后ChaiGBT出来了之后就一直思考也写了很多这方面的文章
[00:00:05,080 -> 00:00:09,080] 包括在M小姐的公众号上发过关于ChaiGBT的最重要的5个问题
[00:00:09,080 -> 00:00:16,039] 也一直在不断的寻找有insight的人去进行交流
[00:00:16,039 -> 00:00:17,600] 提升我自己的认知
[00:00:17,600 -> 00:00:19,640] 从如意这里边学到了很多东西
[00:00:19,640 -> 00:00:21,559] 所以说就赶紧抓住机会
[00:00:21,559 -> 00:00:24,760] 趁着Monica在的时候我们赶紧一起做个节目
[00:00:24,760 -> 00:00:27,359] 我好奇就是我们要不作为一个开场
[00:00:00,000 -> 00:00:02,839] 就那天我们吃饭的时候聊到了很多话题
[00:00:02,839 -> 00:00:07,799] 我好奇是当时卢毅聊到了哪几个点
[00:00:07,799 -> 00:00:11,919] 让你萌生了要跟他做进一步交友的想法
[00:00:11,919 -> 00:00:13,599] 有哪几个你比较印象深刻的点
[00:00:13,599 -> 00:00:15,199] 我们可以作为一个开场
[00:00:15,480 -> 00:00:20,079] 首先我觉得我们在那个群里边有各种各样的人
[00:00:20,079 -> 00:00:20,440] 对吧
[00:00:20,440 -> 00:00:24,960] 就是我觉得大家对这件事情的理解到位的人就已经不多了
[00:00:25,079 -> 00:00:26,839] 就是虽然这么多人关注
[00:00:26,839 -> 00:00:29,480] 虽然我们的群里边已经是这里边讨论最内部
[00:00:00,000 -> 00:00:02,759] 但是我觉得真的对他有比较到位的
[00:00:02,759 -> 00:00:08,000] 就是事实性都是事实正确且理解到位的人
[00:00:08,000 -> 00:00:09,759] 我觉得应该不到100个
[00:00:09,759 -> 00:00:12,000] 然后在这些人里边
[00:00:12,000 -> 00:00:17,480] 你又能有一线经验且能不断帮助你update认知的人就更少
[00:00:17,480 -> 00:00:22,359] 但是更重要的是卢伊本身他在苹果的时候
[00:00:22,359 -> 00:00:23,760] 就是做这方面的
[00:00:23,760 -> 00:00:26,480] 只不过不是用GPT的方式去做
[00:00:26,480 -> 00:00:29,359] 而和现在所在的这个公司
[00:00:00,000 -> 00:00:02,240] 就是一個人工智能
[00:00:02,240 -> 00:00:05,000] 人工智能enable的客服公司
[00:00:05,000 -> 00:00:07,120] 又是大約模型
[00:00:07,120 -> 00:00:10,080] 最promising的應用場景
[00:00:10,080 -> 00:00:12,580] 所以說我覺得這些東西疊加起來
[00:00:12,580 -> 00:00:14,580] 就可能世界上只有五個日月
[00:00:18,300 -> 00:00:21,800] 就是他在講他們的技術選擇的時候
[00:00:21,800 -> 00:00:24,800] 應該是在講17年前後
[00:00:24,800 -> 00:00:27,339] 有一個人在亞馬遜做了一個模型
[00:00:00,000 -> 00:00:04,280] 然后他可以在review里边extract sentiment
[00:00:04,280 -> 00:00:07,160] 然后他当时没有
[00:00:07,160 -> 00:00:09,359] 大家觉得这个东西是一个很正常的东西
[00:00:09,359 -> 00:00:10,800] 可是他们看到了一个
[00:00:10,800 -> 00:00:13,000] 就是很大的希望
[00:00:13,000 -> 00:00:17,239] 因为你只是做从embedding里边就可以extract sentiment
[00:00:17,239 -> 00:00:18,600] 哦 我明白你的意思
[00:00:18,600 -> 00:00:21,199] 我忘了具体是什么模型和具体是
[00:00:21,199 -> 00:00:24,280] 应该是在Elia的一次访谈里边
[00:00:24,280 -> 00:00:26,280] 有人问他就是说你的那个
[00:00:26,280 -> 00:00:29,359] 我们可以现在找一下出一系列的research
[00:00:00,000 -> 00:00:04,280] training on just max token character in the text of the amazon review
[00:00:04,799 -> 00:00:07,080] 这个其实我觉得一方面来讲
[00:00:08,439 -> 00:00:10,519] 怎么说surprise也可以说不surprise
[00:00:10,519 -> 00:00:13,279] 我们它其实怎么说的是unsupervised learning
[00:00:13,279 -> 00:00:15,599] 对这其实这里面如果更specific
[00:00:15,599 -> 00:00:17,440] 他们其实是叫self-supervised
[00:00:17,440 -> 00:00:20,920] 就是我们说什么叫unsupervised
[00:00:20,920 -> 00:00:22,399] 就是说完全没有label对吗
[00:00:22,399 -> 00:00:24,199] 就是你完全不知道你下面要predict
[00:00:24,199 -> 00:00:26,320] 那正好像一个cluster的问题
[00:00:26,320 -> 00:00:27,600] 把东西分类
[00:00:27,600 -> 00:00:29,199] 就是group在一起
[00:00:00,000 -> 00:00:01,000] 那其实是没有label
[00:00:01,000 -> 00:00:02,480] 那就是unsupervised
[00:00:02,480 -> 00:00:04,120] 那有label就是supervised
[00:00:04,120 -> 00:00:06,759] 那像这种next token prediction
[00:00:06,759 -> 00:00:09,519] 实际上是一种比较聪明的方式去自己supervised自己
[00:00:09,519 -> 00:00:09,960] 对
[00:00:09,960 -> 00:00:12,359] 因为你其实没有人给你提供任何一个label
[00:00:12,359 -> 00:00:14,080] 但是下一个token就是你的label
[00:00:14,080 -> 00:00:14,519] 对
[00:00:14,519 -> 00:00:16,960] 那其实这个最开始的时候
[00:00:16,960 -> 00:00:18,679] 当然了这是我的观点
[00:00:18,679 -> 00:00:19,960] 我的理解就是说
[00:00:20,519 -> 00:00:22,120] 最开始的时候这种语言模型
[00:00:22,120 -> 00:00:25,359] 他们的这个language modeling task
[00:00:25,359 -> 00:00:27,839] 其实最开始就是predict next token
[00:00:27,839 -> 00:00:28,839] 到现在也是
[00:00:28,839 -> 00:00:29,559] 到现在也是
[00:00:00,000 -> 00:00:04,080] 但中间有时候大家会去就是从中偏离出来
[00:00:04,080 -> 00:00:06,719] 就是也许不认为是这种方式是最好的
[00:00:06,719 -> 00:00:10,119] 那一开始大家就想法就是说
[00:00:10,119 -> 00:00:11,560] 如果你能够prepare下一个token
[00:00:11,560 -> 00:00:14,400] 你相当于对之前的语言有一个很好的理解
[00:00:14,400 -> 00:00:15,960] 那这个理解什么呢
[00:00:15,960 -> 00:00:18,399] 其实你可以理解成就是它的embedding的东西
[00:00:18,399 -> 00:00:20,440] 如果你对这个语言有很好的理解
[00:00:20,440 -> 00:00:22,640] 那是不是这个embedding就应该包含了
[00:00:22,640 -> 00:00:25,120] 这个语言当中本来的一些信息
[00:00:25,120 -> 00:00:27,359] 剩下的部分只是从这个embedding中
[00:00:27,359 -> 00:00:29,160] 提取出你已经学到的信息
[00:00:00,000 -> 00:00:03,000] 这就是为什么要选择一个task
[00:00:03,000 -> 00:00:04,599] 作为language modeling task
[00:00:04,599 -> 00:00:07,599] 那就是说他们认为如果你能完成这个task
[00:00:07,599 -> 00:00:09,400] 代表你对语言有掌握
[00:00:09,400 -> 00:00:14,199] 那这种方面就是说不见得一定是next token prediction
[00:00:14,199 -> 00:00:15,800] 才是language modeling task
[00:00:15,800 -> 00:00:16,600] 比如说BERT
[00:00:16,600 -> 00:00:17,679] 它其实一开始的时候
[00:00:17,679 -> 00:00:20,920] GPT或者说这种类型叫auto-regressor模型
[00:00:20,920 -> 00:00:22,440] 你有前面的东西
[00:00:22,440 -> 00:00:23,519] 然后predict下一个
[00:00:23,519 -> 00:00:25,600] 就非常适常于生成
[00:00:25,600 -> 00:00:26,879] 那人们就发现有一些问题
[00:00:26,879 -> 00:00:28,920] 就是比如说你在学习这个
[00:00:00,000 -> 00:00:01,760] 因为我的目标是学习语言嘛
[00:00:01,760 -> 00:00:05,320] 并不是并不完全所有的task都是为了生成
[00:00:05,320 -> 00:00:06,559] 那我为了学习语言
[00:00:06,559 -> 00:00:08,519] 那么我去做这种
[00:00:09,039 -> 00:00:11,039] predict下一个token的情况下的时候
[00:00:11,039 -> 00:00:13,199] 那有可能会失去后面的context
[00:00:13,400 -> 00:00:13,960] 对不对
[00:00:14,080 -> 00:00:15,519] 就是我并不知道后面的东西
[00:00:15,519 -> 00:00:18,719] 没有让我后面的词来去描述之前的词
[00:00:18,719 -> 00:00:20,000] 去学这个词的意思
[00:00:20,000 -> 00:00:20,399] 对
[00:00:20,760 -> 00:00:21,760] 对就是
[00:00:21,760 -> 00:00:23,039] 所以这就是所谓的
[00:00:23,039 -> 00:00:24,960] 我觉得这counterintuitive和
[00:00:24,960 -> 00:00:27,079] 为什么这个研究如此的重要
[00:00:00,000 -> 00:00:03,399] 和他为什么就是伊莲娜觉得如此的重要
[00:00:03,399 -> 00:00:08,240] 就是他这个contract interative和需要你有一定的信仰的东西
[00:00:08,240 -> 00:00:11,480] 就是你的你刚刚说的next token
[00:00:12,320 -> 00:00:14,679] 包含了所有的信息
[00:00:14,679 -> 00:00:15,279] 或者说
[00:00:15,279 -> 00:00:16,719] 不包含之前所有的信息
[00:00:16,719 -> 00:00:18,559] 不光是能够包含所有的信息
[00:00:18,559 -> 00:00:21,280] 而且他从一定程度上也包含了之后的信息
[00:00:27,000 -> 00:00:28,160] 这个不好说
[00:00:28,160 -> 00:00:29,079] 对这个怎么理解
[00:00:00,000 -> 00:00:01,159] 它还可以包含之后
[00:00:01,159 -> 00:00:02,240] 就刚刚比如说的
[00:00:02,240 -> 00:00:03,759] 就是刚刚说的一个很重要的
[00:00:03,759 -> 00:00:06,440] BERT和这个GPT思路的区别
[00:00:06,440 -> 00:00:07,719] 就是BERT是双向的
[00:00:07,719 -> 00:00:07,919] 对吧
[00:00:07,919 -> 00:00:09,000] 然后GPT是单向的
[00:00:09,000 -> 00:00:09,759] 对
[00:00:09,759 -> 00:00:11,679] 然后它为什么要用双向
[00:00:11,679 -> 00:00:13,839] 就是因为语言中的上下文
[00:00:13,839 -> 00:00:15,480] 你既然已经能看到全文了
[00:00:15,480 -> 00:00:18,600] 但你当然也应该用全文去理解这个词
[00:00:18,600 -> 00:00:21,800] 那你的全文就包含了之前的信息和之后的信息
[00:00:21,800 -> 00:00:22,079] 对
[00:00:22,079 -> 00:00:23,679] 所以说从这个角度来说
[00:00:23,679 -> 00:00:25,519] 我们想要精确的预测出来
[00:00:25,519 -> 00:00:27,000] 就是给你一篇文章
[00:00:27,000 -> 00:00:28,280] 现在你要去学习这篇文章
[00:00:00,000 -> 00:00:04,080] 然后从这些词里面清确的知道这个词是什么的情况下
[00:00:04,080 -> 00:00:06,080] 你就应该知道全文是最好的
[00:00:06,080 -> 00:00:07,400] 对 肯定的是
[00:00:07,400 -> 00:00:09,519] 你有双向的注意力是最好的
[00:00:09,519 -> 00:00:10,800] 但现在就是像是说GBT
[00:00:10,800 -> 00:00:12,519] 它选择了只有单向的注意力
[00:00:12,519 -> 00:00:13,640] 没有双向的注意力
[00:00:13,640 -> 00:00:16,280] 后来我觉得这个发展应该是一开始的时候
[00:00:16,280 -> 00:00:17,079] 人们认为单向
[00:00:17,079 -> 00:00:22,239] 那这个学习语言就是需要这种做这种单向的生成就可以了
[00:00:22,239 -> 00:00:25,519] 这就是一个by definition什么是language modeling task
[00:00:25,519 -> 00:00:28,480] 那之后人们认为双向的注意力是有好处的
[00:00:00,000 -> 00:00:02,279] 我觉得这也是人们发现确实是有好处的
[00:00:02,279 -> 00:00:03,080] 我觉得不是
[00:00:03,080 -> 00:00:03,839] 我觉得是一个
[00:00:03,839 -> 00:00:05,639] 就是我说发现这个词
[00:00:05,639 -> 00:00:06,919] 我不是说确实是有好处的
[00:00:06,919 -> 00:00:07,320] 肯定是
[00:00:07,320 -> 00:00:08,800] 但是我觉得说发现这个词
[00:00:08,800 -> 00:00:09,640] 我不同意
[00:00:09,640 -> 00:00:10,519] 为什么呢
[00:00:10,519 -> 00:00:12,240] 就是在这之上
[00:00:12,240 -> 00:00:13,439] 其实我就看到了
[00:00:13,439 -> 00:00:15,800] 就是我自己对OpenAI的感觉
[00:00:15,880 -> 00:00:17,800] 就是他们一直在走这一条路
[00:00:17,800 -> 00:00:20,600] 这条路是可以lead to AGI的那一
[00:00:20,600 -> 00:00:21,719] 唯一的一条路
[00:00:21,719 -> 00:00:23,199] 然后剩下所有的人呢
[00:00:23,199 -> 00:00:26,480] 都是在application的过程中take各种shortcut
[00:00:26,480 -> 00:00:27,559] cut各种corners
[00:00:27,559 -> 00:00:28,320] 可以这么理解
[00:00:28,320 -> 00:00:28,679] 对
[00:00:00,000 -> 00:00:03,319] 它的card corners的体现在什么
[00:00:03,319 -> 00:00:05,879] 就是体现在你用更多的信息
[00:00:05,879 -> 00:00:07,719] 或者更specific的任务
[00:00:07,719 -> 00:00:10,119] 或者说你加入了人更多的hard coding
[00:00:10,119 -> 00:00:11,960] 就是它是不同程度的hard coding
[00:00:11,960 -> 00:00:14,480] 像亚马逊Alexa就是写各种if function
[00:00:14,480 -> 00:00:15,880] 如果这个人说了这个
[00:00:15,880 -> 00:00:16,399] 对
[00:00:16,399 -> 00:00:19,960] 然后你就可以在一些任务上perform的更好
[00:00:19,960 -> 00:00:21,800] 它在一些任务上perform更好
[00:00:21,800 -> 00:00:22,839] 它就不general
[00:00:22,839 -> 00:00:23,760] 对
[00:00:23,760 -> 00:00:28,359] 然后OpenAI从头到尾就是二和三为什么坚持下来那么难
[00:00:00,000 -> 00:00:04,040] 和我觉得那么就是展示的这些人非常不一样的地方
[00:00:04,040 -> 00:00:06,400] 就是因为R是比Bert的差很远
[00:00:06,400 -> 00:00:07,400] 各方面都不如
[00:00:07,400 -> 00:00:10,439] 3是有来有回或者说开始超越了
[00:00:10,439 -> 00:00:13,320] 但是从2和3之间的这个坚持是很难的
[00:00:13,320 -> 00:00:13,960] 是的
[00:00:13,960 -> 00:00:17,280] 对他们就坚持了那个唯一一个最general的方向
[00:00:17,280 -> 00:00:20,719] 就是你想想如果一个最general的东西
[00:00:20,719 -> 00:00:24,079] 就必须要用只用上文去predict下文
[00:00:24,079 -> 00:00:26,160] 之外的所有的hard coding或者什么的
[00:00:26,160 -> 00:00:29,120] 都会让他也许在一些事情上做得更好
[00:00:00,000 -> 00:00:04,160] 就是他在这过程中他真的把这些人力物力拿过去
[00:00:04,160 -> 00:00:06,320] 去做一个specific任务的话
[00:00:06,320 -> 00:00:08,320] 我相信他早就做得很好
[00:00:08,320 -> 00:00:10,160] 但是他就到不了
[00:00:10,160 -> 00:00:11,720] 我觉得你确实我同意你说的
[00:00:11,720 -> 00:00:13,640] 这其实是非常需要信仰的
[00:00:13,640 -> 00:00:16,120] 就是说在这个发展的过程当中
[00:00:16,120 -> 00:00:18,199] 其实那transformer出现了以后
[00:00:18,199 -> 00:00:20,079] 出现了三个发展的方向
[00:00:20,079 -> 00:00:23,000] 可以说第一个就是纯Auto-regressive的模型
[00:00:23,000 -> 00:00:24,559] 那这种就是Next-token prediction
[00:00:24,559 -> 00:00:25,800] 那他们的目标就是为了
[00:00:25,800 -> 00:00:27,679] 只有一个目标就是为了生成
[00:00:00,000 -> 00:00:02,439] 因为我只有我为生成目标的时候
[00:00:02,439 -> 00:00:03,480] 我是没有后文的
[00:00:03,480 -> 00:00:04,320] 我只有前文
[00:00:04,320 -> 00:00:05,559] 我要做生成
[00:00:05,559 -> 00:00:08,480] 那如果你的目标或者说你认为真正的AGI
[00:00:08,919 -> 00:00:11,560] 最后就一定要完成很好的生成性任务的话
[00:00:11,560 -> 00:00:15,160] 那其实这种Auto-regressive的component是不能少的
[00:00:15,160 -> 00:00:16,920] 你一定要去完成这件事情
[00:00:16,920 -> 00:00:19,199] T5的时间是什么时候
[00:00:19,719 -> 00:00:21,679] 我们可以查一查T5是什么时候
[00:00:21,679 -> 00:00:26,160] 但是我估计也是个1920年左右的
[00:00:26,160 -> 00:00:26,640] 对
[00:00:00,000 -> 00:00:06,400] 就是他们有信仰去说自己用Auto-regressive这件事情去做AGI
[00:00:06,400 -> 00:00:09,160] 是17年的时候的信仰
[00:00:09,160 -> 00:00:15,359] 然后这个T5也就是所有的自然语言任务都可以被生成式任务统一
[00:00:15,359 -> 00:00:16,559] 是20年的文章
[00:00:16,559 -> 00:00:20,920] 就是他们在不知道T5的那个结论的时候
[00:00:20,920 -> 00:00:24,920] 我们现在回去看的时候觉得好像一切都make sense对吧
[00:00:24,920 -> 00:00:29,559] 但是在那个时候你不知道你的所有任务是可以被生成式任务给统一的
[00:00:00,000 -> 00:00:01,760] 然后你就走了AutoRegressive
[00:00:01,760 -> 00:00:03,160] 那是很难很难的一件事
[00:00:03,160 -> 00:00:06,200] 还有就是像另外就是说你
[00:00:06,200 -> 00:00:08,000] 当然你当时人民就可以选择说
[00:00:08,000 -> 00:00:09,480] 如果我要得到一个好的效果
[00:00:09,480 -> 00:00:11,240] 我可以选择是一种双向注意力
[00:00:11,240 -> 00:00:12,320] 就是这种AutoEncoding
[00:00:12,320 -> 00:00:13,080] 像BERT的方向
[00:00:13,080 -> 00:00:14,359] 同时还有一种就是
[00:00:14,359 -> 00:00:19,079] 那结合了AutoEncoder和AutoRegressive的方法
[00:00:19,079 -> 00:00:20,320] 比如说有一个叫BART
[00:00:20,320 -> 00:00:23,519] 它的模型就是先做像BERT一样的东西
[00:00:23,519 -> 00:00:26,679] 获得它的像是表达
[00:00:26,679 -> 00:00:29,320] 再通过一个生成性模型再去做生成
[00:00:00,000 -> 00:00:02,759] 那如果我们其实在那一刻的时候
[00:00:02,759 -> 00:00:05,320] 其实我也在那一刻的时候去了解这种这样的模型嘛
[00:00:05,320 -> 00:00:07,400] 那一刻是什么时候
[00:00:07,400 -> 00:00:09,240] 就是比如说BERT刚刚出来的时候
[00:00:09,240 -> 00:00:14,519] 就比如说是2021年或者是20年21年的时候
[00:00:14,519 -> 00:00:17,559] 那种时候就是大家其实确实非常的以后
[00:00:17,559 -> 00:00:21,199] 就是说这三个思路到底要往哪个方向去发展
[00:00:21,199 -> 00:00:25,359] 对明显我们知道这种像BERT这样的模型
[00:00:25,359 -> 00:00:26,640] 它的发展是更多的
[00:00:26,640 -> 00:00:28,519] 注意力更多效果会更好
[00:00:00,000 -> 00:00:04,480] 大家有很多文章都是讲的是如何提高这种东西的模型
[00:00:04,480 -> 00:00:06,719] 甚至就是说我如何去改变
[00:00:07,599 -> 00:00:09,800] 就用一种非常巧妙的方式
[00:00:09,800 -> 00:00:13,400] 用BERT这种双向注意力去完成生雄式问题
[00:00:13,400 -> 00:00:15,919] 这其实我觉得在那一刻你也是无可厚非的
[00:00:15,919 -> 00:00:18,399] 就是就算是为了要完成这个好问题
[00:00:18,760 -> 00:00:20,920] 那你可能人们都不知道是用哪种方式
[00:00:20,920 -> 00:00:23,120] 更是能够有最好的结果
[00:00:23,120 -> 00:00:25,800] 但最后其实OpenAI也向大家证明了
[00:00:25,800 -> 00:00:28,839] 就是这种Auto-regressive单向注意力的
[00:00:00,000 -> 00:00:04,000] 就通過各種各樣的方法是可以完成到很好的
[00:00:04,000 -> 00:00:07,400] 對 就是這個信仰之月是月在哪裡呢
[00:00:07,400 -> 00:00:10,400] 就是其他的東西都是短時間
[00:00:10,400 -> 00:00:11,720] 一定程度內效果好
[00:00:11,720 -> 00:00:14,599] 問題是GPT這個方向你根本不知道能不能成
[00:00:14,599 -> 00:00:17,000] 對 就是你剛才所說的這個就是
[00:00:17,000 -> 00:00:20,199] 二到三之間就是他們看到了什麼
[00:00:20,199 -> 00:00:22,920] 就是在二到三之間他們看到了什麼
[00:00:22,920 -> 00:00:24,280] 別人沒有看到的
[00:00:24,280 -> 00:00:25,760] 或者說有一些大家都看到了
[00:00:25,760 -> 00:00:28,239] 但是他們從裡面看到了一個更
[00:00:00,000 -> 00:00:01,919] 对,我觉得17年这个文章真的很重要
[00:00:01,919 -> 00:00:04,120] 就是这个unsupervised sentiment neuron
[00:00:04,120 -> 00:00:07,719] 就是这个是告诉他们这个东西是可能可以的
[00:00:07,719 -> 00:00:10,839] 不是说可行甚至,就是可能可行
[00:00:10,839 -> 00:00:12,839] 然后他们就背着这个可能可行去了
[00:00:12,839 -> 00:00:18,239] 而且是通过这种暴力美学的加算的方式
[00:00:18,239 -> 00:00:21,399] 是可能可行到一个突破threshold的
[00:00:21,399 -> 00:00:22,760] 对
[00:00:00,000 -> 00:00:08,759] 我好奇你前面也提到
[00:00:08,759 -> 00:00:10,759] 我相信你那篇文章出來以後
[00:00:10,759 -> 00:00:13,119] 也會有很多人找你來探討
[00:00:13,119 -> 00:00:15,119] 你感覺到GPT
[00:00:15,119 -> 00:00:18,239] 你看GPT出來也大半年已經過去了
[00:00:18,239 -> 00:00:19,559] 你感覺這過程中
[00:00:19,559 -> 00:00:24,519] 大家仍然最常見的一些誤解可能是在哪
[00:00:24,519 -> 00:00:26,399] 最常見的誤解就是
[00:00:26,399 -> 00:00:29,879] 永遠會發生的短期高估和長期低估
[00:00:00,000 -> 00:00:01,740] 這個Howie當時跟他聊的時候
[00:00:01,740 -> 00:00:04,660] 他說在雲和Mobile上都出現過這樣的情況
[00:00:04,660 -> 00:00:06,259] 那他不理解就是說為什麼會這樣的
[00:00:06,259 -> 00:00:07,860] 現在我就理解了為什麼會這樣
[00:00:07,860 -> 00:00:09,660] 就是因為短期的時候
[00:00:09,660 -> 00:00:13,419] 大家想的是用Chai GPT能解決我現在手上的什麼問題
[00:00:13,419 -> 00:00:15,660] 這一定是一個小機會
[00:00:15,660 -> 00:00:17,100] 因為Chai GPT對比
[00:00:17,100 -> 00:00:21,019] 因為現在能看到的問題都會被現有的技術解決的還不錯
[00:00:21,019 -> 00:00:22,899] 所以Chai GPT也許能增效
[00:00:22,899 -> 00:00:24,579] 能增效個50%是了不起了
[00:00:24,579 -> 00:00:29,460] 但是它真正能給我們帶來的一定是我們現在還不存在的問題
[00:00:00,000 -> 00:00:02,520] 嗯对就是你在mobile之前
[00:00:02,520 -> 00:00:04,320] Uber是不可能做的对吧
[00:00:04,320 -> 00:00:06,559] 但是有了mobile才可能做Uber
[00:00:06,559 -> 00:00:09,759] 但是你需要很多东西都存在了以后才能做Uber
[00:00:09,759 -> 00:00:11,519] 嗯你互联网之前
[00:00:11,519 -> 00:00:13,720] Google是不可能做的
[00:00:13,720 -> 00:00:14,880] 但是互联网刚出来的时候
[00:00:14,880 -> 00:00:15,599] Google也做不了
[00:00:15,599 -> 00:00:17,280] 因为那个时候没有什么信息在网上
[00:00:17,280 -> 00:00:19,600] 嗯我觉得这是大家最大的误解
[00:00:19,600 -> 00:00:21,359] 诶卢伊呢从你的角度
[00:00:21,359 -> 00:00:23,760] 我觉得正好我有一个特别相关的话题
[00:00:23,760 -> 00:00:26,120] 就是其实大学模型出来了以后
[00:00:26,120 -> 00:00:27,800] 我就在思考一个问题
[00:00:00,000 -> 00:00:03,200] 就是说我们所谓的应用型的机器学习
[00:00:03,200 -> 00:00:04,679] 像applied machine learning
[00:00:04,679 -> 00:00:06,559] 这个领域到底之后还会不会存在
[00:00:06,559 -> 00:00:09,359] 或者说什么是应用型机器学习
[00:00:09,359 -> 00:00:13,240] 比如说我们原来就认为机器学习分成几个部分
[00:00:13,240 -> 00:00:15,679] 有些人是更加偏工程型的
[00:00:15,679 -> 00:00:16,000] 对
[00:00:16,000 -> 00:00:17,800] 那就是有一些工程性的问题
[00:00:17,800 -> 00:00:19,480] 就其实和software engineer
[00:00:19,480 -> 00:00:21,000] 我认为其实没有本质上的区别
[00:00:21,000 -> 00:00:21,960] 我是learning engineer嘛
[00:00:21,960 -> 00:00:22,399] 对
[00:00:22,399 -> 00:00:23,719] 其实没有本质上区别
[00:00:23,719 -> 00:00:26,480] 他只做的是不同类型的engineering问题
[00:00:26,480 -> 00:00:28,239] 不可能说有人叫as engineer
[00:00:00,000 -> 00:00:03,200] 有些人叫什么Gateway Engineer
[00:00:03,200 -> 00:00:04,679] 但其实都是Software Engineer
[00:00:04,679 -> 00:00:05,440] Machine Learning Engineer
[00:00:05,440 -> 00:00:07,519] 其实在很多时候只是它的context有一些不一样
[00:00:07,519 -> 00:00:09,119] 我觉得这是最engineering的程度
[00:00:09,119 -> 00:00:10,759] 那其实还有另外一个极端
[00:00:10,759 -> 00:00:12,679] 就是像Scientist
[00:00:12,679 -> 00:00:16,239] 他们就是做的东西其实就比较脱离
[00:00:16,239 -> 00:00:18,960] 因为我也做过一些关于research方面的工作嘛
[00:00:18,960 -> 00:00:21,480] 然后其实就主要就是
[00:00:21,480 -> 00:00:24,719] 你有一个具体的一个非常well defined的一个task
[00:00:24,719 -> 00:00:27,199] 你去如何找到这些数据
[00:00:00,000 -> 00:00:03,200] 如何提高在这个东西上面的效果
[00:00:03,200 -> 00:00:04,280] 也是一个比较玻璃的
[00:00:04,280 -> 00:00:06,879] 就比较怎么说理论比较强的
[00:00:06,879 -> 00:00:08,560] 那中间就有一个
[00:00:08,560 -> 00:00:10,560] applied scientist
[00:00:10,560 -> 00:00:12,119] or applied machine learning
[00:00:12,119 -> 00:00:15,519] 就是把这些技术通过工程的方式
[00:00:15,519 -> 00:00:19,000] 应用在一个具体的产品或者一种形式上
[00:00:19,000 -> 00:00:21,399] 去所谓实现商业价值
[00:00:21,399 -> 00:00:25,800] 那之后这个中间的行业到底还是什么样子
[00:00:25,800 -> 00:00:27,199] 未来会变成怎么样的发展
[00:00:00,000 -> 00:00:02,919] 我觉得我自己是特别的好奇这个方面
[00:00:02,919 -> 00:00:04,879] 嗯 那其实在今天来讲
[00:00:04,879 -> 00:00:08,640] 就你当然了我们长期看也许这个行业就已经消失了
[00:00:08,640 -> 00:00:11,599] 嗯 就变成了每个人都应该获得的技能
[00:00:11,599 -> 00:00:13,519] 就比如说我们每个人不会说
[00:00:13,519 -> 00:00:16,120] 我需要一个人去帮我去做Google engineer
[00:00:16,120 -> 00:00:17,640] 就如果我们去做Google search
[00:00:17,640 -> 00:00:19,199] 每个人都会去做这件事情
[00:00:19,199 -> 00:00:21,519] 每个人可能都应该会去做Prompt engineer
[00:00:21,519 -> 00:00:23,960] 那这个apply到底是有什么样的变化
[00:00:23,960 -> 00:00:24,839] 以后会怎么发展
[00:00:24,839 -> 00:00:29,399] 嗯 就是也是一个我们日后可以讨论的一个有意思的话题吧
[00:00:00,000 -> 00:00:03,759] 对,这个其实我本来是想留到最后有一个类似的讨论嘛,
[00:00:03,759 -> 00:00:06,719] 其实就是我们吃饭的时候和我当时总结的问题第4个,
[00:00:06,719 -> 00:00:12,160] 就是当JPT把它的reinforcement learning这个layer开放给大家的时候,
[00:00:12,160 -> 00:00:18,160] 它是不是市面上现在所有的这种所谓的垂直大模型之类的全都会被干掉,
[00:00:18,160 -> 00:00:20,359] 我们的观点是会的,对吧?
[00:00:20,359 -> 00:00:22,079] 我觉得只是reinforcement learning嘛,
[00:00:22,079 -> 00:00:25,039] 就是说比如说它只是如果把那个,
[00:00:25,039 -> 00:00:28,039] 比方说fine tuning这一块也放也放开的更彻底的话,
[00:00:00,000 -> 00:00:02,000] 你觉得足够吗?还是说必须得要把
[00:00:02,000 -> 00:00:04,480] reinforcement learning这块放开才可以。
[00:00:05,480 -> 00:00:07,919] 我在那个文章里边一直强调的就是
[00:00:07,919 -> 00:00:10,960] Fenton这个词现在太broad,所以说大家在说
[00:00:10,960 -> 00:00:13,199] Fenton的时候,不同的人指代的是不同的意思,
[00:00:13,199 -> 00:00:15,759] 所以就不能随便的使用那个词。
[00:00:16,079 -> 00:00:19,039] 呃,然后我我的理解啊,就是
[00:00:19,039 -> 00:00:22,280] 卢奕欢,你纠正我,随时纠正我,就是
[00:00:22,280 -> 00:00:26,920] GPT4是它的底层大模型,接下来的那一层是
[00:00:26,920 -> 00:00:29,039] reinforcement learning with human feedback,
[00:00:00,000 -> 00:00:02,120] 但是其实你也可以用不同的feedback
[00:00:02,120 -> 00:00:05,879] 你可以让他直接只跟code来对话
[00:00:05,879 -> 00:00:09,240] 然后给他要构建相应的reward function来做
[00:00:09,240 -> 00:00:11,560] 那这一层其实自由度是非常之高的
[00:00:11,560 -> 00:00:13,919] 然后它能带来的效果也非常之
[00:00:13,919 -> 00:00:18,039] 就是为什么GPT3大家都觉得很这样子
[00:00:18,039 -> 00:00:20,399] 但是XGPT就觉得很惊艳
[00:00:20,399 -> 00:00:23,399] 除了它本身3到3.5可能有一些变化之外
[00:00:23,399 -> 00:00:27,239] 就是它在reinforcement with human feedback这一步
[00:00:27,239 -> 00:00:28,960] 和人的alignment做得太好了
[00:00:00,000 -> 00:00:01,760] 所以说所有人都懂了
[00:00:01,760 -> 00:00:04,160] 所有人突然懂了它是一个特别牛逼的东西
[00:00:04,160 -> 00:00:08,160] 那你这这是跟人类的偏好的alignment
[00:00:08,160 -> 00:00:10,560] 但是你也可以跟不同的知识
[00:00:10,560 -> 00:00:12,439] 不同的能力去进行alignment
[00:00:12,439 -> 00:00:15,119] 所以说这能解锁的可能性是非常之高的
[00:00:15,119 -> 00:00:16,960] 再往下是prompt对吧
[00:00:16,960 -> 00:00:21,399] 然后现在的prompt也是给了你一个非常死的方式
[00:00:21,399 -> 00:00:23,120] 但是这个prompt其实是在
[00:00:23,120 -> 00:00:25,960] Chad的这个reinforcement learning的
[00:00:25,960 -> 00:00:28,960] 这个环节之下的一个方式之一
[00:00:00,000 -> 00:00:05,919] 我的预判是它会开放更多更定制化的prompt的方式
[00:00:05,919 -> 00:00:08,119] 比如说更长或者说全种可调
[00:00:08,119 -> 00:00:13,000] 或者说你可以给它的网上再多记一些东西等等
[00:00:13,000 -> 00:00:14,839] 或者说多开几层layer
[00:00:14,839 -> 00:00:17,199] 但是在网上它现在就能做到
[00:00:17,199 -> 00:00:18,199] 只是还没有做的东西
[00:00:18,199 -> 00:00:20,760] 就是把reinforcement learning这一步给开放出来
[00:00:20,760 -> 00:00:23,480] 我把从reinforcement learning到prompt
[00:00:23,480 -> 00:00:25,640] 这整个环节都叫funtune
[00:00:25,640 -> 00:00:29,039] 但是它和我们之前的funtune那种方式不一样
[00:00:00,000 -> 00:00:02,319] 因为就像那个罗伊说的
[00:00:02,319 -> 00:00:03,799] 他没有改变模型的权重
[00:00:03,799 -> 00:00:04,480] 对吧
[00:00:04,480 -> 00:00:07,160] 他没有改变楼型权重和之前我们的那个
[00:00:07,160 -> 00:00:08,359] Machine Learning的Phantom
[00:00:08,359 -> 00:00:10,359] 其实在这上面是有很大的区别
[00:00:10,359 -> 00:00:11,960] 其实他在做这个
[00:00:11,960 -> 00:00:13,240] 我稍微查一下
[00:00:13,240 -> 00:00:15,359] 就是他在做这种就所谓
[00:00:15,359 -> 00:00:17,600] Reinforcement Learning with Human Feedback的过程当中
[00:00:17,600 -> 00:00:21,079] 实际上也是改变了模型的参数的
[00:00:21,559 -> 00:00:24,399] 他是拿出来了Distill出来
[00:00:24,399 -> 00:00:27,199] 相当于就是把那个4里边的一部分拿出来了
[00:00:27,199 -> 00:00:28,960] 还是改变了4本身的权重
[00:00:00,000 -> 00:00:01,639] 他应该是改变了我的理解
[00:00:01,639 -> 00:00:03,600] 当然了没有人知道具体这个东西怎么做的
[00:00:03,600 -> 00:00:05,040] 但是我的理解就是说
[00:00:05,040 -> 00:00:08,599] 他首先先通过我们正常的语言模型的问题
[00:00:08,599 -> 00:00:10,359] 去训练出来一个普通的模型
[00:00:10,359 -> 00:00:12,759] 然后他用先把他
[00:00:12,759 -> 00:00:15,759] 首先他做了一个模型的clone
[00:00:15,759 -> 00:00:20,120] 然后这个模型专门去训练成他的一个reward模型
[00:00:20,120 -> 00:00:23,440] 这个reward模型就通过一些人工的数据
[00:00:23,440 -> 00:00:24,399] 去告诉他这个人
[00:00:24,399 -> 00:00:25,920] 这人告诉他好还是不好
[00:00:25,920 -> 00:00:28,239] 所以有一个clone就专门做这个reward模型
[00:00:00,000 -> 00:00:04,200] 然后他用这个reward模型去重新再用强化学习训练
[00:00:04,200 -> 00:00:06,480] 他之前的那个底层模型
[00:00:06,480 -> 00:00:08,519] 那在训练这个底层模型当中的时候
[00:00:08,519 -> 00:00:10,359] 这个底层模型实际上也是在变化的
[00:00:11,320 -> 00:00:14,439] 这个我觉得我们可以回头把instructGPT
[00:00:14,439 -> 00:00:16,160] 那个paper拿出来重新看一下
[00:00:16,160 -> 00:00:19,559] 我当时还因为张俊林在那个他的
[00:00:19,559 -> 00:00:21,679] 就是大语言模型技术经验那个里边
[00:00:21,679 -> 00:00:24,719] 有讲in-context learning和讲这方面的东西
[00:00:24,719 -> 00:00:26,239] 然后我看了他了以后
[00:00:26,239 -> 00:00:29,879] 我回去再看instructGPT和chaiGPT的两个论文是吧
[00:00:00,000 -> 00:00:03,399] 其实instruct cpt是蓝色的,chart cpt是绿色的
[00:00:03,399 -> 00:00:04,879] 就是他们在那个图里边
[00:00:04,879 -> 00:00:06,320] 然后流程是一样的
[00:00:06,320 -> 00:00:07,280] 就是你刚刚说的
[00:00:07,280 -> 00:00:09,439] 当然他们不光是做了一个reward model
[00:00:09,439 -> 00:00:11,720] 就是他们是先构造了一个
[00:00:11,720 -> 00:00:15,359] 用instruct和chart的方式去构造数据集
[00:00:15,359 -> 00:00:18,800] 然后去用大模型在这数据集下去perform
[00:00:18,800 -> 00:00:22,679] 然后在这个perform里边再用reward model去选择是
[00:00:22,679 -> 00:00:25,600] 就是他看的是哪个叫policy
[00:00:25,600 -> 00:00:26,280] 对吧
[00:00:26,280 -> 00:00:28,280] 我当时的理解
[00:00:00,000 -> 00:00:03,520] 就是这是一个distill或者说用in-context learning
[00:00:03,520 -> 00:00:08,400] 去选择性的激活大模型的不同的位置
[00:00:08,400 -> 00:00:10,320] 但是没有改变那个大模型本身
[00:00:10,320 -> 00:00:13,199] 就是说GPT-4它有这么多个参数是吧
[00:00:13,199 -> 00:00:14,519] 然后每个参数不同权重
[00:00:14,519 -> 00:00:16,199] 这个权重其实是没有改
[00:00:16,199 -> 00:00:20,440] 对 但是如果说你在in-context learning的这个部分
[00:00:20,440 -> 00:00:22,839] 其实in-context learning并没有learn任何的东西
[00:00:22,839 -> 00:00:25,600] 对 他只是说你提供了一些东西
[00:00:25,600 -> 00:00:28,440] 在额外的作为一个例子给他
[00:00:00,000 -> 00:00:04,480] 那个时候确实是指示说你可以理解成激活了模型的某一个部分
[00:00:04,480 -> 00:00:08,560] 但是我们在训练一个模型想通过强化学习的时候
[00:00:08,560 -> 00:00:13,720] 那我们需不需要找到的是一个更好的in-context examples呢
[00:00:13,720 -> 00:00:17,719] 还是我们要找到的是一个模型在不提供in-context example
[00:00:17,719 -> 00:00:20,600] 或者提供同样类似in-context example的时候
[00:00:20,600 -> 00:00:21,839] 得到一个更好的结果
[00:00:21,839 -> 00:00:22,679] 我觉得是后者
[00:00:22,679 -> 00:00:23,480] 我也觉得是后者
[00:00:23,480 -> 00:00:24,440] 如果是后者的话
[00:00:24,440 -> 00:00:26,800] 那就比模型本身就要发生一些变化
[00:00:26,800 -> 00:00:27,199] 对吗
[00:00:00,000 -> 00:00:03,439] 这就是Gin林里边说in context learning的魔力
[00:00:03,439 -> 00:00:06,200] 就是模型的权重并没有发生变化的情况下
[00:00:06,200 -> 00:00:07,559] 它却能改
[00:00:07,559 -> 00:00:09,119] 有一个具体的截图
[00:00:09,119 -> 00:00:10,439] 我回头可以找出来
[00:00:10,439 -> 00:00:11,480] 就是基本
[00:00:11,480 -> 00:00:13,599] 而且这里边还有一个很神奇的点
[00:00:13,720 -> 00:00:16,320] 就是在你in context learning的情况下
[00:00:16,320 -> 00:00:18,839] 你不需要给他提就是一个y
[00:00:18,839 -> 00:00:21,480] 你给他提供的例子是y等于这些x
[00:00:21,480 -> 00:00:21,800] 对吧
[00:00:21,800 -> 00:00:22,719] y等于这些x
[00:00:22,719 -> 00:00:25,519] 然后你给他一个新的一堆x的项链
[00:00:25,519 -> 00:00:26,760] 然后他能得到一个新的y
[00:00:26,760 -> 00:00:29,399] 结果他发现你在in context learning给他提供的时候
[00:00:00,000 -> 00:00:03,600] 只要是一个X的集合和一个Y的集合对应起来就行了
[00:00:03,600 -> 00:00:05,599] 你不需要它的等号一一对应
[00:00:05,599 -> 00:00:07,200] 它都能学到东西
[00:00:07,200 -> 00:00:08,560] 所以就是
[00:00:08,560 -> 00:00:09,320] 不是学到东西
[00:00:09,320 -> 00:00:10,759] 只是说你提供了这个例子
[00:00:10,759 -> 00:00:11,400] 它就可以
[00:00:11,400 -> 00:00:13,519] 那可以像这样方式去表现
[00:00:13,519 -> 00:00:14,000] 对
[00:00:14,000 -> 00:00:14,480] 对
[00:00:14,480 -> 00:00:15,519] 就是它这个
[00:00:15,519 -> 00:00:18,920] 所以就是in context learning的魔力起码
[00:00:18,920 -> 00:00:19,679] according to Jun
[00:00:19,679 -> 00:00:20,199] 一样就是
[00:00:20,199 -> 00:00:21,000] 就是
[00:00:21,000 -> 00:00:22,879] 看来其实应该连线他
[00:00:22,879 -> 00:00:23,440] 对
[00:00:23,440 -> 00:00:24,559] 就是我没有
[00:00:24,559 -> 00:00:28,760] 我我我我基于对他的信任和那个理解
[00:00:00,000 -> 00:00:02,720] 也就是说他不需要改变模型成就
[00:00:02,720 -> 00:00:07,320] 就可以去激发他举一反三的表现
[00:00:07,320 -> 00:00:09,080] 但是在你训练一个模型
[00:00:09,080 -> 00:00:10,759] 训练一个更好的模型
[00:00:10,759 -> 00:00:13,400] 去完成同样的income tax earning的时候
[00:00:13,400 -> 00:00:16,239] 你是不是应该改变这个模型的参数呢
[00:00:16,239 -> 00:00:18,600] 我不确定这是一个问号
[00:00:18,600 -> 00:00:20,399] 我换一个举个例子
[00:00:20,399 -> 00:00:24,399] 就是说我们现在可能会有一个LAMA模型
[00:00:24,399 -> 00:00:25,000] 对吗
[00:00:25,000 -> 00:00:25,359] 对
[00:00:25,359 -> 00:00:27,960] 如果我们想要通过一个强化学习
[00:00:27,960 -> 00:00:29,679] 去强化这个LAMA模型的话
[00:00:00,000 -> 00:00:05,200] 那么我们原来能够提供的这个例子是123
[00:00:05,200 -> 00:00:08,599] 我是不是强化了之后给了强化的模型
[00:00:08,599 -> 00:00:09,759] 同样的例子123
[00:00:09,759 -> 00:00:11,199] 它应该给我一个更好的结果
[00:00:11,560 -> 00:00:12,080] 是
[00:00:12,359 -> 00:00:14,160] 那这个模型本身变化了吗
[00:00:18,719 -> 00:00:22,239] 未必还是我还是想说
[00:00:22,239 -> 00:00:24,879] 你是把这个看成了一部对吧
[00:00:24,879 -> 00:00:26,280] 我给了它的例子
[00:00:26,280 -> 00:00:29,480] 然后我去让它调整了以后
[00:00:00,000 -> 00:00:07,839] 整个模型变得更好,但是我在这一步里面看到了两步,一个是底层的模型和它的这个reinforcement learning这一步,
[00:00:07,839 -> 00:00:18,480] 呃,就是所谓的in context learning的激活这一步,嗯,就是就是这个人可能还是同样一个人,我不是确定这里应该打比方,anyway还是说模型吧,
[00:00:18,480 -> 00:00:27,920] 就是他的模型权重仍然是这些,然后你的这个in context learning,你把它,就是其实就是你那天说的这个nonparametric,parametric两步,
[00:00:00,000 -> 00:00:01,919] 你把它看成一个系统的话
[00:00:01,919 -> 00:00:03,520] 整个系统是发生了变化
[00:00:03,520 -> 00:00:05,719] 但是你回到这个底层大模型的话
[00:00:05,719 -> 00:00:07,799] 底层大模型是不是全中一定发生变化
[00:00:07,799 -> 00:00:08,480] 我不确定
[00:00:08,480 -> 00:00:11,960] 因为有可能底层大模型的全中不发生变化的情况下
[00:00:11,960 -> 00:00:15,480] 你的in-context learning的能力是强的
[00:00:15,480 -> 00:00:20,920] 你去追逐你in-context learning这些例子回答的更好
[00:00:20,920 -> 00:00:23,120] 去overfit这些例子的情况下
[00:00:23,120 -> 00:00:25,120] 反而它的模型的能力
[00:00:25,120 -> 00:00:27,559] in-context learning的能力有可能会下降
[00:00:00,000 -> 00:00:03,560] 这个我们可以展开到时候以后我们可以接着聊
[00:00:03,560 -> 00:00:04,280] 对
[00:00:04,280 -> 00:00:09,480] 所以其实现在业界大家并不知道在prechain之后还有什么办
[00:00:09,480 -> 00:00:12,640] prechain之后做的那些事情有没有可能改变这个
[00:00:12,640 -> 00:00:14,160] foundation model的这个模型选种
[00:00:14,160 -> 00:00:15,160] 我们并不知道
[00:00:15,160 -> 00:00:17,399] 在今天之前我的理解都是没有改变
[00:00:17,760 -> 00:00:19,920] 今天我会打一个问号
[00:00:19,920 -> 00:00:21,320] 也希望观众如果
[00:00:21,320 -> 00:00:22,640] 对我们到时候可以再讨论
[00:00:22,640 -> 00:00:23,239] 对
[00:00:23,239 -> 00:00:25,199] 这个可能是我理解上的一些偏差
[00:00:25,199 -> 00:00:25,640] 我觉得
[00:00:25,640 -> 00:00:26,039] 对
[00:00:26,039 -> 00:00:27,719] 就是拆开再继续
[00:00:27,719 -> 00:00:28,359] 拆开讨论
[00:00:00,000 -> 00:00:03,799] 我们可以在群里也可以请到时候观众们来确定知道的告诉我们
[00:00:03,799 -> 00:00:06,040] 但是这个问题再复述一下的话
[00:00:06,040 -> 00:00:10,119] 就是pre-trained大模型在in-context learning之后
[00:00:10,640 -> 00:00:14,400] 第一,in-context learning有没有改变权重
[00:00:14,400 -> 00:00:15,439] 大概率是没有
[00:00:15,439 -> 00:00:18,199] 但是第二,为了让它的performance更好
[00:00:18,199 -> 00:00:21,519] 它需不需要或者说应不应该去改变模型的权重
[00:00:21,519 -> 00:00:25,199] 在强化学习的过程当中有没有改变模型的权重
[00:00:25,199 -> 00:00:27,039] 我的观点是是你的观点是
[00:00:00,000 -> 00:00:03,759] 我的观点是 distill 是better distillation
[00:00:03,759 -> 00:00:05,719] 然后你的观点是要改变模型的参数
[00:00:05,719 -> 00:00:08,039] 我觉得它的模型的参数会发生一些变化
[00:00:08,039 -> 00:00:08,320] 嗯
[00:00:08,320 -> 00:00:08,839] 嗯
[00:00:08,839 -> 00:00:11,960] 其实就是因为前面这个柯代表做了一个这个
[00:00:11,960 -> 00:00:14,560] clarification 就是你对于这个fine tune的这个定义
[00:00:14,560 -> 00:00:16,239] 可能我就可能是因为
[00:00:16,239 -> 00:00:18,359] 因为大家现在如果大家讲fine tune的时候
[00:00:18,359 -> 00:00:21,160] 可能更多讲的是SFT的那个环节
[00:00:21,160 -> 00:00:24,440] 但是你这样就quick clarify这个是不是因为在
[00:00:24,440 -> 00:00:26,239] 传统的machinery里边
[00:00:26,239 -> 00:00:28,559] 当你讲到fine tune的时候就默认你其实改变了
[00:00:28,559 -> 00:00:29,800] 这个模型的这个权重
[00:00:00,000 -> 00:00:02,680] 你觉得如果我们用Fenton这个词就可能就默认了
[00:00:02,680 -> 00:00:06,879] 我们在,呃,其实SFT这个这个这个过程是不改变这个
[00:00:06,879 -> 00:00:10,199] 对,这个就是我在写另一个文章的第二个问题时候
[00:00:10,199 -> 00:00:14,599] 我反复跟人沟通,然后发现跟所有的传统的机器学习的人
[00:00:14,599 -> 00:00:17,399] 但是没有接触过,或者说没有对大模型了解非常深
[00:00:17,399 -> 00:00:19,559] 或者非常细的人的时候,总是会出现的一个
[00:00:19,879 -> 00:00:23,839] 呃,争执,就是Fenton,尤其是对这个词的definition的区别
[00:00:23,839 -> 00:00:28,239] 他们心中一想到Fenton,第一想的就是我去改变模型的权重
[00:00:00,000 -> 00:00:02,319] 但是我说不对啊,你看这有incount task learning,
[00:00:02,319 -> 00:00:05,559] 它不需要改变模型权这种情况下也能给你输出更好的结果,
[00:00:05,559 -> 00:00:09,519] 然后我就会发现,哦,是大家对这个词的定义已经产生了很大的变化。
[00:00:09,519 -> 00:00:13,400] 对,其实刚才有一点,我觉得可以我想来继续问一下,
[00:00:13,400 -> 00:00:16,120] 因为其实我们刚才提到这个这个话题,
[00:00:16,120 -> 00:00:25,160] 其实是因为你聊到说,如果等到OpenAI开放了这个RHF的这一些更多的这个权限以后,
[00:00:25,160 -> 00:00:27,160] 我们有很多这个可以enable的东西。
[00:00:00,000 -> 00:00:06,480] 那也许有些人会就是那我想可能有一些朋友会想说那到底
[00:00:06,480 -> 00:00:09,320] 开放这个有什么是我们现在不能做的
[00:00:09,320 -> 00:00:12,519] 而你觉得说等下开放这个全场我们就可以做的
[00:00:12,519 -> 00:00:14,560] 因为现在其实我们是可以做一些
[00:00:14,560 -> 00:00:18,559] 呃一定程度上的这个呃sft对吧
[00:00:18,559 -> 00:00:22,719] 然后还有包括这个呃拉玛的话其实也可以做一些嘛
[00:00:22,719 -> 00:00:25,239] 就是你觉得开放后有哪些都可以做的
[00:00:25,239 -> 00:00:26,000] 我先直接回答
[00:00:26,000 -> 00:00:27,519] 然后再prompt 卢毅来回答啊
[00:00:27,519 -> 00:00:28,920] 就是我我先我先不用
[00:00:00,000 -> 00:00:03,839] 然后这个这个这个可以我帮他就是我们当时聊到这个话题
[00:00:03,839 -> 00:00:06,719] 就是因为他现在在做的这个业务
[00:00:06,719 -> 00:00:10,480] 就是去做一个更垂直的模型
[00:00:10,480 -> 00:00:13,599] 就是他们两条一步一个是应用市场上
[00:00:13,599 -> 00:00:14,880] 一个是自己开发
[00:00:14,880 -> 00:00:18,079] 然后我们当时说如果JPT
[00:00:18,079 -> 00:00:20,239] 如果OpenAI开放了这个layer的话
[00:00:20,239 -> 00:00:22,640] 会不会所有的垂直模型都没有意义了
[00:00:22,640 -> 00:00:24,440] 然后也很有可能是这样子的
[00:00:24,440 -> 00:00:26,760] 因为它模型performance好太多
[00:00:26,760 -> 00:00:29,239] 它能给你开放这样一个定制的环节的话
[00:00:00,000 -> 00:00:02,200] 完全就把你们所有人全都淹死了
[00:00:02,200 -> 00:00:05,639] 然后我举一个例子就是它开放能达到什么程度
[00:00:05,639 -> 00:00:07,919] 我举的例子现实中不会存在
[00:00:07,919 -> 00:00:10,599] 但是它可以帮助大家思考这件事
[00:00:10,599 -> 00:00:14,119] 就是我们现在人是无法学会狗说话的
[00:00:14,119 -> 00:00:17,719] 假设狗真的能给GPT提供一个reward function
[00:00:17,719 -> 00:00:21,320] 那你让GPT去学狗叫
[00:00:21,320 -> 00:00:23,440] GPT很有可能就会学会狗的语言
[00:00:23,960 -> 00:00:24,519] 不存在
[00:00:24,519 -> 00:00:26,079] 但是或者说就是代码
[00:00:26,079 -> 00:00:27,640] 或者说是no
[00:00:27,640 -> 00:00:29,000] 是一个更实际的例子
[00:00:00,000 -> 00:00:02,799] 就比如说我们的医生是很难训练的
[00:00:02,799 -> 00:00:06,320] 因为你很多知识不是是否问题
[00:00:06,320 -> 00:00:09,080] 它不是一个rule based这个function
[00:00:09,080 -> 00:00:10,400] 你看到了一些东西
[00:00:10,400 -> 00:00:12,320] 你怎么会得到这样的诊断
[00:00:12,320 -> 00:00:14,519] 其实不同的医生他给你的
[00:00:14,519 -> 00:00:16,800] 根据他的经验直觉什么的都不一样
[00:00:17,160 -> 00:00:20,000] 但是假设我们有一个高质量的
[00:00:20,000 -> 00:00:22,120] 这样的一个数据集
[00:00:22,120 -> 00:00:24,480] 就是说病例所有医生能看到的
[00:00:24,480 -> 00:00:25,640] 收集到的所有信息
[00:00:25,640 -> 00:00:27,199] 和他做出来的判断
[00:00:27,199 -> 00:00:29,440] 和也许他写下来了一些判断的依据
[00:00:00,000 -> 00:00:01,000] 也許沒有寫
[00:00:01,199 -> 00:00:03,000] 我們也許就可以拿這樣一個東西
[00:00:03,000 -> 00:00:06,799] 去構建一個這個reward的數據集
[00:00:06,799 -> 00:00:11,000] 然後讓GPT學會怎麼樣子去進行專家級的
[00:00:11,000 -> 00:00:13,000] 就真正專家級的醫療診斷
[00:00:13,000 -> 00:00:16,920] 對 這個知識不是在現有的那個所謂的
[00:00:16,920 -> 00:00:19,000] 案例庫裡面直接存在的
[00:00:19,000 -> 00:00:20,800] GPT現有的能力
[00:00:20,800 -> 00:00:24,399] 也許能去模仿一些人在這個案例庫裡的對話
[00:00:24,399 -> 00:00:28,000] 但是他得不到那個醫生下診斷的那個知識
[00:00:00,000 -> 00:00:03,960] 可是你如果有了这个reward数据集的话
[00:00:03,960 -> 00:00:07,679] GPT有可能就能得到一个真正医生的那个
[00:00:07,679 -> 00:00:08,560] 呃 知识
[00:00:08,560 -> 00:00:10,039] 嗯 明白你的意思
[00:00:10,039 -> 00:00:12,880] 我其实我就想先退一步说这个问题
[00:00:12,880 -> 00:00:15,279] 就是我们首先我们想说
[00:00:15,279 -> 00:00:18,839] OpenAI给开放一个可以定制化或者
[00:00:18,839 -> 00:00:20,879] 个性化这个模型的这个服务
[00:00:20,879 -> 00:00:22,640] 到底像我们要解决什么样的问题
[00:00:22,640 -> 00:00:25,079] 我们可能有几类
[00:00:25,079 -> 00:00:28,960] 几个东西是我们需要去做个性化的
[00:00:00,000 -> 00:00:02,560] 一个是它就是知识
[00:00:02,560 -> 00:00:05,280] 你GPT学习的知识就是
[00:00:05,280 -> 00:00:07,200] 比如说我们简单的说就是Wikipedia
[00:00:07,200 -> 00:00:08,800] 或者说我们互联网上的文字
[00:00:08,800 -> 00:00:10,279] 你上面获得的这些知识
[00:00:10,279 -> 00:00:13,599] 那有一些实际上是专有的一些知识
[00:00:13,599 -> 00:00:16,600] 比如说是就像你说的医疗方面的知识
[00:00:17,480 -> 00:00:19,199] 或者是法律方面的知识
[00:00:19,199 -> 00:00:22,839] 如何让这个模型去专门去复习也好
[00:00:22,839 -> 00:00:24,519] 或者是专门去强调这方面的知识
[00:00:24,519 -> 00:00:26,719] 就把这些知识放到它的参数当中
[00:00:00,000 -> 00:00:03,680] 那可能是一个我们想去做个性化的部分
[00:00:03,680 -> 00:00:05,240] 这另外一个其实呢
[00:00:05,240 -> 00:00:08,199] 更多的是你已经拥有了这个知识
[00:00:08,199 -> 00:00:10,080] 拥有了这种推理能力
[00:00:10,080 -> 00:00:14,359] 那如何我去用一种方式去把你的这个能力
[00:00:14,359 -> 00:00:16,320] 应用在我的这种任务上
[00:00:16,320 -> 00:00:19,679] 也是一种个性化的思路
[00:00:19,679 -> 00:00:22,640] 那么这其实往往就是我们可以说一下
[00:00:22,640 -> 00:00:23,480] 传统来讲
[00:00:23,480 -> 00:00:24,920] 如果要去达到这两个目标
[00:00:24,920 -> 00:00:27,679] 我有一个比如说是白盒模型
[00:00:00,000 -> 00:00:03,240] 我们如果是就像GPT现在我们可以认为是
[00:00:03,240 -> 00:00:04,919] 对于我们个人来讲我们是黑盒模型
[00:00:05,360 -> 00:00:07,679] 那如果假如说我们像Lemma或者说对OpenAI
[00:00:07,679 -> 00:00:09,359] 他们自己来讲这就是一个白盒模型
[00:00:09,359 -> 00:00:11,359] 对白盒模型想去完成这个目标
[00:00:11,359 -> 00:00:13,480] 可能需要做的任务稍微有一些不一
[00:00:13,480 -> 00:00:15,480] 就是方法稍微有一点不同
[00:00:16,039 -> 00:00:18,559] 这也是我经常收到的一些问题
[00:00:18,679 -> 00:00:22,640] 就是如何去把一个大语言模型
[00:00:22,640 -> 00:00:24,440] 应用在一个特定的领域当中
[00:00:25,480 -> 00:00:26,879] 那其实我往往就想说
[00:00:26,879 -> 00:00:28,440] 如果你要去学到知识
[00:00:00,000 -> 00:00:03,200] 其实现在学校知识最好的方法就是类似
[00:00:03,200 -> 00:00:05,360] language modeling task一样的方法
[00:00:05,360 -> 00:00:08,400] 或者是instruction finding
[00:00:08,400 -> 00:00:09,599] 所谓instruction finding
[00:00:09,599 -> 00:00:11,800] 就比如说像Flynn他们的做法
[00:00:11,800 -> 00:00:14,199] 就是我有这么一个问题或者是有一个fixed problem
[00:00:14,199 -> 00:00:15,279] 那最后出来一个结果
[00:00:15,279 -> 00:00:17,120] 这种方式也可以学到知识
[00:00:17,120 -> 00:00:20,320] 那么我给了一 textbook的话
[00:00:20,320 -> 00:00:21,719] 也是可以学到知识的
[00:00:21,719 -> 00:00:24,160] 我自己之前自己玩的时候也去做这种东西
[00:00:24,160 -> 00:00:26,559] 就是说你给他这个textbook做这种
[00:00:26,559 -> 00:00:28,600] causal language modeling task
[00:00:00,000 -> 00:00:02,480] 也就是next token production这种task
[00:00:02,480 -> 00:00:03,640] 它就确实会
[00:00:03,640 -> 00:00:07,160] 等等你说你自己玩和你刚做这个是
[00:00:07,160 -> 00:00:08,960] 就相当于是我自己的一些探索
[00:00:08,960 -> 00:00:10,439] 是在GPT之后还是之前
[00:00:10,439 -> 00:00:12,839] 在GPT之后做这些探索
[00:00:12,839 -> 00:00:15,640] 然后用的一些也是开源的这种
[00:00:15,640 -> 00:00:18,800] 对 decoder这样的模型去做这种事情
[00:00:18,800 -> 00:00:19,920] 然后我就会去
[00:00:19,920 -> 00:00:23,679] 比如说我对房地产这方面的东西感兴趣
[00:00:23,679 -> 00:00:27,760] 然后我就把他们的教科书教材考试的题
[00:00:00,000 -> 00:00:03,120] 全都变成那个语料
[00:00:03,120 -> 00:00:04,280] 收集下来以后
[00:00:04,280 -> 00:00:05,679] 我去训练这个模型
[00:00:05,679 -> 00:00:07,160] 原来我同样问这个问题
[00:00:07,160 -> 00:00:08,199] 他会回答一个答案
[00:00:08,199 -> 00:00:09,599] 也许他有一点合理
[00:00:09,599 -> 00:00:12,080] 但是我把这个教科书给他了以后
[00:00:12,080 -> 00:00:13,519] 同样这个问题他回答答案
[00:00:13,519 -> 00:00:16,399] 他就会更像教科书里面讲的那样的答案
[00:00:16,399 -> 00:00:16,839] 对
[00:00:16,839 -> 00:00:19,079] 他就是这就是相当于学到了一些知识
[00:00:19,079 -> 00:00:21,960] 那这种其实就是我觉得像Cosmos
[00:00:21,960 -> 00:00:23,640] Languages 会更合适的东西
[00:00:23,640 -> 00:00:25,600] 那还有一种
[00:00:25,600 -> 00:00:27,280] 我这加个pin回头再讨论
[00:00:27,280 -> 00:00:27,920] 你先继续
[00:00:27,920 -> 00:00:28,399] 好的
[00:00:00,000 -> 00:00:03,279] 然后还有就是如何去format最后的结果
[00:00:03,279 -> 00:00:06,519] 也就是我有一个也是非常overloaded term
[00:00:06,519 -> 00:00:07,599] 就是alignment
[00:00:07,599 -> 00:00:10,119] 如何把这个GPT的结果
[00:00:10,119 -> 00:00:12,839] 它也许GPT已经拥有这样的能力
[00:00:12,839 -> 00:00:15,240] 但是我希望你去完成的一件事情
[00:00:15,240 -> 00:00:18,800] 是按照我的这个任务的format去完成
[00:00:18,800 -> 00:00:20,399] 或者说这些步骤去完成
[00:00:20,399 -> 00:00:23,320] 那这个时候如果我能够提供一些
[00:00:23,320 -> 00:00:26,600] 比如说训练数据的话
[00:00:26,600 -> 00:00:29,239] 那其实可以达到的就是两个效果
[00:00:00,000 -> 00:00:04,200] 一个就是我不需要再通过in context learning的方式去达到这个效果
[00:00:04,200 -> 00:00:06,360] 因为in context learning我提供了一些例子
[00:00:06,360 -> 00:00:08,359] 那它就可能prompt的更长
[00:00:08,359 -> 00:00:09,800] 然后更慢
[00:00:09,800 -> 00:00:10,720] 更加的贵
[00:00:10,720 -> 00:00:13,119] 那如果在之前训练的过程当中
[00:00:13,119 -> 00:00:14,480] 我就已经提供了这些例子
[00:00:14,480 -> 00:00:15,839] 我可以提供更多的例子
[00:00:15,839 -> 00:00:20,199] 我在去做inference或者说去做生成的时候
[00:00:20,199 -> 00:00:23,320] 我就不需要提供额外的例子去做这件事情
[00:00:23,320 -> 00:00:25,039] 这也是它的一个好处吧
[00:00:25,039 -> 00:00:27,320] 就是我觉得也不能
[00:00:00,000 -> 00:00:02,960] 这也是就我们回到那个customization的好处
[00:00:02,960 -> 00:00:05,559] 那我 我们这就又说到
[00:00:05,559 -> 00:00:07,839] 呃我们不得不说就是强化学习
[00:00:07,839 -> 00:00:09,519] 它到底开放的是什么东西
[00:00:09,519 -> 00:00:13,119] 我个人的观点可能和你稍微有一点点不一样
[00:00:13,119 -> 00:00:15,640] 就是我自己觉得
[00:00:15,640 -> 00:00:18,160] 你去开放你要完成的
[00:00:18,160 -> 00:00:19,440] 如果是学习知识的话
[00:00:19,440 -> 00:00:20,480] 那是一个思路
[00:00:20,480 -> 00:00:23,120] 如果开放的是format alignment的话
[00:00:23,120 -> 00:00:24,480] 是一种不同的开放
[00:00:24,480 -> 00:00:26,000] 如果你要开放强化学习
[00:00:26,000 -> 00:00:29,760] 它是一种第三种我觉得不一样的类型的任务
[00:00:00,000 -> 00:00:02,439] 什么时候我觉得用强化学习
[00:00:02,439 -> 00:00:04,799] 我自己之前就在想有几个可能性
[00:00:04,799 -> 00:00:08,000] 一个是说你需要完成一系列
[00:00:08,000 -> 00:00:10,320] 就a sequence of decision
[00:00:10,320 -> 00:00:11,839] 而不是一个decision
[00:00:11,839 -> 00:00:13,279] 然后直接出一个结果的
[00:00:14,759 -> 00:00:17,039] 比如说我们下棋更像是一个
[00:00:17,280 -> 00:00:22,039] 一系列决定最后获得的一个结果
[00:00:22,760 -> 00:00:25,239] 这种东西我觉得更适合于强化学习
[00:00:25,359 -> 00:00:28,519] 如果只是说我说一个东西回一个东西
[00:00:00,000 -> 00:00:02,120] 然后这个任务就结束了
[00:00:02,120 -> 00:00:04,719] 永远都是这种就是一回合的
[00:00:04,719 -> 00:00:09,279] 其实并不见得都是适合强化学习的
[00:00:09,279 -> 00:00:10,359] 不见得是强化学习
[00:00:10,359 -> 00:00:15,119] 也许就是我们所谓说的这种传统的方式去微调模型
[00:00:15,119 -> 00:00:17,839] 或者是更加方便直接的一个方法
[00:00:17,839 -> 00:00:19,519] 因为你会直接告诉他的结果
[00:00:19,519 -> 00:00:21,600] 那如果是一个sequence of decision
[00:00:21,600 -> 00:00:23,199] 那你其实就比如说conversation
[00:00:23,199 -> 00:00:24,960] 就是一个sequence of decision
[00:00:24,960 -> 00:00:26,239] 你回答一个东西
[00:00:26,239 -> 00:00:26,960] 你说了一个东西
[00:00:26,960 -> 00:00:28,280] 你他别人又说了一个
[00:00:00,000 -> 00:00:02,799] 这个时候他们别人再根据你的回答
[00:00:02,799 -> 00:00:04,320] 可能会说完全不一样的话
[00:00:04,320 -> 00:00:04,719] 对吗
[00:00:04,719 -> 00:00:07,360] 你完全猜不出别人可能会怎么回答
[00:00:07,360 -> 00:00:08,800] 在你说了这句话之前
[00:00:09,359 -> 00:00:15,679] 像这种一系列互相相depend在一起的问题
[00:00:15,679 -> 00:00:17,839] 就更适合于强化学习这个问题
