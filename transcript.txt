[00:00:00,000 -> 00:00:03,000] 其实一直以来我都是一个Deep Learning大喷子
[00:00:03,000 -> 00:00:05,000] 搞了一段时间Deep Learning之后
[00:00:05,000 -> 00:00:07,000] 有很多私货想要夹带一下
[00:00:07,000 -> 00:00:09,000] 所以就做了这样一个slides
[00:00:09,000 -> 00:00:11,000] 整个东西会比较长
[00:00:11,000 -> 00:00:12,000] 大约是一个小时左右
[00:00:12,000 -> 00:00:14,000] 但这次先发一个预览版
[00:00:14,000 -> 00:00:15,000] 用十分钟的时间
[00:00:15,000 -> 00:00:18,000] 我们简单的看一下Deep Learning的历史
[00:00:18,000 -> 00:00:19,000] 它为什么会work
[00:00:19,000 -> 00:00:22,000] 以及为什么大家这么喜欢它
[00:00:22,000 -> 00:00:26,160] 注意这并不是一个非常严谨的说
[00:00:26,839 -> 00:00:28,280] Technical Review
[00:00:28,280 -> 00:00:30,920] 而会是一个以广度优先的
[00:00:30,920 -> 00:00:33,600] 更加注重直觉的构建的
[00:00:34,039 -> 00:00:36,359] 这样一个Deep Learning的思惑
[00:00:37,119 -> 00:00:40,240] 注意我里面会有很多个人的见解
[00:00:40,240 -> 00:00:44,240] 因为想要把一些比较强烈的观点表达出来
[00:00:44,240 -> 00:00:45,240] 所以批判性的看法
[00:00:47,039 -> 00:00:49,759] 那么首先看一下在deployment之前
[00:00:49,759 -> 00:00:53,399] 大家搞Machine learning一般是分成几块
[00:00:53,399 -> 00:00:55,119] 有做feature的有做aggregation
[00:00:55,119 -> 00:00:56,840] 比如说pooling的有做最后的model
[00:00:56,840 -> 00:00:57,719] 比如说SVM的
[00:00:58,399 -> 00:01:00,640] 这样搞虽然分工比较好
[00:01:00,640 -> 00:01:03,399] 但是护住头护不了定
[00:01:04,120 -> 00:01:07,500] 大家做feature的未必是为了做model来优化
[00:01:08,200 -> 00:01:09,599] 各自为政 效果不是特别好
[00:01:10,400 -> 00:01:12,799] 但也不是说12年Deep Learning活了以后
[00:01:12,799 -> 00:01:13,799] 突然就出了一个东西
[00:01:13,799 -> 00:01:15,500] 把所有东西轴到一块它就work了
[00:01:16,200 -> 00:01:17,599] 其实历史比这个复杂很多
[00:01:18,400 -> 00:01:21,700] 不过我们说到Deep Learning很多二叔能详的术语
[00:01:21,700 -> 00:01:22,799] 比如说perception啊
[00:01:23,400 -> 00:01:28,959] multilayer neural network啊甚至是PyTorch的TensorFlow之类
[00:01:28,959 -> 00:01:33,599] 现代deep learning工具箱的独门绝活,symbolic auto differentiation等等
[00:01:33,599 -> 00:01:36,400] 其实他们在1950年代1960年代都已经出来了
[00:01:36,400 -> 00:01:38,799] 只是因为各种原因
[00:01:38,799 -> 00:01:44,000] 大家一直没有什么办法能够train超过四层或者三四层的neural network
[00:01:44,000 -> 00:01:45,560] 一搞就overfit
[00:01:45,560 -> 00:01:48,079] 这是为什么deep learning在90年以后
[00:01:48,079 -> 00:01:50,079] 我想95年以后就迎来了一个低谷
[00:01:51,439 -> 00:01:55,680] 直到2012年搞了一个英文GNET的比赛
[00:01:56,200 -> 00:01:58,239] 出了一个叫AlexNet的neural network
[00:01:58,560 -> 00:02:00,079] 它基本上就是以前的deep learning
[00:02:00,079 -> 00:02:04,280] 但是把之前所有的流行的算法全都按在地上爆锤
[00:02:04,959 -> 00:02:06,599] 比如说你就可以看见他这里
[00:02:07,280 -> 00:02:10,439] Everweight比之前的流行算法都低了8个点
[00:02:11,159 -> 00:02:13,719] 大家一看你一下把Neural Network
[00:02:13,719 -> 00:02:15,599] 从比如说两三层一下搞多了八层
[00:02:15,599 -> 00:02:17,639] 非常deep deep learning牛逼牛逼
[00:02:18,599 -> 00:02:19,879] 就吸引了很多注意力
[00:02:19,919 -> 00:02:21,800] 当然deep learning名字的来源并不是这里
[00:02:21,800 -> 00:02:23,159] 这个我们以后再说
[00:02:24,120 -> 00:02:26,500] 那这个AlexNet它为啥突然就work了呢
[00:02:26,780 -> 00:02:28,819] 从技术上来说主要是两个原因
[00:02:28,819 -> 00:02:29,939] 一个是他用了dropout
[00:02:30,300 -> 00:02:31,780] 把overfitting这个问题解决了
[00:02:31,860 -> 00:02:34,379] 但我觉得dropout从后来的历史来看
[00:02:34,379 -> 00:02:36,780] 也不是最好的能解决overfitting的方法
[00:02:37,300 -> 00:02:38,780] 它可能更重要的一个原因
[00:02:38,780 -> 00:02:39,900] 非常核心的原因是说
[00:02:39,900 -> 00:02:41,500] 它解决了grid and vanishing这个问题
[00:02:42,020 -> 00:02:43,340] 那啥叫grid and vanishing呢
[00:02:43,340 -> 00:02:44,860] 任何一个Machine Learning的算法
[00:02:45,280 -> 00:02:46,759] 比如讲你用gridient dissonance来解
[00:02:46,759 -> 00:02:48,560] 它干的事情就是有一个目标函数
[00:02:48,560 -> 00:02:52,479] 然后这个算法就在目标函数的某一个点上面
[00:02:52,479 -> 00:02:55,240] 这个这个区面就是这个目标函数的形状
[00:02:55,240 -> 00:02:55,960] 在这个点上面
[00:02:56,120 -> 00:02:59,000] 它的目标是要走到这个区面的谷底
[00:02:59,199 -> 00:03:00,759] 也就是最小化这个目标函数
[00:03:01,080 -> 00:03:02,080] 或者是loss function
[00:03:02,680 -> 00:03:03,639] 它怎么去做这一点
[00:03:04,080 -> 00:03:07,000] 它会先从这个点出发向周围看一圈
[00:03:07,000 -> 00:03:09,000] 然后找到最陡峭的那个方向
[00:03:09,000 -> 00:03:11,000] 向那边跨一小步
[00:03:11,000 -> 00:03:12,000] 然后重复这个过程
[00:03:12,000 -> 00:03:13,000] 再向周围看一圈
[00:03:13,000 -> 00:03:16,000] 再朝最陡峭的方向走一小步
[00:03:16,000 -> 00:03:18,000] 直到走到谷底
[00:03:18,000 -> 00:03:20,000] 这样呢在这样的形状
[00:03:20,000 -> 00:03:21,000] 比如泡面的形状上面
[00:03:21,000 -> 00:03:23,000] 看上去还是比较简单的
[00:03:23,000 -> 00:03:25,319] 哪个方向最陡峭比较容易判断出来
[00:03:25,680 -> 00:03:27,120] 但是比如讲像右图这样
[00:03:27,360 -> 00:03:29,800] deep learning里面经常用到的一种activation function
[00:03:29,800 -> 00:03:30,520] 就是sigmoid
[00:03:30,960 -> 00:03:33,439] 在这边如果x在比较大的情况下
[00:03:33,439 -> 00:03:35,719] 比如说810它gradient就是0了
[00:03:36,159 -> 00:03:38,919] 那么如果对deep learning有一些初步的了解
[00:03:38,919 -> 00:03:39,919] 知道backpropagation
[00:03:39,919 -> 00:03:42,520] 啊验尸法则之类就知道那么一层
[00:03:42,520 -> 00:03:43,560] 它的gradient接近0
[00:03:43,560 -> 00:03:46,000] 很多层乘起来它gradient基本上就是0了所以在训练习法则之类就知道那么一层它的gradient接近零很多层乘起来它gradient基本上就是零了
[00:03:46,000 -> 00:03:50,199] 所以在训练的时候整个训练过程基本上就停滞了
[00:03:50,199 -> 00:03:52,000] 这也是为什么之前trim出来的原因之一
[00:03:52,000 -> 00:03:56,699] 当然gradient vanishing还有很多其他的解释
[00:03:56,699 -> 00:03:58,000] 这个我们到完整版再说
[00:03:58,000 -> 00:04:00,599] AlexNet它是怎么解决这个问题的呢
[00:04:00,599 -> 00:04:02,599] 他把sigmoid换成了relo
[00:04:02,599 -> 00:04:06,159] 那么可以看一下它这个形状就解决了
[00:04:06,159 -> 00:04:07,280] Grid Vanishing的问题
[00:04:07,280 -> 00:04:09,680] 因为它没有像之前的那样的形状
[00:04:09,680 -> 00:04:11,039] 具体为什么可以自己想一想
[00:04:12,479 -> 00:04:13,520] 这是非常重要的一点
[00:04:13,520 -> 00:04:15,680] 因为在很多相对独立的
[00:04:15,680 -> 00:04:17,120] Deep Learning的Worker中间
[00:04:17,519 -> 00:04:20,240] 大家用各自不同的方法都解决了这个问题
[00:04:20,480 -> 00:04:22,279] 只要你解决了这个问题效果就会特别好
[00:04:22,720 -> 00:04:25,000] 比如说AlexNet把它堆到了8层
[00:04:25,000 -> 00:04:29,000] 接下来ResNet用ResidueBlock把它堆到了100层
[00:04:29,000 -> 00:04:31,000] 后面堆到了1000层,效果特别好
[00:04:31,000 -> 00:04:35,000] 甚至在持续分析NLP领域的LSTM
[00:04:35,000 -> 00:04:38,000] 97年的时候也是从RN一路走过来
[00:04:38,000 -> 00:04:43,000] 也用类似的方法实现了这样Grid and Vanishing
[00:04:43,000 -> 00:04:46,060] 或者RN还有一个Grid and Explosion之类的解决也都实现了比Grid and Vanishing啊或者RN还有一个Grid and Explosion啊
[00:04:46,399 -> 00:04:47,259] 之类的解决
[00:04:47,339 -> 00:04:48,800] 也都实现了比较好的效果
[00:04:49,360 -> 00:04:50,459] 所以我个人认为
[00:04:50,740 -> 00:04:51,939] Grid and Vanishing是
[00:04:51,939 -> 00:04:54,000] Deep Learning为什么Work的一个重要的因素
[00:04:54,500 -> 00:04:55,699] 此外还有两个因素
[00:04:55,860 -> 00:04:57,399] 一个是我们终于有了
[00:04:57,660 -> 00:04:59,360] 大规模的人工标注的数据
[00:04:59,939 -> 00:05:00,540] 比如说
[00:05:01,300 -> 00:05:04,300] 以前Caltech 1.1大家做啊都做9000张图
[00:05:04,300 -> 00:05:05,560] 我觉得挺大的
[00:05:05,560 -> 00:05:07,439] 到到李飞飞他们组
[00:05:07,439 -> 00:05:08,199] 12年左右
[00:05:08,199 -> 00:05:08,800] 11年左右
[00:05:08,800 -> 00:05:11,920] 弄出来一个200万张图的ImageNet
[00:05:11,920 -> 00:05:12,879] 因为有这么多图
[00:05:12,879 -> 00:05:16,319] 所以Neural Network才不会overfit
[00:05:16,319 -> 00:05:17,279] 还有个重要的一点呢
[00:05:17,279 -> 00:05:19,639] 是大家终于知道用GPU来train了
[00:05:19,639 -> 00:05:21,160] 其实这件事情并不是那么直观
[00:05:22,279 -> 00:05:24,199] 现在CUDA很方便
[00:05:24,199 -> 00:05:26,519] 但当时CUDA还是1.0
[00:05:26,519 -> 00:05:29,800] 在说我主要用来做天气预报这样的情况下
[00:05:29,800 -> 00:05:31,439] 能够意识到deep learning
[00:05:31,439 -> 00:05:34,720] deep neural network能够用GPU来并行training
[00:05:34,720 -> 00:05:35,800] 是一件很不容易的事情
[00:05:35,800 -> 00:05:38,240] 甚至Alex Nath那篇文章一半的篇幅
[00:05:38,240 -> 00:05:39,439] 都在讲怎么去干这件事
[00:05:39,439 -> 00:05:41,920] 这是非常有开拓性的事业
[00:05:41,920 -> 00:05:46,000] 那啥时候适合用deep learning呢
[00:05:46,000 -> 00:05:47,000] 我觉得主要有两个
[00:05:47,000 -> 00:05:50,000] 一个是当你有很多数据的时候
[00:05:50,000 -> 00:05:52,000] 但其实也有一些例外
[00:05:52,000 -> 00:05:55,000] 比如说transfer learning就可以允许你用比较小的数据
[00:05:55,000 -> 00:05:58,000] 甚至几十张图就能成一个相对靠谱的模型出来
[00:05:58,000 -> 00:06:01,000] 同时也有一些其他的例外
[00:06:01,000 -> 00:06:03,000] 比如说你可以生成一些数据
[00:06:03,000 -> 00:06:05,379] 或者是比如像alphago一样
[00:06:05,379 -> 00:06:07,620] 或者是这数据哪怕非常noisy
[00:06:07,620 -> 00:06:08,939] 没有人工标注也可以
[00:06:08,939 -> 00:06:11,779] Facebook之前在Instagram上
[00:06:11,779 -> 00:06:14,660] train了一个非常noisy的数据
[00:06:14,660 -> 00:06:15,540] 上面train了一个model
[00:06:15,540 -> 00:06:16,300] 效果也特别好
[00:06:16,300 -> 00:06:18,819] 此外还有一个要求是
[00:06:18,819 -> 00:06:20,220] 这个数据最好是规整的
[00:06:20,220 -> 00:06:22,019] 比如说图像的像素是网格
[00:06:22,019 -> 00:06:26,160] 或者文本语音它本身是一个线性的序列
[00:06:26,160 -> 00:06:27,079] 这就比较好
[00:06:27,079 -> 00:06:28,560] 像图呢不是不能做
[00:06:28,560 -> 00:06:30,279] 而是就需要更多的处理
[00:06:30,279 -> 00:06:33,199] 相对来说就不是那么适合用deep learning来做
[00:06:33,199 -> 00:06:37,000] 那啥时候deep learning就不适合用来解决这个问题呢
[00:06:37,000 -> 00:06:40,399] 除了说啊你没有很多的数据或者数据不规则以外
[00:06:40,399 -> 00:06:43,000] 还有一个是你需要很多interpretability的时候
[00:06:43,000 -> 00:06:47,160] 或者你需要一个随机性特别小
[00:06:47,160 -> 00:06:48,040] 特别可靠
[00:06:48,480 -> 00:06:50,120] 或者是你没有多少资源
[00:06:50,160 -> 00:06:53,319] 计算资源或者是电力来做inference的时候
[00:06:53,439 -> 00:06:54,399] 就不是特别适合
[00:06:55,920 -> 00:06:58,639] 为什么人们就这么喜欢deep learning
[00:06:58,639 -> 00:06:59,680] 它突然就这么火呢
[00:07:00,079 -> 00:07:03,040] 我觉得不是因为它的performance是最好的
[00:07:03,079 -> 00:07:06,000] 这是因为任何时间段总有个performance最好的算法
[00:07:06,000 -> 00:07:09,000] 为啥到deep learning时间它突然就引爆了呢
[00:07:10,000 -> 00:07:13,000] 也不是因为说deep learning就解决了一些问题
[00:07:13,000 -> 00:07:15,000] 它是强人工智能要来了
[00:07:15,000 -> 00:07:16,000] 好像也不是
[00:07:17,000 -> 00:07:19,000] 我的答案是
[00:07:19,000 -> 00:07:22,000] 它到目前为止还没有出现饱和这个情况
[00:07:22,000 -> 00:07:25,000] 我偷了一个图来解释这个观点
[00:07:25,000 -> 00:07:26,000] 啥叫饱和呢
[00:07:26,000 -> 00:07:28,000] 就以前的诚哥那个算法
[00:07:28,000 -> 00:07:30,000] 它像这条红线一样
[00:07:30,000 -> 00:07:32,000] 搞了一段时间
[00:07:32,000 -> 00:07:34,000] 即使你给它扔更多的数据
[00:07:34,000 -> 00:07:37,000] 你用更多的算力
[00:07:37,000 -> 00:07:39,000] 更复杂的模型
[00:07:39,000 -> 00:07:41,000] 它performance也肯定涨不上去了
[00:07:41,000 -> 00:07:42,000] 这就叫saturation
[00:07:42,000 -> 00:07:44,000] 然后deep learning到目前为止
[00:07:44,000 -> 00:07:45,399] 至少是人有多大胆
[00:07:45,399 -> 00:07:46,480] 地有多大产
[00:07:46,480 -> 00:07:48,720] 你只要有足够的钱来标数据
[00:07:48,720 -> 00:07:50,199] 有足够的机器
[00:07:50,199 -> 00:07:52,360] 来支撑更复杂的算法
[00:07:52,360 -> 00:07:53,279] 更大的模型
[00:07:53,279 -> 00:07:56,000] 你就能拿到更好的performance
[00:07:56,000 -> 00:07:57,319] 所以这个东西
[00:07:57,319 -> 00:07:58,639] 大公司特别喜欢
[00:07:58,639 -> 00:08:00,319] 因为它把技术上
[00:08:00,319 -> 00:08:01,160] 人才上的竞争
[00:08:01,160 -> 00:08:03,439] 直接变成了钱上面的竞争
[00:08:03,439 -> 00:08:05,399] 只要你加三倍的服务器
[00:08:05,699 -> 00:08:08,600] 我就能拿到更好的product
[00:08:09,000 -> 00:08:10,000] 这是非常简单的
[00:08:10,800 -> 00:08:11,800] 他们非常喜欢的
[00:08:12,600 -> 00:08:14,199] 所以这是一个悖论
[00:08:14,199 -> 00:08:17,100] 就是deployment它门槛又在变低又在变高
[00:08:17,300 -> 00:08:20,600] 说变低是因为你看高中生都可以train一个model出来
[00:08:20,600 -> 00:08:21,800] 说AI model
[00:08:22,300 -> 00:08:24,100] 同时也不需要做fisher engineering
[00:08:25,360 -> 00:08:27,480] 可能不太需要domain knowledge
[00:08:27,480 -> 00:08:28,160] 有些时候
[00:08:29,079 -> 00:08:30,600] 同时它又门槛又要变高
[00:08:30,600 -> 00:08:32,720] 因为你要是真想把它做成产品
[00:08:32,840 -> 00:08:37,200] 你需要很多钱去训练或者是标数据等等
[00:08:37,759 -> 00:08:38,960] 所以这玩意还挺矛盾的
[00:08:40,279 -> 00:08:41,159] 在正式版里面
[00:08:41,279 -> 00:08:44,919] 我会继续介绍一些deep learning相关的模型
[00:08:45,000 -> 00:08:47,500] 不一定是模型,可能是一些思路
[00:08:47,500 -> 00:08:53,000] 比如说transfer learning, representation learning, general model
[00:08:53,000 -> 00:09:00,000] 像adversarial attack, RNLSTM transformer之类的东西
[00:09:00,000 -> 00:09:02,000] New architecture search
[00:09:02,000 -> 00:09:05,679] 这次时间不够,就暂时先这样
[00:09:05,679 -> 00:09:07,919] 我总结一下刚才说了啥
[00:09:07,919 -> 00:09:11,759] deep learning为什么12年左右突然work呢
[00:09:11,759 -> 00:09:14,240] 我觉得主要是因为有GPU的算力
[00:09:14,240 -> 00:09:15,679] 有大规模的数据集
[00:09:15,679 -> 00:09:18,159] 以及解决了gradient vanishing的问题
[00:09:18,159 -> 00:09:19,919] 啥时候用deep learning好呢
[00:09:19,919 -> 00:09:23,200] 在有大规模的数据和规整数据结构的情况下
[00:09:23,200 -> 00:09:26,399] 为什么大家尤其是大公司喜欢DPU呢
[00:09:26,399 -> 00:09:29,200] 是因为到目前为止还没有看到它饱和的迹象
[00:09:29,200 -> 00:09:32,000] 好 今天就到这吧
[00:09:32,000 -> 00:09:34,399] 我回头会把这个slides放出来
[00:09:34,399 -> 00:09:37,000] 里面后面有reference
[00:09:37,000 -> 00:09:39,200] 感兴趣的话可以读一下里面具体的paper
[00:09:39,200 -> 00:09:41,600] 等到正式版的时候
[00:09:41,600 -> 00:09:43,600] 我再看做另外一个视频出来吧
