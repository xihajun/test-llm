[00:00:00,000 -> 00:00:02,040] 特斯拉就现在的硬件
[00:00:02,040 -> 00:00:03,960] 我觉得它是不可能达到IOS的
[00:00:04,339 -> 00:00:06,299] 你选择使用特斯拉的就是你的责任
[00:00:06,740 -> 00:00:09,339] 我觉得科技进步不是太快的而是太慢的
[00:00:09,560 -> 00:00:11,500] 忙吗?忙还是算忙吧
[00:00:11,599 -> 00:00:12,939] 正常的startup的忙
[00:00:13,000 -> 00:00:14,500] 介绍一下你的startup
[00:00:14,599 -> 00:00:16,699] 但本身是做无人卡车
[00:00:17,000 -> 00:00:19,199] 对我们路上有没有无人卡车的
[00:00:19,500 -> 00:00:20,539] 你说我们公司的
[00:00:21,039 -> 00:00:22,640] 就是in general
[00:00:22,800 -> 00:00:24,960] 美国高速公路上有没有无人卡车
[00:00:24,960 -> 00:00:25,000] 真正无人卡车
[00:00:25,000 -> 00:00:26,000] 真正无人的没有
[00:00:26,000 -> 00:00:28,000] 有几个公司在路侧
[00:00:28,000 -> 00:00:30,000] 但是应该都还是有安全员的
[00:00:30,000 -> 00:00:31,000] 没有完全无人
[00:00:31,000 -> 00:00:34,000] 你觉得还离这个真正无人差多远
[00:00:34,000 -> 00:00:37,000] 第一这是一个整个产业的问题
[00:00:37,000 -> 00:00:38,000] 比如说我们公司
[00:00:38,000 -> 00:00:40,000] 甚至大部分做无人卡车的公司
[00:00:40,000 -> 00:00:41,000] 我们是不生产这个卡车的
[00:00:41,000 -> 00:00:44,000] 卡车还是卡车OEM
[00:00:44,000 -> 00:00:46,159] 就是厂商去生产所以他们生产这个卡车的卡车还是卡特OEM就是厂商去生产的
[00:00:46,159 -> 00:00:47,600] 所以他们生产的卡车
[00:00:47,600 -> 00:00:51,719] 得达到能够无人的安全标准
[00:00:51,719 -> 00:00:52,679] 举个例子就是
[00:00:52,679 -> 00:00:54,320] 它必须要有一套
[00:00:54,320 -> 00:00:56,439] 冗余的控制系统
[00:00:56,439 -> 00:00:57,320] 当你的主要的
[00:00:57,320 -> 00:01:00,240] 有相当于是有电脑控制的
[00:01:00,240 -> 00:01:01,280] 这个系统失效了以后
[00:01:01,280 -> 00:01:03,920] 你还要能够来控制它
[00:01:03,920 -> 00:01:06,439] 这个是达到安全的一个必须的
[00:01:06,439 -> 00:01:08,480] 但是现在大部分就是没有哪一家
[00:01:08,480 -> 00:01:10,480] 因为这东西太新了
[00:01:10,480 -> 00:01:11,560] 这些厂商没有
[00:01:11,560 -> 00:01:13,599] 他没有需求来做这个事
[00:01:13,599 -> 00:01:15,439] 所以这个东西还能达不到
[00:01:15,439 -> 00:01:17,040] 所以只有当他们
[00:01:17,040 -> 00:01:18,439] 换句话说
[00:01:18,439 -> 00:01:19,359] 或者说简单一点说
[00:01:19,359 -> 00:01:20,959] 只有当硬件达到那个时候
[00:01:21,920 -> 00:01:24,319] 才有可能真正的有无人车
[00:01:24,319 -> 00:01:26,000] 所以特斯拉也不能满足这个标准
[00:01:26,000 -> 00:01:27,439] 因为它也没有龙玉的启动
[00:01:27,439 -> 00:01:30,760] 对 然后我之前信息里我就说
[00:01:30,760 -> 00:01:33,680] 特斯拉就现在的硬件
[00:01:33,680 -> 00:01:35,560] 我觉得它是不可能达到L4的
[00:01:35,560 -> 00:01:38,239] L4是在限定的某一个叫
[00:01:38,239 -> 00:01:40,159] Operational Design Domain ODD里面
[00:01:40,640 -> 00:01:42,560] 能够完全的无人驾驶
[00:01:43,400 -> 00:01:45,359] L3 L4 L5的区别是什么来着
[00:01:45,359 -> 00:01:47,280] L3和L4最大区别就是
[00:01:47,280 -> 00:01:48,640] L3还是人负责
[00:01:48,640 -> 00:01:50,400] L4以上就是车负责
[00:01:50,400 -> 00:01:52,799] 那你觉得特斯拉线能达到L3吗
[00:01:52,799 -> 00:01:58,239] L3是一个我其实对L3的完全定义
[00:01:58,239 -> 00:02:00,239] 我都不记得很清楚
[00:02:00,239 -> 00:02:01,840] 你可以说达到也可以说能达到
[00:02:01,840 -> 00:02:03,359] 但我不记得L3具体定义
[00:02:03,359 -> 00:02:05,439] 但是其实对我来说最大的区别就是说
[00:02:05,439 -> 00:02:08,639] 出了事或者是这个车到底是人在最后在负责
[00:02:08,639 -> 00:02:11,199] 还是有这个机器的这个情况
[00:02:11,199 -> 00:02:13,759] 为什么我说是个伦理或者法律问题呢
[00:02:13,759 -> 00:02:17,120] 就第一如果车来负责法律归谁
[00:02:17,120 -> 00:02:18,560] 对肯定是个法律问题
[00:02:18,560 -> 00:02:21,360] 所以这个产业很大一部分的
[00:02:21,360 -> 00:02:22,400] 也不是很大
[00:02:22,400 -> 00:02:23,680] 有一部分的effort也是说
[00:02:23,680 -> 00:02:25,599] 怎么样去跟policy maker
[00:02:25,599 -> 00:02:29,500] 跟各个地区的infrastructure去配合起来
[00:02:29,500 -> 00:02:32,300] 但是先抛开法律问题不太
[00:02:32,300 -> 00:02:33,400] 可能这有两个问题
[00:02:33,400 -> 00:02:35,900] 第一个就是谁会做出更好的决定
[00:02:35,900 -> 00:02:39,099] 还有可能L4它意味着
[00:02:39,099 -> 00:02:41,300] 条件的成熟的一个必要条件就是
[00:02:41,300 -> 00:02:45,120] 机器在大多数情况下能做出比人更好的决定
[00:02:46,960 -> 00:02:49,680] 但是我觉得这个时候现在就有了就是我其实我有的时候坐lift uber
[00:02:49,680 -> 00:02:51,840] 我觉得司机开的瞎开就是很可怕
[00:02:52,280 -> 00:02:52,639] 对吧
[00:02:52,879 -> 00:02:56,719] 我觉得一个vimo可能比司机开的好
[00:02:57,080 -> 00:02:59,879] 但是不意味着大家在观念上
[00:02:59,879 -> 00:03:01,560] 绝大多数人的观念上可以接受
[00:03:02,000 -> 00:03:04,919] 他更相信这个机器而不是相信uber司机
[00:03:05,000 -> 00:03:05,900] 对吧
[00:03:06,199 -> 00:03:09,500] 所以说第一条件什么时候可以成熟到
[00:03:09,500 -> 00:03:14,099] 大多数时候车比人的水平好
[00:03:14,099 -> 00:03:14,800] 我觉得这个
[00:03:16,000 -> 00:03:18,400] 起码从那个uber司机的角度来看的话
[00:03:18,400 -> 00:03:19,300] 已经比他好了
[00:03:19,500 -> 00:03:21,000] 自动驾驶最后是一个产品的话
[00:03:21,000 -> 00:03:22,000] 它不光是技术
[00:03:22,000 -> 00:03:24,900] 还有怎么样让产品赢得市场
[00:03:24,900 -> 00:03:25,000] 不光是这个产品本身对吧有很多术还有怎么样让产品赢得市场
[00:03:25,000 -> 00:03:26,199] 不光是产品本身
[00:03:26,199 -> 00:03:27,680] 有很多其他的因素
[00:03:27,680 -> 00:03:28,960] 这个产品怎么样去推广
[00:03:28,960 -> 00:03:31,680] 从自动驾驶本身这个产品来说
[00:03:31,680 -> 00:03:33,560] 它要很大一份是信任
[00:03:33,560 -> 00:03:38,439] 怎么样要让乘客去信任这个车是安全的
[00:03:38,439 -> 00:03:40,360] 我们今天就谈技术问题
[00:03:40,360 -> 00:03:41,639] OK谈技术问题
[00:03:41,639 -> 00:03:43,039] 来给我们科普一下
[00:03:43,039 -> 00:03:45,439] 在你看来技术有什么大问题
[00:03:45,439 -> 00:03:48,479] 最大的问题应该是所谓的厂委问题
[00:03:48,479 -> 00:03:51,680] 怎么样你能让你这个车能够去解决
[00:03:51,680 -> 00:03:57,280] 因为至少有一部分无人车系统是用机械系的方法来作为它的一部分
[00:03:57,280 -> 00:03:59,199] 或者是大部分的一个component
[00:03:59,199 -> 00:04:05,240] 所以你怎么样去验证机�学系统在遇到常委问题
[00:04:05,240 -> 00:04:08,199] 就是说他的训练书籍很少见到的情况下的时候
[00:04:08,639 -> 00:04:10,800] 他还能够做出正确的决定
[00:04:10,800 -> 00:04:14,879] 这个决定不一定说他做他机器学系系统做出了正确的判断
[00:04:14,879 -> 00:04:19,439] 而是机器学系统比较难的一点是他怎么样让他在他不知道这个时候
[00:04:19,800 -> 00:04:21,959] 很confident的说我不知道这个是什么
[00:04:21,959 -> 00:04:23,199] 我需要停下来
[00:04:23,199 -> 00:04:26,560] 因为大部分的系统是你designer给他一个task
[00:04:26,560 -> 00:04:28,399] 他要去做出一个判断
[00:04:28,399 -> 00:04:30,839] 就让我想起来了特斯拉
[00:04:30,839 -> 00:04:34,439] 他最近的一个FSD的update
[00:04:34,439 -> 00:04:36,160] 就是他把一个deterministic的
[00:04:36,160 -> 00:04:38,720] 这种if-else变成了一个
[00:04:39,680 -> 00:04:41,720] supposedly neural network
[00:04:41,720 -> 00:04:43,959] 我不知道他最后这个东西
[00:04:43,959 -> 00:04:48,439] 他是希望就是让这个机器真的去自主学习呢
[00:04:48,439 -> 00:04:52,740] 还是人在人的指导之下去学习
[00:04:52,740 -> 00:04:56,579] 就是我们要把人的先验知识带到这个problem space里面吗
[00:04:56,579 -> 00:04:59,459] 还是我们人在这里边就说
[00:04:59,459 -> 00:05:00,660] This is a problem
[00:05:00,660 -> 00:05:03,660] 我给你define好你的parameter和你的last function
[00:05:03,660 -> 00:05:06,939] 我最多再给你调节一下数据的权重
[00:05:06,939 -> 00:05:08,699] 比如说那个三百万分之一的东西
[00:05:08,699 -> 00:05:11,899] 我把它调成三千分之一去学学
[00:05:11,899 -> 00:05:14,500] 学完了以后好了这个结果不管了
[00:05:14,500 -> 00:05:17,939] 我觉得第一你说有没有人的先例
[00:05:17,939 -> 00:05:18,980] 肯定是有对吧
[00:05:18,980 -> 00:05:20,939] 就是比如说在他这个系统上线之前
[00:05:20,939 -> 00:05:24,480] 他怎么去验证肯定是他有这个以前
[00:05:24,480 -> 00:05:25,000] 也有说shadow mode对吧就是要么你的这个软件在车上跑在这个系统上线之前它怎么去验证肯定是它有这个以前
[00:05:25,000 -> 00:05:26,000] 也有说叫shadow mode
[00:05:26,000 -> 00:05:28,000] 就是相当于你的这个软件在车上跑
[00:05:28,000 -> 00:05:31,000] 但它不实际操纵的时候
[00:05:31,000 -> 00:05:33,000] 然后把这个人开的这个
[00:05:33,000 -> 00:05:34,000] 像这个路线和这个
[00:05:34,000 -> 00:05:36,000] 所谓的这个方向盘
[00:05:36,000 -> 00:05:39,000] 或者刹车的这个控制的
[00:05:39,000 -> 00:05:43,000] 跟这个它的模型预测的那些结果去做比较
[00:05:43,000 -> 00:05:44,000] 实际上这个里面就已经有
[00:05:44,000 -> 00:05:47,199] 至少即使说它这个模型训练的过程中
[00:05:47,199 -> 00:05:48,639] 没有用到人的知识
[00:05:48,639 -> 00:05:50,120] 但是在验证的过程中也用到了
[00:05:50,120 -> 00:05:50,959] 这是第一个
[00:05:50,959 -> 00:05:52,560] 第二个就是所有
[00:05:52,560 -> 00:05:56,199] 就是跟开车跟driving相关的所有的知识
[00:05:56,199 -> 00:05:58,199] 里面也用到了很多人的知识
[00:05:58,199 -> 00:05:58,439] 对吧
[00:05:58,439 -> 00:05:59,720] 就是所有的交通规则
[00:05:59,720 -> 00:06:01,120] 所有的交通信号
[00:06:01,120 -> 00:06:02,519] 这都是人制定的规则
[00:06:02,519 -> 00:06:03,120] 他也要
[00:06:03,120 -> 00:06:04,959] 你也要通过某一种方式
[00:06:04,959 -> 00:06:06,199] 告诉这个模型去学到这些东西
[00:06:06,199 -> 00:06:07,600] 那既然聊到这儿了
[00:06:07,600 -> 00:06:09,199] 我就把他这个再聊深一点
[00:06:09,199 -> 00:06:11,399] 就是我不知道你看没看过
[00:06:11,399 -> 00:06:13,399] 我写那个就是鹦鹉乌鸦
[00:06:13,399 -> 00:06:14,399] 朱松春
[00:06:14,399 -> 00:06:15,800] 鹦鹉什么
[00:06:15,800 -> 00:06:18,199] 鹦鹉模式和乌鸦模式
[00:06:18,199 -> 00:06:20,399] 鹦鹉就是过去的Machine Learning
[00:06:20,399 -> 00:06:21,600] 寻找Correspondence
[00:06:21,600 -> 00:06:23,399] 鹦鹉学舌的方式
[00:06:23,399 -> 00:06:27,100] 乌鸦看起来有Inference能力
[00:06:27,100 -> 00:06:28,399] which reasoning
[00:06:28,399 -> 00:06:29,500] 就是它
[00:06:30,399 -> 00:06:34,100] 例子就是说乌鸦在一个日本
[00:06:34,100 -> 00:06:35,600] 就是它有Documentary
[00:06:35,600 -> 00:06:38,100] 它可以 它想打开它的坚果
[00:06:38,100 -> 00:06:39,199] 但是它不知道怎么打
[00:06:39,199 -> 00:06:40,000] 就是它打不开
[00:06:40,000 -> 00:06:41,399] 所以说它就去
[00:06:41,399 -> 00:06:44,300] 它进一步发现车可以压碎坚果
[00:06:44,300 -> 00:06:46,660] 对 然后但是车会撞死它然后它进一步发现车可以压碎坚果然后当时车会撞死他
[00:06:46,660 -> 00:06:48,699] 然后他进一步发现红绿灯可以stop车
[00:06:48,699 -> 00:06:51,000] 于是他就在红灯的时候把坚果扔下去
[00:06:51,000 -> 00:06:52,540] 然后绿灯开过去
[00:06:52,540 -> 00:06:53,819] 然后红灯的时候再捡起来
[00:06:53,819 -> 00:06:55,360] 这个东西他不能失败
[00:06:55,360 -> 00:06:56,379] 他失败了就死掉了
[00:06:56,379 -> 00:07:00,480] 所以说他完全是在自己脑子里边理解了看似
[00:07:00,480 -> 00:07:03,040] 我们觉得是理解了他理解了这个机制
[00:07:03,040 -> 00:07:04,319] 然后去做了这么一个事情
[00:07:04,319 -> 00:07:06,339] 这是两个范式
[00:07:06,339 -> 00:07:09,680] 会说我在里边是说看似大模型拥有了一个无压能力
[00:07:09,680 -> 00:07:11,779] 不一定 但是我觉得可能有
[00:07:11,779 -> 00:07:14,120] 然后我最近有一些新的想法
[00:07:14,120 -> 00:07:15,680] 就是这个无压能力到底是怎么来的
[00:07:15,680 -> 00:07:19,680] 有可能是shortest path of understanding prior knowledge
[00:07:19,680 -> 00:07:20,920] 这先不展开
[00:07:20,920 -> 00:07:26,000] 我们还是回过来聊一些大家可能非常直接感兴趣的话题
[00:07:26,000 -> 00:07:27,800] 比如说特斯拉
[00:07:27,800 -> 00:07:29,600] 你看起来是不太看好的特斯拉
[00:07:29,600 -> 00:07:32,399] 不太看好这个说的太
[00:07:32,399 -> 00:07:34,800] 首先你很讨厌FSD的命名对吧
[00:07:34,800 -> 00:07:36,800] 对因为它并不是
[00:07:36,800 -> 00:07:38,199] 你不就是名不副其实吗
[00:07:38,199 -> 00:07:39,000] 然后这是第一
[00:07:39,000 -> 00:07:43,199] 第二因为这个名字我觉得是造成
[00:07:43,199 -> 00:07:44,800] 特斯拉也有一些事故
[00:07:44,800 -> 00:07:48,959] 我觉得有一部分是因为这个名字造成误导
[00:07:48,959 -> 00:07:51,759] 这就说到我刚刚提到产品的问题
[00:07:51,759 -> 00:07:53,060] 相当于他叫这个名字以后
[00:07:53,060 -> 00:07:57,160] 让人以为它是一个完全的自动驾驶的产品
[00:07:57,160 -> 00:08:00,860] 所以有人会把它当做一个完全自动驾驶产品用
[00:08:00,860 -> 00:08:03,759] 就是像你开的时候完全人丝巨不复
[00:08:03,759 -> 00:08:05,399] 我觉得不是FSD的过
[00:08:05,399 -> 00:08:06,000] 怎么说呢
[00:08:06,000 -> 00:08:07,199] 这特别是一个更多的是个
[00:08:07,199 -> 00:08:08,000] marketing的角度
[00:08:08,000 -> 00:08:09,600] 不是一个技术的角度
[00:08:09,600 -> 00:08:11,600] 但是我觉得marketing这种角度
[00:08:11,600 -> 00:08:13,000] 很难说就是
[00:08:13,000 -> 00:08:14,600] 没有特斯拉的话
[00:08:14,600 -> 00:08:16,600] 电车肯定不是今天这样
[00:08:16,600 -> 00:08:17,000] 对
[00:08:17,000 -> 00:08:19,800] 所以我所以你刚一开始问我是不是
[00:08:19,800 -> 00:08:20,800] 不喜欢特斯拉
[00:08:20,800 -> 00:08:21,600] 我觉得这是要
[00:08:21,600 -> 00:08:23,399] 因为特斯拉有很多方面对吧
[00:08:23,399 -> 00:08:24,800] 所以我不会说是一个二轮数
[00:08:24,800 -> 00:08:25,000] 我不喜欢特斯拉对像你说的特斯拉在这个电动吧所以我不会说是一个二轮数我不喜欢特斯拉
[00:08:25,000 -> 00:08:26,000] 对像你说的
[00:08:26,000 -> 00:08:28,000] 特斯拉在电动车的推广方面
[00:08:28,000 -> 00:08:30,000] 做了很大的贡献
[00:08:30,000 -> 00:08:32,000] 但是从无人驾驶这个角度
[00:08:32,000 -> 00:08:34,000] 我觉得是做了更多的害处吧
[00:08:34,000 -> 00:08:36,000] 至少从比如说我们要说产品的角度
[00:08:36,000 -> 00:08:38,000] 它作为一个
[00:08:38,000 -> 00:08:39,000] 首先它是一个
[00:08:39,000 -> 00:08:41,000] 说清楚它是一个辅助驾驶的产品
[00:08:41,000 -> 00:08:43,000] FSD是一个辅助驾驶的产品
[00:08:43,000 -> 00:08:44,000] 至少目前为止
[00:08:44,000 -> 00:08:46,639] 它没有达到自动驾驶的能完成自助驾驶的产品FSD是一个辅助驾驶的产品至少目前为止它没有达到自动驾驶的
[00:08:46,639 -> 00:08:48,879] 这个能完成自动驾驶的能力
[00:08:49,240 -> 00:08:50,559] 但是它由它的命名
[00:08:50,559 -> 00:08:52,399] 以及它的这种迭代的方式吧
[00:08:52,399 -> 00:08:54,080] 从最早期它基本上就是
[00:08:54,879 -> 00:08:56,120] 至少我认为它是把
[00:08:56,679 -> 00:08:58,879] 特斯拉的车主当做
[00:08:59,360 -> 00:09:02,440] 用特斯拉的车主来测试它的产品
[00:09:02,799 -> 00:09:04,080] 我觉得这是至少
[00:09:04,559 -> 00:09:06,679] 对这样一种产品来说
[00:09:06,679 -> 00:09:08,639] 是一个不是很负责任的行为
[00:09:08,639 -> 00:09:11,279] 因为这是一个安全的产品
[00:09:11,279 -> 00:09:16,320] 好像我之前看一个是每年因为车祸会死300万人
[00:09:17,080 -> 00:09:18,960] 然后我就想那些刚学车的那些人
[00:09:18,960 -> 00:09:21,039] 大家说的就是吐槽的那些司机
[00:09:21,039 -> 00:09:22,360] 对吧乱开车的司机
[00:09:22,960 -> 00:09:25,159] 转向不打灈下高速隨便下
[00:09:25,600 -> 00:09:26,480] 這些人我相信
[00:09:26,480 -> 00:09:29,080] 每天殺死的人比特斯拉多很多
[00:09:29,600 -> 00:09:30,960] 假設我們今天所有人
[00:09:30,960 -> 00:09:33,279] 今天全都使用FSD
[00:09:33,279 -> 00:09:36,200] 我相信車禍會變少而不是變多
[00:09:37,360 -> 00:09:39,559] 我們確實FSD
[00:09:39,559 -> 00:09:41,399] 直接歸因可以歸因到
[00:09:41,399 -> 00:09:43,519] 他這個事情好像怎麼樣導致的問題
[00:09:43,519 -> 00:09:44,840] 那個事情導致的問題
[00:09:45,200 -> 00:09:47,799] 但是他的alternative也有很多问题
[00:09:48,360 -> 00:09:50,840] 然后我觉得科技进步不是太快了而是太慢了
[00:09:51,000 -> 00:09:51,799] 就是这样
[00:09:51,799 -> 00:09:56,759] 那个factor是不是说我们用一个更谨慎的方式
[00:09:56,759 -> 00:09:59,440] 同样快的速度去发展自动驾驶
[00:09:59,799 -> 00:10:02,840] 而是更谨慎的速度可能发展的速度是现在1 1十分
[00:10:03,360 -> 00:10:05,159] 那你是希望用一个更aggressive的方式速度可能发展的速度是现在1 1 十分那你是希望用一个更
[00:10:07,919 -> 00:10:13,399] aggressive的方式更快的发展还是对但是现实是你再aggressive也不可能是所有人都开特斯拉
[00:10:13,399 -> 00:10:15,399] 或者所有人都有s fsd
[00:10:15,399 -> 00:10:17,279] 因为不能所有人都使用
[00:10:17,279 -> 00:10:19,399] 对不可能所有人都同时使用上这个技术
[00:10:19,399 -> 00:10:21,080] 这现实上是不可能的对吧
[00:10:21,440 -> 00:10:24,000] 然后我对你
[00:10:24,000 -> 00:10:26,000] 就你刚才说如果所有人都用FSD
[00:10:26,000 -> 00:10:27,000] 或者不用所有人吧
[00:10:27,000 -> 00:10:29,000] 就是the more people you FSD
[00:10:29,000 -> 00:10:31,000] the more people will benefit
[00:10:31,000 -> 00:10:35,000] 有了FSD这个技术减少了一些事故
[00:10:35,000 -> 00:10:37,000] 就能justify它导致那些事故
[00:10:37,000 -> 00:10:38,000] 那不能
[00:10:38,000 -> 00:10:41,000] 对但是这就涉及到
[00:10:41,000 -> 00:10:43,000] 我们一开始说这个到底是人负责任
[00:10:43,000 -> 00:10:45,200] 人的责任还是机器的责任人的责任
[00:10:45,200 -> 00:10:46,700] 在我看来就是人的责任
[00:10:46,700 -> 00:10:48,899] 你选择使用特斯拉的就是你的责任
[00:10:48,899 -> 00:10:51,700] 但是特斯拉的责任是他没有
[00:10:51,700 -> 00:10:54,200] 他提供的技术没有
[00:10:54,200 -> 00:10:57,700] 就是你开车撞死人是谁的责任
[00:10:57,700 -> 00:11:00,700] 对 但你说你是因为你说你是FSD了
[00:11:00,700 -> 00:11:02,700] 你说你是Full Self Driving
[00:11:02,700 -> 00:11:05,200] 所以为什么我开车还是我的责任呢
[00:11:06,080 -> 00:11:07,440] 我撞死人还是我的责任呢
[00:11:07,440 -> 00:11:09,759] 这个是我认为他误导人的地方
[00:11:09,759 -> 00:11:12,799] 就是不是所有人在开一个
[00:11:12,919 -> 00:11:15,000] 或者在运用使用FSD的时候
[00:11:15,000 -> 00:11:16,440] 都有这个清晰的认识说
[00:11:16,879 -> 00:11:19,240] 我是是我在掌控这个最后的车
[00:11:19,240 -> 00:11:20,240] 是我是我的责任
[00:11:20,240 -> 00:11:21,759] 而是像你说的
[00:11:21,759 -> 00:11:23,759] 完全相信了这个机器
[00:11:23,759 -> 00:11:26,299] 那但这个相信的有一部分原因就是因为这个
[00:11:26,299 -> 00:11:29,000] 命名和他这个marketing导致
[00:11:29,000 -> 00:11:33,000] 所以我认为这是一个他有很大责任的地方
[00:11:33,000 -> 00:11:36,500] 就是在我看来技术进步大于很多其他的东西
[00:11:36,500 -> 00:11:39,200] 因为我的目标是这个full self-driving
[00:11:39,200 -> 00:11:40,600] 对你的目标是full self-driving
[00:11:40,600 -> 00:11:44,200] 但当你有了full self-driving这个capability的时候
[00:11:44,200 -> 00:11:46,200] 再去release这样一个product
[00:11:46,200 -> 00:11:48,200] 我现在是一个beta版的Full Self-Driving
[00:11:48,200 -> 00:11:51,200] 就是当大家不使用FSD
[00:11:51,200 -> 00:11:53,200] 你可能会觉得有两个原因
[00:11:53,200 -> 00:11:55,700] 第一个原因就是这件事情没有价值
[00:11:55,700 -> 00:11:56,700] 大家不用
[00:11:56,700 -> 00:11:59,200] 第二个就是三个原因吧
[00:11:59,200 -> 00:12:01,700] 第一个是大家就是fundamentally没有价值
[00:12:01,700 -> 00:12:06,639] 第二个是大家就是没有习惯它
[00:12:06,639 -> 00:12:09,840] 所以不知道它的存在
[00:12:09,840 -> 00:12:13,840] 第三个就是fundamentally是有价值的
[00:12:13,840 -> 00:12:14,639] 但是你没做好
[00:12:14,639 -> 00:12:17,440] 他现在不知道他是在三个里面哪一个
[00:12:17,440 -> 00:12:21,679] 最差的情况是现在fundamentally没有价值
[00:12:21,679 -> 00:12:24,080] 是我觉得有可能发生的事情
[00:12:24,080 -> 00:12:27,639] Robotaxi出现了以后
[00:12:27,639 -> 00:12:30,279] 也许Robotaxi是有价值的
[00:12:30,279 -> 00:12:32,000] 但是那也是个问号
[00:12:32,000 -> 00:12:33,559] 就是Robotaxi在
[00:12:34,840 -> 00:12:37,360] 就假设Robotaxi真的做好了
[00:12:37,360 -> 00:12:39,000] 它对人没有价值
[00:12:39,000 -> 00:12:39,879] 为什么没有
[00:12:39,879 -> 00:12:40,679] 你说没有价值
[00:12:40,679 -> 00:12:41,759] 还是你有可能没有价值
[00:12:41,759 -> 00:12:43,080] 为什么呢
[00:12:43,080 -> 00:12:44,919] 就是我有一辆车可以开
[00:12:44,919 -> 00:12:46,500] 我为什么一定要不开它
[00:12:46,600 -> 00:12:48,200] 为什么买一辆不开的车
[00:12:48,600 -> 00:12:49,399] 哦 robotaxi
[00:12:49,899 -> 00:12:51,399] 等一下 robotaxi 是说
[00:12:51,500 -> 00:12:53,200] 我不需要有这个 car ownership
[00:12:53,500 -> 00:12:55,000] 我可以就像我打 uber 一样
[00:12:55,000 -> 00:12:55,799] 但是只是把
[00:12:56,100 -> 00:12:57,799] uber 的成本降低了很多
[00:12:57,799 -> 00:12:59,000] 同时也减少了
[00:12:59,700 -> 00:13:01,500] 呃人力的需求
[00:13:01,600 -> 00:13:02,600] 这是我理解的 robotaxi
[00:13:02,600 -> 00:13:03,700] 而不是说我 own 一个车
[00:13:03,700 -> 00:13:06,919] 然后他去当 robotaxi那我觉得确实没有什么意义
[00:13:06,919 -> 00:13:08,320] 我干嘛要own一个Taxi呢
[00:13:08,320 -> 00:13:08,919] 对吧
[00:13:08,919 -> 00:13:09,879] 特别是对美国来说
[00:13:09,879 -> 00:13:10,600] 对吧
[00:13:10,600 -> 00:13:13,600] 这个因为人力成本比较高
[00:13:13,600 -> 00:13:14,200] 同时
[00:13:15,480 -> 00:13:16,600] 人的生活习惯
[00:13:16,600 -> 00:13:17,799] 很多做在Sandburg
[00:13:17,799 -> 00:13:19,200] 所以需要有一个车
[00:13:19,200 -> 00:13:19,559] 对
[00:13:19,559 -> 00:13:20,399] 但是当你
[00:13:21,799 -> 00:13:24,759] 同时这个对整个城市的规划来说
[00:13:24,759 -> 00:13:27,519] 他需要花很多的建立去考虑停车场
[00:13:27,519 -> 00:13:28,480] 这种方面
[00:13:28,480 -> 00:13:32,559] 当你有一个车是可以自动的开的时候
[00:13:32,559 -> 00:13:34,840] 这个所有的这一切都会发生一些改变
[00:13:34,840 -> 00:13:36,799] 你知道上一个完全可以类比的
[00:13:36,799 -> 00:13:38,360] Argument是什么产品吗
[00:13:38,360 -> 00:13:38,879] 什么
[00:13:38,879 -> 00:13:40,279] 云游戏
[00:13:40,279 -> 00:13:40,840] 什么东西
[00:13:40,840 -> 00:13:41,519] 云游戏
[00:13:41,519 -> 00:13:42,200] 什么叫云游戏
[00:13:42,200 -> 00:13:43,399] Cloud Gaming
[00:13:43,399 -> 00:13:43,799] 我不知道
[00:13:43,799 -> 00:13:45,279] Google Stadia
[00:13:46,120 -> 00:13:47,759] OK
[00:13:49,080 -> 00:13:49,919] 你为什么觉得这是一个雷比
[00:13:51,480 -> 00:13:53,480] 就我为什么要买一个主机我为什么买一个such a powerful PC
[00:13:53,480 -> 00:13:54,679] Xbox PSC
[00:13:54,679 -> 00:13:56,000] 有那么贵对吧
[00:13:56,000 -> 00:13:57,440] 然后那么好的硬件
[00:13:57,440 -> 00:14:00,759] 放在家里99.99%的时间都不会打开它
[00:14:00,759 -> 00:14:02,519] 我只是在玩的时候去玩一下
[00:14:03,279 -> 00:14:04,960] 为什么我不Cloud
[00:14:04,960 -> 00:14:05,120] 然后Stream但是他没有办法compete过Xbox为什么都不会打开它我只是在玩的时候去玩一下为什么我不cloud
[00:14:05,120 -> 00:14:05,879] 然后stream呢
[00:14:07,000 -> 00:14:08,960] 但是他没有办法compete过Xbox
[00:14:08,960 -> 00:14:09,759] 为什么
[00:14:09,759 -> 00:14:12,679] 因为有这个需求的人已经买了一个Xbox了
[00:14:12,679 -> 00:14:14,039] 特斯拉大家很多时候说
[00:14:14,039 -> 00:14:17,159] 他有一个最大的优势就是他collect data是世界第一
[00:14:17,159 -> 00:14:19,000] 或者这个行业的发展
[00:14:19,000 -> 00:14:20,919] 对你个人的影响其实很小
[00:14:20,919 -> 00:14:24,720] 但是我还是希望世界上多一些这样的人
[00:14:24,720 -> 00:14:27,600] 技术是由市场才能被培育起来的
[00:14:27,600 -> 00:14:28,399] 对 可以这么说
