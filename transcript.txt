[00:00:00,000 -> 00:00:15,199] 大家好,这期视频我邀请到了我的朋友和前同事,在亚马逊工作的应用科学家卢瑞同学,来和我一起讨论一下人工智能在互联网大厂的应用。
[00:00:00,000 -> 00:00:03,200] 这段视频我会把它剪成两个部分
[00:00:03,200 -> 00:00:05,799] 上一部分是罗威同学帮我们讲一下
[00:00:05,799 -> 00:00:09,640] 人工智能机器学习和深度学习的历史
[00:00:09,640 -> 00:00:11,720] 它们之间的关系
[00:00:11,720 -> 00:00:16,359] 以及我们为什么是在大概2011年2012年之后
[00:00:16,359 -> 00:00:20,160] 看到深度学习有一个广泛的应用
[00:00:20,160 -> 00:00:23,160] 和这里面有一些标志性的事件是什么
[00:00:23,160 -> 00:00:26,519] 下半部分我们会一起讨论一下
[00:00:26,519 -> 00:00:28,359] 尤其是深度学习
[00:00:00,000 -> 00:00:02,560] 在互联网大厂里的应用
[00:00:02,560 -> 00:00:05,759] 包括我们到底现在是到了一个瓶颈
[00:00:05,759 -> 00:00:08,240] 还是未来有广阔的发展空间
[00:00:08,240 -> 00:00:10,560] 我们能不能会看到一个范式突破
[00:00:10,560 -> 00:00:12,800] 或者范式突破在这里边重要不重要
[00:00:12,800 -> 00:00:14,720] 包括数据的价值是什么
[00:00:14,720 -> 00:00:18,239] 以及互联网大厂他们的门槛到底是什么
[00:00:18,239 -> 00:00:20,600] 上半部分大概会十几分钟
[00:00:20,600 -> 00:00:23,039] 下半部分应该是半小时的样子
[00:00:23,039 -> 00:00:24,800] 那话不多说我们就开始吧
[00:00:24,800 -> 00:00:25,359] 谢谢
[00:00:25,359 -> 00:00:29,600] 感谢卢卫同学今天来给我们
[00:00:00,000 -> 00:00:03,640] 大家講一下人工智能在大廠的應用
[00:00:03,640 -> 00:00:08,199] 魯威同學是清華大學的本科水利工程
[00:00:08,199 -> 00:00:11,000] 之後去德州農工讀了博士
[00:00:11,000 -> 00:00:15,000] 他在2012年的時候就發表了一篇基於
[00:00:15,000 -> 00:00:18,800] 風群智慧和neural fencing的算法
[00:00:18,800 -> 00:00:22,199] 在控制交通方面應用的論文
[00:00:22,199 -> 00:00:26,199] 也算是接觸人工智能比較早的一批人了
[00:00:00,000 -> 00:00:03,759] 再之后他也去福特和IBM实验室
[00:00:03,759 -> 00:00:06,080] 有过工作经历
[00:00:06,080 -> 00:00:10,199] 也都是有人工智能在不同领域的应用
[00:00:10,199 -> 00:00:15,800] 之后他在亚马逊先后是在广告和客服部门
[00:00:15,800 -> 00:00:18,760] 做人工智能科学 做机器学习科学家
[00:00:18,760 -> 00:00:21,320] 我不知道介绍对不对
[00:00:21,320 -> 00:00:22,399] 如果你可以自带补充一下
[00:00:22,399 -> 00:00:23,199] 挺好的 挺好的
[00:00:23,199 -> 00:00:27,280] 谢谢孙玉珍同学的邀请
[00:00:27,280 -> 00:00:29,440] 很高兴来跟大家聊这个东西
[00:00:00,000 -> 00:00:10,240] 我觉得,罗一同学,你的经历是比较接触了人工智能的,算是比较多的方面。
[00:00:10,240 -> 00:00:13,839] 因为像我们知道,现在大厂里面应用说的人工智能,
[00:00:13,839 -> 00:00:17,120] 大概讲的就是机器学习,尤其是深度学习。
[00:00:17,120 -> 00:00:22,399] 但是我想请你先帮我们讲一下人工智能到底是什么,
[00:00:22,399 -> 00:00:26,000] 和简单的介绍一下这方面的历史,然后可不可以。
[00:00:00,000 -> 00:00:02,560] 对 我觉得可以
[00:00:02,560 -> 00:00:07,280] 就是这个问题是我是这么看的
[00:00:07,280 -> 00:00:10,160] 人工智能有一个定义是很有意思
[00:00:10,160 -> 00:00:15,119] 他就说人工智能就是你凡是你还没有做出来的东西
[00:00:15,119 -> 00:00:16,719] 它就是一个人工智能
[00:00:16,719 -> 00:00:17,879] 怎么说呢
[00:00:17,879 -> 00:00:21,039] 比如说在IBM的深蓝出来之前
[00:00:21,039 -> 00:00:23,039] 你要是下国际象棋下的很好
[00:00:23,039 -> 00:00:24,120] 你就是个人工智能
[00:00:24,120 -> 00:00:26,920] 但是他做出来之后这个东西已经解决了
[00:00:26,920 -> 00:00:29,000] 大家再说这个他就不酷了
[00:00:00,000 -> 00:00:03,000] 在AlphaGo做出围棋AI之前
[00:00:03,000 -> 00:00:04,599] 大家都认为围棋AI
[00:00:04,599 -> 00:00:08,199] 围棋它可能是很难做出来一个战胜人的一个东西
[00:00:08,199 -> 00:00:11,000] 这是很多原因决定的
[00:00:11,000 -> 00:00:12,800] 但是他后来就把它做出来了
[00:00:12,800 -> 00:00:16,600] 基于深度神经网络和强化学就把它给做出来了
[00:00:16,600 -> 00:00:19,600] 那之后大家就认为这个东西已经解决了
[00:00:19,600 -> 00:00:25,199] 所以人工智能就是我认为凡是
[00:00:25,199 -> 00:00:29,199] 你用人的智力解决的问题
[00:00:00,000 -> 00:00:02,040] 或者甚至是人的智力解决不了的问题
[00:00:02,040 -> 00:00:03,160] 你用机器来解决
[00:00:03,160 -> 00:00:05,759] 它就可以放到人工智能里面去
[00:00:06,759 -> 00:00:08,720] 这是一个比较宽泛的定义的方式
[00:00:08,720 -> 00:00:11,800] 我们后来说的机器学习
[00:00:11,800 -> 00:00:14,160] 特别是现在的深度学习
[00:00:14,599 -> 00:00:16,079] 它是人工智能的一个分支
[00:00:16,079 -> 00:00:20,800] 像我在11年上研究生level的人工智能课的时候
[00:00:22,719 -> 00:00:27,280] 那几十讲理课里面没有一讲在提深度学习
[00:00:00,000 -> 00:00:03,000] 因为那时候刚好是那个深度学习爆发的前夜
[00:00:03,640 -> 00:00:07,080] 就是机器学习课和人工学习课两个
[00:00:07,080 -> 00:00:10,720] 你会发现你选机器学习课那个的人很少
[00:00:11,080 -> 00:00:11,679] 对
[00:00:12,199 -> 00:00:14,759] 那这个样子其实蛮有意思的
[00:00:14,759 -> 00:00:17,000] 就是首先我们定义了一下
[00:00:17,000 -> 00:00:19,440] 就是这个机器学习和深度学习
[00:00:19,679 -> 00:00:21,920] 是人工智能的一个子集对吧
[00:00:22,320 -> 00:00:25,199] 但是现在大家机器学习
[00:00:25,199 -> 00:00:27,079] 尤其是深度学习又特别特别火
[00:00:27,359 -> 00:00:29,839] 包括你刚刚说的很多之前解决不了的问题
[00:00:00,000 -> 00:00:03,319] 现在都是因为有了深度学习的发展才可以解决的
[00:00:03,600 -> 00:00:07,080] 那可不可以帮我们把这三个者之间的关系介绍一下呢
[00:00:08,240 -> 00:00:09,800] 就是再详细阐述一下
[00:00:12,080 -> 00:00:14,359] 可以 我觉得这样来说
[00:00:14,359 -> 00:00:16,320] 我们先说机器学习和深度学习吧
[00:00:16,320 -> 00:00:20,480] 就是深度学习肯定是机器学习之后才出来的嘛
[00:00:20,480 -> 00:00:24,239] 机器学习你的按照常见的分类
[00:00:24,239 -> 00:00:27,879] 你可以分成监督学习 无监督学习
[00:00:00,000 -> 00:00:02,279] 还有强化学习你可以这样分类
[00:00:02,279 -> 00:00:04,320] 然后刨开强化学习
[00:00:04,320 -> 00:00:06,160] 监督学习无监督学习
[00:00:06,160 -> 00:00:09,199] 它你可以都认为它是一个给定一个数据集
[00:00:09,199 -> 00:00:12,800] 然后我要从这个数据集里面学习到一些pattern
[00:00:12,800 -> 00:00:13,960] 或者一些表征
[00:00:13,960 -> 00:00:16,320] 然后再给你一个新的数据集
[00:00:16,320 -> 00:00:20,920] 你可以把它给generalize到那个新的数据集
[00:00:20,920 -> 00:00:23,960] either是监督学习的一个标签
[00:00:23,960 -> 00:00:27,199] 或者是无监督学习的一些特点或者一些pattern
[00:00:00,000 -> 00:00:07,440] 所以它,你如果从那个,因为它本质上是个基于一个特定数据级的一个优化问题。
[00:00:07,440 -> 00:00:14,199] 就是从你从数学优化,从那个运程学或者应用数学的角度来看,它就是这样,它的本质就是这样的。
[00:00:14,199 -> 00:00:20,600] 你这个说的稍稍有点,有点就是专业啊,我可不可以这样理解一下。
[00:00:20,600 -> 00:00:26,199] 就是所谓的机器学习,它做的一件事情,就是把一堆特征,
[00:00:00,000 -> 00:00:03,759] 把一堆事物的特征和一些结果给联系起来,
[00:00:03,759 -> 00:00:06,360] 然后把这些特征和结果都告诉机器,
[00:00:06,360 -> 00:00:09,160] 机器从这学中学习其中的规律,
[00:00:09,160 -> 00:00:13,439] 然后他把这些规律再应用到那些我新的东西上,
[00:00:13,439 -> 00:00:17,039] 然后我告诉他特征,他告诉我一个结果,是这样吧。
[00:00:17,039 -> 00:00:20,960] 对,就是他一个强调的一个点就是他一定要能generalize,
[00:00:20,960 -> 00:00:22,280] 他从已经观测到的东西,
[00:00:22,280 -> 00:00:26,559] 他能够放花到一个新的未观测到的东西上面去,
[00:00:26,559 -> 00:00:28,000] 这就是他的一个分析。
[00:00:00,000 -> 00:00:02,720] 也就是说明了为什么数学这么重要是吧,
[00:00:02,720 -> 00:00:04,080] 因为我们有了这个数学,
[00:00:04,080 -> 00:00:05,599] 我们可能会去练这些模型。
[00:00:06,120 -> 00:00:08,839] 对,数学和计算机科学,对,
[00:00:08,839 -> 00:00:10,199] 就是它的那个基础。
[00:00:10,400 -> 00:00:11,400] 那为什么我们,
[00:00:11,400 -> 00:00:14,240] 按道理这些数据可能在之前就有了呀,
[00:00:14,400 -> 00:00:16,000] 为什么是在2011年之后,
[00:00:16,000 -> 00:00:17,960] 这个深度学习才有一个大发展呢?
[00:00:18,000 -> 00:00:20,079] 哦,还有刚才没有完全说完的,
[00:00:20,079 -> 00:00:24,879] 深度学习和基地学习到底是这个中间的联系和差别是什么?
[00:00:25,800 -> 00:00:28,199] 对,其实你刚才说这两个问题可以一起说,
[00:00:00,000 -> 00:00:05,000] 就是为什么他突然深度学习就爆发了,成了一个特别时髦的一个词。
[00:00:05,000 -> 00:00:21,000] 其实是这样的,就是神经网络其实很早就有了,七八十年代就有了,那个现在的得了图林奖的三巨头之一,那个桂脸,我不知道现在还在不在桂脸,桂脸的那个人工智能中心的那个杨乐坤,他就是先驱。
[00:00:21,000 -> 00:00:24,000] 刚刚说七八十年代,是不是就概念提出更早一点?
[00:00:00,000 -> 00:00:05,000] 对 其实50年代就有那个比如说那个perceptron那个感知器
[00:00:05,000 -> 00:00:08,000] 然后感知器组成一个神经网络的layer
[00:00:08,000 -> 00:00:09,000] 就是这样的概念已经有了
[00:00:09,000 -> 00:00:14,000] 但是七八十年代是提出了那个反向那个propagation
[00:00:14,000 -> 00:00:16,000] 那个中文叫啥
[00:00:16,000 -> 00:00:18,000] 就是那个back propagation和forward propagation
[00:00:18,000 -> 00:00:20,000] 这样他才能够train这个parameter
[00:00:20,000 -> 00:00:23,000] 他才能够把它给根据数据给train出来
[00:00:23,000 -> 00:00:25,000] train出他的那个放话的能力
[00:00:25,000 -> 00:00:28,000] 呃 这个东西很早就有了
[00:00:00,000 -> 00:00:04,160] 当时杨乐坤我记得他把它做了一个应用
[00:00:04,160 -> 00:00:08,679] 就是给那个美国邮政局识别那个那些字
[00:00:08,679 -> 00:00:10,240] 就是我给他一个信封
[00:00:10,240 -> 00:00:12,640] 他可能可以把那个地址给他识别出来
[00:00:12,640 -> 00:00:14,880] 就极大地提高了那个他的效率
[00:00:14,880 -> 00:00:18,079] 这是他应该是很早就很出名的一个应用
[00:00:18,079 -> 00:00:23,239] 为什么到了90年代和一直到11年12年左右
[00:00:23,239 -> 00:00:24,640] 他都就沉寂了呢
[00:00:24,640 -> 00:00:27,719] 我觉得主要是两个原因
[00:00:00,000 -> 00:00:04,599] 一个是最重要的一个原因是其实神机网络它算起来还挺复杂的
[00:00:04,599 -> 00:00:08,199] 它其实是一个比较复杂的一个算法
[00:00:08,199 -> 00:00:10,720] 这个算法导致了一个什么原因呢
[00:00:10,720 -> 00:00:17,199] 就是它如果把那个神机网络的结构做的很复杂
[00:00:17,199 -> 00:00:18,320] 就往深了做
[00:00:18,320 -> 00:00:19,480] 你就算不出来
[00:00:19,480 -> 00:00:21,719] 因为当时都是CPU单机
[00:00:21,719 -> 00:00:23,519] 就最多几个核在那算
[00:00:23,519 -> 00:00:25,079] 如果这样就算不出来
[00:00:25,079 -> 00:00:25,879] 如果太浅了
[00:00:25,879 -> 00:00:28,280] 你可以用别的方法来取得差不多的效果
[00:00:00,000 -> 00:00:07,519] 比如说你可以用这些算量机或者说一些更简单的一些算法都可以达到可能也差不多的一样的效果
[00:00:07,519 -> 00:00:10,080] 大家就没有这个动力去做这个事情了
[00:00:10,080 -> 00:00:15,199] 所以是说还是因为这个算力导致的它的性价比不高
[00:00:15,199 -> 00:00:19,559] 对算力是一个然后还有一点就是当时其实它数据确实也不多
[00:00:19,559 -> 00:00:23,359] 那时候就是90年代零几年互联网公司也才刚起来
[00:00:23,359 -> 00:00:25,160] 那没有那么多海量的数据
[00:00:25,160 -> 00:00:29,440] 那时候最多可能就大家平时的数据机可能几百兆不得了
[00:00:00,000 -> 00:00:08,400] 就是他也没有那么多use case,大家也没想到有什么那个就是可以用的,一直到那个11年12年。
[00:00:08,400 -> 00:00:17,399] 因为你说到use case,我觉得其实应用是这个这个所谓的人工智能非常重要的一方面,但是我们回头再。
[00:00:17,399 -> 00:00:28,679] 对,其实你看美国的科技史,它都是由应用驱动它的发展,比如说军队方面一个类似internet的概念,它不知道拿来干嘛,是吧?
[00:00:00,000 -> 00:00:02,160] 然后民间它才有各种各样的use case
[00:00:02,600 -> 00:00:04,599] 好,那我们先继续把这个话题聊完
[00:00:04,599 -> 00:00:05,679] 然后再回来讲应用吧
[00:00:06,120 -> 00:00:07,080] 对,可以
[00:00:07,879 -> 00:00:10,279] 对,你刚刚说就是因为这个算
[00:00:10,279 -> 00:00:11,919] 首先是算力的不足
[00:00:11,919 -> 00:00:15,160] 然后是这个数据和就是因为没有应用
[00:00:15,160 -> 00:00:18,719] 所以说导致大家没有去收集这些数据
[00:00:18,960 -> 00:00:20,800] 所以导致它没有发展起来
[00:00:20,960 -> 00:00:24,199] 那11年有没有什么这个劃时代的发现
[00:00:24,199 -> 00:00:28,000] 或者说是一个一些特别大的进步
[00:00:28,000 -> 00:00:29,280] 导致它一下子爆发了呢
[00:00:00,000 -> 00:00:05,440] 对,据我了解有这么几个点吧,一个是GPU起来了,
[00:00:05,440 -> 00:00:10,199] 就GPU以前都拿来打游戏,后来突然发现它可以用来做一些计算,
[00:00:10,199 -> 00:00:14,599] 然后就是stochastic gradient descent这个算法。
[00:00:14,599 -> 00:00:19,679] 给我们简单的介绍一下就是GPU和CPU的运算的区别是什么?
[00:00:20,480 -> 00:00:23,280] 就是为什么GPU算起来会快很多?
[00:00:24,120 -> 00:00:29,199] 对,就是CPU它相当于是一个,就除了你几何的是吧?
[00:00:00,000 -> 00:00:02,200] 你是一个single thread的计算
[00:00:02,200 -> 00:00:06,080] 它用来算你的浮点运算是比较好的
[00:00:06,080 -> 00:00:11,800] 但是像你深度学习里面有个算法
[00:00:11,800 -> 00:00:15,560] 比如说你最经典的stochastic gradient descent
[00:00:15,560 -> 00:00:17,199] 随机的t度下降
[00:00:17,199 -> 00:00:20,519] 那时候你用GPU就可以得到很快的加速
[00:00:20,519 -> 00:00:23,280] 因为你可以认为它一个一个小的
[00:00:23,280 -> 00:00:26,559] 它在同时进行很多的sample和计算
[00:00:26,559 -> 00:00:27,440] 它可以同时
[00:00:27,440 -> 00:00:29,199] 你可以通过一些设计
[00:00:00,000 -> 00:00:03,220] 你可以让它,其实就是变形算法那些东西,
[00:00:03,220 -> 00:00:05,240] 但是你可以让它很快的进行那个
[00:00:05,240 -> 00:00:07,360] parameter的那个update,
[00:00:07,360 -> 00:00:11,720] 这样就是一下子就可以让那个神经网络做得更深了。
[00:00:11,720 -> 00:00:13,560] 我不可以这样理解啊,
[00:00:13,560 -> 00:00:17,000] 就是CPU是用来算一些比较复杂的运算,
[00:00:17,000 -> 00:00:20,079] 但是GPU可以同时算很多简单的运算。
[00:00:20,079 -> 00:00:21,359] 很多简单的运算,对,
[00:00:21,359 -> 00:00:23,399] 你可能可以开一万个东西同时算。
[00:00:23,399 -> 00:00:26,600] 因为这也是GPU当时设计出来,
[00:00:26,600 -> 00:00:29,000] 就是为了它同时render一个画面嘛。
[00:00:00,000 -> 00:00:03,080] 对的,就是这样的,exactly,对。
[00:00:03,080 -> 00:00:06,839] 那这样的话就是相当于我们一下子把算力
[00:00:06,839 -> 00:00:10,039] 在那个在硬件没有大发展的情况下
[00:00:10,039 -> 00:00:13,560] 发现了GPU的应用导致深度学习的算力
[00:00:13,560 -> 00:00:15,359] 一下子被放大了几万倍,
[00:00:15,359 -> 00:00:17,800] 那它很多算法之前做不了很深。
[00:00:17,800 -> 00:00:19,960] 对,它现在可以做深了。
[00:00:19,960 -> 00:00:23,839] 它的模型的performance一下子就提高了很多,
[00:00:23,839 -> 00:00:27,239] 那可能比70%的精度,现在90%的精度,
[00:00:00,000 -> 00:00:03,640] 那它就可以这个,大家就可以想象出来很多应用。
[00:00:03,640 -> 00:00:07,799] 然后又去收集数据从而把它带上一个正循环,
[00:00:07,799 -> 00:00:09,720] 导致现在的这个发展。
[00:00:09,720 -> 00:00:12,519] 但是那个时候呢,就是还没有现在那么多use case,
[00:00:12,519 -> 00:00:16,760] 那个时候大家就从那么几个经典的use case里面来测试,
[00:00:16,760 -> 00:00:20,640] 比如说这个深度网络,深度神经网络到底是不是work。
[00:00:20,640 -> 00:00:23,399] 就比如说像我们刚才提到Siri,
[00:00:23,399 -> 00:00:27,440] 其实更早,我觉得微软那个语音助手好像比Siri还要早。
[00:00:00,000 -> 00:00:04,459] 所以说在11年左右那个Hinton就是那个大佬
[00:00:04,459 -> 00:00:08,060] Geoffrey Hinton和微软研究院的那个邓丽是个中国人
[00:00:08,060 -> 00:00:12,300] 他们就写了一个发表在那个IEEE
[00:00:12,300 -> 00:00:15,099] 好像是Signal Processing上面的一个论文
[00:00:15,099 -> 00:00:18,620] 就是那个其实算深度学习的一个化石台的一个论文
[00:00:18,620 -> 00:00:23,460] 他用在了ASR就是Speech Recognition上面得到了很好的效果
[00:00:23,460 -> 00:00:27,420] 就是一下子把那个以前的baseline给提高了很多很多
[00:00:00,000 -> 00:00:02,240] 这是什么时候的?
[00:00:02,240 -> 00:00:05,440] 11年我记得是,11年
[00:00:05,440 -> 00:00:08,039] OK,11年12年这样子
[00:00:08,039 -> 00:00:11,640] 然后包括后来大家看到这个好像可以
[00:00:11,640 -> 00:00:13,240] 那我们试一下另外一个标准的问题
[00:00:13,240 -> 00:00:14,080] 图像识别
[00:00:14,080 -> 00:00:15,960] 对,说到这个图像识别
[00:00:15,960 -> 00:00:18,519] 就是我们要感谢李飞飞教授
[00:00:18,519 -> 00:00:21,079] 他一直在致力于搜集那个ImageNet
[00:00:21,079 -> 00:00:23,280] 他从早就开始有这个意识
[00:00:23,280 -> 00:00:24,679] 一直在搜集这个数据
[00:00:24,679 -> 00:00:27,839] 所以到了后来大家就用那个做一个标准的测试机
[00:00:00,000 -> 00:00:03,439] 我不断的在上面提高performance和刷榜
[00:00:03,439 -> 00:00:07,280] 就是像你推出来一个比如说AlexNet
[00:00:07,280 -> 00:00:09,519] 我第二年又出来一个ResNet
[00:00:09,519 -> 00:00:12,320] 然后就这样,它可以一下子把那个performance
[00:00:12,320 -> 00:00:14,439] 我觉得AlexNet提出来的时候
[00:00:14,439 -> 00:00:17,399] 它把之前的那个performance甩开好多好多
[00:00:17,399 -> 00:00:21,760] 大家一下子就觉得深度学习好像真的很厉害
[00:00:21,760 -> 00:00:22,280] 对
[00:00:22,280 -> 00:00:26,359] 我就是给没有背景的同学稍微解释一下
[00:00:26,359 -> 00:00:28,320] ImageNet它其实是一个数据集
[00:00:28,320 -> 00:00:29,839] 它里边有很多很多的数据
[00:00:00,000 -> 00:00:03,919] 然后你的可以放上去大家是驴子是马拉出来遛遛
[00:00:03,919 -> 00:00:07,879] 然后看一下你的识别的准确度是如何的
[00:00:07,879 -> 00:00:10,080] 那这样子所有的新的模型出来以后
[00:00:10,080 -> 00:00:12,080] 它都会拿到这个标准的数据上
[00:00:12,080 -> 00:00:15,279] 去看我能不能把它很好的给这个识别出来
[00:00:15,279 -> 00:00:16,879] 所以说它还有一个榜单
[00:00:16,879 -> 00:00:19,440] 那所有的模型都会在这上面有一个榜单
[00:00:19,440 -> 00:00:23,879] 其实国内有很多这个公司热衷于干这种大胆的事情
[00:00:23,879 -> 00:00:27,000] 对 包括其实这也是一个很好的途径吧
[00:00:27,000 -> 00:00:29,000] 比如说你刚刚那个矿石出来的时候
[00:00:00,000 -> 00:00:03,439] 他们干了很多刷榜的事情
[00:00:03,439 -> 00:00:05,879] 这样你可以快速提高你的知名度
[00:00:05,879 -> 00:00:08,039] 对 也是一个相对科学
[00:00:08,039 -> 00:00:10,519] 当然我知道这里面还有很多各种各样的事情
[00:00:10,519 -> 00:00:12,320] 但是也很相对科学
