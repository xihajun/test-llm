[00:00:00,000 -> 00:00:01,399] Hello大家好欢迎回到课代表雷震
[00:00:01,399 -> 00:00:02,439] 我是门德的课代表
[00:00:02,439 -> 00:00:05,040] 这期视频我们继续课代表聊数据
[00:00:05,040 -> 00:00:08,000] 我们继续聊数据科学家和决策的关系
[00:00:08,000 -> 00:00:09,439] 在这我来问一个问题
[00:00:09,800 -> 00:00:12,839] 如果屏幕前的同学是数据科学家
[00:00:12,839 -> 00:00:15,320] 或者你们跟数据科学家打过交道
[00:00:15,359 -> 00:00:17,280] 有多少人认为
[00:00:17,719 -> 00:00:21,359] 你在有很确定的cultural evidence的时候
[00:00:21,399 -> 00:00:23,199] 敢对决策发表意见
[00:00:23,280 -> 00:00:24,519] 有的话扣1
[00:00:24,800 -> 00:00:26,039] 如果不敢的话扣2
[00:00:26,199 -> 00:00:26,559] 好
[00:00:26,559 -> 00:00:27,239] 接下来
[00:00:27,280 -> 00:00:29,359] 如果你没有cultural evidence
[00:00:00,000 -> 00:00:02,120] 这个时候你有一些correlation
[00:00:02,120 -> 00:00:04,120] 然后你有一些其他的analysis
[00:00:04,120 -> 00:00:07,320] 然后你对你的insights觉得很确定
[00:00:07,320 -> 00:00:10,199] 你觉得就是A虽然没有causal evidence
[00:00:10,199 -> 00:00:11,240] 但是能导致B
[00:00:11,240 -> 00:00:13,000] 或者说你觉得做A是对的
[00:00:13,119 -> 00:00:14,839] 哪怕在没有A B实验
[00:00:14,839 -> 00:00:17,039] 或者各种各样causal analysis的
[00:00:17,160 -> 00:00:18,120] 基础之上
[00:00:18,120 -> 00:00:19,519] 你觉得做A是对的
[00:00:19,679 -> 00:00:23,239] 这个时候你敢不敢对决策发表意见
[00:00:23,239 -> 00:00:25,199] 如果敢的话请打1
[00:00:25,199 -> 00:00:26,920] 如果不敢的话请打2
[00:00:26,920 -> 00:00:27,800] 第三种情况
[00:00:00,000 -> 00:00:02,359] 你只有一些基本的data或者facts
[00:00:02,359 -> 00:00:05,759] 可是你对这件事情有一定的理解
[00:00:05,759 -> 00:00:09,439] 所以说你虽然你的analysis
[00:00:09,439 -> 00:00:11,599] 不能证明作为A是对的
[00:00:11,599 -> 00:00:13,640] 可是你觉得作为A是对的
[00:00:13,640 -> 00:00:17,679] 有多少人敢在这个时候对这个决策发表意见
[00:00:17,679 -> 00:00:19,879] 敢的请打一不敢的请打二
[00:00:19,879 -> 00:00:22,559] 然后我这个也是在Facebook跟那个大佬
[00:00:22,559 -> 00:00:23,719] 上那门课的时候
[00:00:23,719 -> 00:00:25,559] 他给我的一个特别大的启发
[00:00:25,559 -> 00:00:29,719] 他认为数据科学家在情况三的时候
[00:00:00,000 -> 00:00:01,439] 也应该敢说话
[00:00:01,480 -> 00:00:03,080] 因为在情况三的时候
[00:00:03,080 -> 00:00:05,160] 其他的所有的职业都敢说话
[00:00:05,200 -> 00:00:07,960] 尤其是作为一个PM是吧
[00:00:08,000 -> 00:00:10,199] 然后各种各样的leadership
[00:00:10,320 -> 00:00:12,039] 他们的工作就是在
[00:00:12,039 -> 00:00:14,720] 充满uncertainty的情况下去做一个决定
[00:00:14,720 -> 00:00:15,960] 这个决定可能是对的
[00:00:15,960 -> 00:00:16,879] 可能是错的
[00:00:16,920 -> 00:00:19,039] 有的时候是需要你的intuition的
[00:00:19,039 -> 00:00:21,440] 但是大家敢做这个决定
[00:00:21,440 -> 00:00:24,480] 起码大家敢对这个决定发表意见
[00:00:24,480 -> 00:00:26,800] 就是我们应不应该做这个决定发表意见
[00:00:27,000 -> 00:00:29,399] 但是很多数据科学家
[00:00:00,000 -> 00:00:01,840] 因为职业受到的training
[00:00:01,840 -> 00:00:02,919] 我们就会觉得
[00:00:02,919 -> 00:00:05,240] 我只有在最concrete
[00:00:05,240 -> 00:00:08,039] 最concise最无懈可击的证据之下
[00:00:08,039 -> 00:00:09,199] 我才能发表意见
[00:00:09,199 -> 00:00:11,800] 如果我没有这些数据或者这些analysis
[00:00:11,800 -> 00:00:13,519] 去backup我的观点的话
[00:00:13,519 -> 00:00:15,519] 那我就不应该去发表意见
[00:00:15,519 -> 00:00:19,039] 我应该把发表意见的责任和权利
[00:00:19,039 -> 00:00:20,199] 交给其他人
[00:00:20,280 -> 00:00:22,920] 这样做在我看来是不对的
[00:00:22,920 -> 00:00:23,719] 为什么呢
[00:00:23,719 -> 00:00:25,559] 就是因为第一
[00:00:25,559 -> 00:00:27,640] PM比我们都长了个脑袋吗
[00:00:27,640 -> 00:00:28,199] 没有啊
[00:00:00,000 -> 00:00:03,279] 就是他们受到了很多他们独有的training
[00:00:03,279 -> 00:00:07,799] 能帮助他们对这个决策产生有价值的input
[00:00:07,799 -> 00:00:10,119] 可是我们也受到了很多的training
[00:00:10,119 -> 00:00:12,119] 我们有这个scientific的training
[00:00:12,119 -> 00:00:13,759] 我们对数据更敏感
[00:00:13,759 -> 00:00:16,000] 我们有很多logical的training
[00:00:16,000 -> 00:00:19,719] 所以说我们一样可以对这个决策
[00:00:19,719 -> 00:00:21,359] 在没有数据的情况下
[00:00:21,359 -> 00:00:24,800] 在我们没有一个analysis来证明这个决策正确的情况下
[00:00:24,800 -> 00:00:27,079] 然后对这个决策产生了正向的贡献
[00:00:27,079 -> 00:00:28,519] 这是事情的一面
[00:00:00,000 -> 00:00:02,240] 就是我们不要把自己的决策
[00:00:02,240 -> 00:00:04,919] 因为我们受到的training更rigorous
[00:00:04,919 -> 00:00:09,080] 而局限在了只能得到rigorous answer的这一部分
[00:00:09,080 -> 00:00:13,320] 而是你应该把你的注意力放在更大范围
[00:00:13,320 -> 00:00:14,439] 更重要的决策上
[00:00:14,439 -> 00:00:17,719] 这些重要的决策很多时候是有很多模糊性的
[00:00:17,719 -> 00:00:20,039] 但是作为一个数据科学家
[00:00:20,039 -> 00:00:24,120] 你也应该敢于对这些决策发表意见
[00:00:24,120 -> 00:00:29,719] 但是事情的反面就是我们要对一个决策中
[00:00:00,000 -> 00:00:02,960] 到底有多少是能被数据measure的
[00:00:02,960 -> 00:00:04,679] 有一个清醒的认知
[00:00:04,679 -> 00:00:07,879] 前面的一部分是在你没有数据的情况下
[00:00:07,879 -> 00:00:09,000] 也要敢做决策
[00:00:09,000 -> 00:00:10,960] 后面这部分就是不要过分
[00:00:10,960 -> 00:00:13,839] quadrate数据在决策中的作用
[00:00:13,839 -> 00:00:15,720] 我发现后面这种情况
[00:00:15,720 -> 00:00:17,600] 就是我之前的一个视频讲的
[00:00:17,600 -> 00:00:21,920] 很多企业会依赖模型做决策而犯下的错误
[00:00:21,920 -> 00:00:24,839] 但是我其实后来有一个苏刚同学
[00:00:24,839 -> 00:00:26,960] 在跟我讨论的时候跟我说了
[00:00:00,000 -> 00:00:03,200] 這個 McNamara fallacy
[00:00:03,200 -> 00:00:05,000] 我才發現
[00:00:05,000 -> 00:00:07,480] 原來這件事情不是什麼
[00:00:07,480 -> 00:00:10,000] 現代的互聯網大企業才發生的
[00:00:10,000 -> 00:00:12,960] 這是一個講的美國越戰的故事
[00:00:12,960 -> 00:00:16,879] 當時美國的國防部長叫這個 McNamara
[00:00:16,879 -> 00:00:19,120] 他當時在這個越戰中
[00:00:19,120 -> 00:00:22,000] 就犯下了一個很致命的錯誤
[00:00:22,000 -> 00:00:23,800] 這個致命的錯誤是什麼呢
[00:00:23,800 -> 00:00:26,480] 他這裡邊說有四步對吧
[00:00:26,480 -> 00:00:27,640] 第一步呢
[00:00:00,000 -> 00:00:03,000] 是去measure whatever can be easily measured
[00:00:03,480 -> 00:00:05,280] 比如說在越戰的時候
[00:00:05,280 -> 00:00:09,400] 他們去measure的就是我們消滅了多少敵軍
[00:00:09,400 -> 00:00:10,880] 我們的損失是多少
[00:00:10,919 -> 00:00:13,400] 他們覺得如果我們消滅了足夠多的敵軍
[00:00:13,400 -> 00:00:16,199] 我們的這個戰爭就會走向勝利
[00:00:16,600 -> 00:00:18,480] 聽起來好像是有點道理的
[00:00:19,000 -> 00:00:22,600] 但是這第二部分就是disregard
[00:00:22,640 -> 00:00:25,079] that which can't be easily measured
[00:00:25,440 -> 00:00:27,960] 就是你不能去measure的東西
[00:00:27,960 -> 00:00:29,480] 你就直接不管了
[00:00:00,000 -> 00:00:02,000] 第三步就是去assume
[00:00:02,000 -> 00:00:04,240] What cannot be measured
[00:00:04,240 -> 00:00:05,839] really isn't important
[00:00:05,839 -> 00:00:07,519] 然後第四步是
[00:00:07,519 -> 00:00:08,960] What can be easily measured
[00:00:08,960 -> 00:00:10,240] really doesn't exist
[00:00:10,240 -> 00:00:12,240] 就是開始追上眼睛說
[00:00:12,240 -> 00:00:13,679] 我不能measure的東西
[00:00:13,679 -> 00:00:15,439] 其實對我的決策不重要
[00:00:15,439 -> 00:00:16,879] 或者說根本就不存在
[00:00:16,879 -> 00:00:21,120] 在這裡邊就是越南人民對美國的憎恨
[00:00:21,120 -> 00:00:23,199] 越南人民的抵抗決心
[00:00:23,199 -> 00:00:27,039] 他比如說去轟炸更多的地方
[00:00:27,039 -> 00:00:29,120] 或者說是把軍事介入
[00:00:00,000 -> 00:00:02,120] 投入到一个更大的范围
[00:00:02,120 -> 00:00:06,879] 以及他去屠杀了更多的越南的士兵
[00:00:06,879 -> 00:00:08,039] 或者民众也好
[00:00:08,039 -> 00:00:10,160] 激起了越南人民的反抗决心
[00:00:10,199 -> 00:00:13,439] 其实是让他离他战争胜利的目标
[00:00:13,439 -> 00:00:14,400] 会越来越远
[00:00:14,400 -> 00:00:16,719] 从而他消灭多少敌军的
[00:00:16,719 -> 00:00:18,640] 那个可以被easily measure的
[00:00:18,640 -> 00:00:20,839] 那个goal变得不是那么重要的
[00:00:20,879 -> 00:00:24,079] 因为他激起了整个国家的反抗意志
[00:00:24,120 -> 00:00:27,000] 他陷入了人民战争的汪洋大海
[00:00:27,039 -> 00:00:28,120] 尸道寡住
[00:00:28,160 -> 00:00:28,640] 对吧
[00:00:00,000 -> 00:00:03,240] 抽離來看的話是一個很明顯的事情
[00:00:03,240 -> 00:00:07,679] 可是當時美國的國防部就犯了一個
[00:00:07,679 -> 00:00:10,599] 試圖scientifically來track
[00:00:10,599 -> 00:00:12,119] 這個world progress的東西
[00:00:12,119 -> 00:00:14,839] 因為他們是已經形成了一個匯報體系
[00:00:14,839 -> 00:00:17,559] 坐在辦公室裡的國防部長
[00:00:17,559 -> 00:00:20,559] 他說我需要用各種各樣的一套metrics
[00:00:20,559 -> 00:00:22,679] 來track我的戰爭的進展
[00:00:22,679 -> 00:00:23,879] 我是不是要贏了
[00:00:23,879 -> 00:00:24,920] 我可以去跟人匯報
[00:00:24,920 -> 00:00:25,839] 我可以去拿預算
[00:00:25,839 -> 00:00:27,519] 變成了challenge的一件事情以後
[00:00:00,000 -> 00:00:02,879] 他们就忽略了很多常识
[00:00:02,879 -> 00:00:06,120] 而去用这种所谓的data driven decisions
[00:00:06,120 -> 00:00:08,599] 最后导致决策的失败
[00:00:09,599 -> 00:00:10,160] 好
[00:00:10,199 -> 00:00:12,679] 这就是事情的另外一部分
[00:00:12,720 -> 00:00:15,400] 我们一定要知道这个决策里边
[00:00:15,439 -> 00:00:19,000] 到底有多少东西是可以被easily measure的
[00:00:19,000 -> 00:00:22,320] 然后也要respect那些不能被easily measure的
[00:00:22,320 -> 00:00:24,800] 并且给他们足够多的attention
[00:00:24,800 -> 00:00:26,280] 我们要运用我们的常识
[00:00:26,280 -> 00:00:27,559] 我们要运用我们的
[00:00:27,559 -> 00:00:29,480] 像那些视频说的knowhow的能力
[00:00:00,000 -> 00:00:01,600] 去把这些地方搞懂
[00:00:01,600 -> 00:00:04,040] 最后才能做出一个好的决策
[00:00:04,040 -> 00:00:04,719] 好的
[00:00:04,719 -> 00:00:06,719] 这期视频我们就到这
[00:00:06,719 -> 00:00:09,480] 我就是想跟各位数据科学家们
[00:00:09,480 -> 00:00:12,119] 去讲清楚这一个道理
[00:00:12,119 -> 00:00:13,560] 就是第一
[00:00:13,560 -> 00:00:16,239] 你要勇于在没有数据
[00:00:16,239 -> 00:00:17,960] backup你的观点的情况下
[00:00:17,960 -> 00:00:20,280] 去对重要的决策发表意见
[00:00:20,280 -> 00:00:24,079] 并且你要去积极的寻找那些重要的决策
[00:00:24,079 -> 00:00:25,559] 而不只是把你的目光
[00:00:25,559 -> 00:00:28,199] 局限在可以被数据回答的决策
[00:00:00,000 -> 00:00:02,480] 第二对数据科学家也好
[00:00:02,480 -> 00:00:03,359] 对所有人也好
[00:00:03,359 -> 00:00:06,639] 尤其我觉得对数据科学家有义务去告诉所有人
[00:00:06,719 -> 00:00:08,320] 就是在这个决策中
[00:00:08,320 -> 00:00:10,400] 数据到底占了一个多大的比重
[00:00:10,400 -> 00:00:12,599] 有的决策可能就是一个很简单的
[00:00:12,599 -> 00:00:14,519] 用数据基本上就可以做的决策
[00:00:14,560 -> 00:00:16,000] 但是很多决策不是
[00:00:16,000 -> 00:00:18,160] 那这个时候你有必要跟大家说
[00:00:18,199 -> 00:00:21,719] 虽然大家对uncertainty是uncomfortable的
[00:00:21,760 -> 00:00:23,839] 可是我们数据只能measure30%
[00:00:23,879 -> 00:00:25,800] 剩下的70%就是uncertain
[00:00:25,800 -> 00:00:27,079] 就是数据回答不了的
[00:00:00,000 -> 00:00:03,000] 我们就是不应该去用数据去框定这部分
[00:00:03,000 -> 00:00:05,160] 但是我们要足够重视这一部分
[00:00:05,160 -> 00:00:06,759] 好 这期视频就到这
[00:00:06,759 -> 00:00:08,400] 谢谢大家 我们下期再见 拜拜
