[00:00:00,000 -> 00:00:07,000] Also it becomes extremely important like, okay, well, which team should I double down
[00:00:07,000 -> 00:00:08,000] on?
[00:00:08,000 -> 00:00:10,000] Or which team should I wind down on?
[00:00:10,000 -> 00:00:12,000] Or which project should I wind down on?
[00:00:12,000 -> 00:00:13,000] Welcome back.
[00:00:13,000 -> 00:00:14,000] Thank you.
[00:00:14,000 -> 00:00:15,000] It's good to have you.
[00:00:15,000 -> 00:00:18,000] It's good to see the company after a year.
[00:00:18,000 -> 00:00:19,000] Yeah.
[00:00:19,000 -> 00:00:21,600] I think the last time you were here, we were in the Kirkland office, right?
[00:00:21,600 -> 00:00:22,600] Yeah.
[00:00:22,600 -> 00:00:23,600] Yeah, that was a...
[00:00:23,600 -> 00:00:24,600] You had about 10 to 15 people.
[00:00:24,600 -> 00:00:25,600] Yeah.
[00:00:25,600 -> 00:00:26,600] Now we have over 45, 46 people.
[00:00:26,600 -> 00:00:29,079] It's grown quite a bit.
[00:00:00,000 -> 00:00:02,600] By the time you post the video, it'll probably be 50.
[00:00:02,600 -> 00:00:05,919] And how much will you grow by valuation and revenue?
[00:00:05,919 -> 00:00:09,080] I think, so the last time we were at Series A,
[00:00:09,080 -> 00:00:12,439] so we raised Series B, you probably saw,
[00:00:12,439 -> 00:00:16,960] we raised $43 million, and a pretty good valuation.
[00:00:16,960 -> 00:00:18,719] So we're pretty happy.
[00:00:18,719 -> 00:00:20,760] Do you have real customers now?
[00:00:20,760 -> 00:00:22,320] Yeah, we have lots of customers now,
[00:00:22,320 -> 00:00:24,640] so we're pretty, we're doing well
[00:00:24,640 -> 00:00:26,440] in terms of like customer attraction.
[00:00:26,440 -> 00:00:28,960] I know it's one of those things where early stage,
[00:00:00,000 -> 00:00:04,040] When you're building a company, first for five months you get, you're worried if anyone
[00:00:04,040 -> 00:00:05,839] will want to use your product.
[00:00:05,839 -> 00:00:07,559] Every founder's nightmare is that, right?
[00:00:07,559 -> 00:00:11,599] You build something that you believe in and then you wait for somebody to come and use
[00:00:11,599 -> 00:00:12,599] it.
[00:00:12,599 -> 00:00:13,599] Yeah.
[00:00:13,599 -> 00:00:17,039] And then when you actually have people coming in and using the product, the next stage is
[00:00:17,039 -> 00:00:21,039] like, are they going to pay for the service?
[00:00:21,039 -> 00:00:25,280] So getting paying customers is the next level of validation.
[00:00:25,280 -> 00:00:28,120] And so that was nerve wracking.
[00:00:00,000 -> 00:00:04,120] And then once you get one of those, then you write a contract, and then it becomes like,
[00:00:04,120 -> 00:00:08,720] okay, whatever we're building is actually still valuable for people to the point where
[00:00:08,720 -> 00:00:14,359] they're willing to pay a good amount of money for a contract, which is great.
[00:00:14,359 -> 00:00:18,260] And then the next step is like repeat that.
[00:00:18,260 -> 00:00:23,039] Make them feel like the tool is valuable, retain them, and then grow that, make it into
[00:00:23,039 -> 00:00:24,940] a repeatable motion, and so on.
[00:00:00,000 -> 00:00:06,000] So far we have been very fortunate to get a lot of customers that find value in what
[00:00:06,000 -> 00:00:07,200] we're building.
[00:00:07,200 -> 00:00:09,839] So when did all those steps happen?
[00:00:09,839 -> 00:00:13,160] Yeah, I think over time.
[00:00:13,160 -> 00:00:16,800] So I think our first...
[00:00:16,800 -> 00:00:20,800] Last interview, I think you were like two, three months into building the company?
[00:00:20,800 -> 00:00:21,800] Yeah.
[00:00:21,800 -> 00:00:24,120] And the first contract, when did it come?
[00:00:24,120 -> 00:00:28,320] So the first customer came four months in.
[00:00:00,000 -> 00:00:07,080] So that was, I think, a Take app, which was a small app that was built by an ex-Facebook
[00:00:07,080 -> 00:00:10,699] engineer in Singapore.
[00:00:10,699 -> 00:00:14,439] And then he launched it, and then we started seeing traffic.
[00:00:14,439 -> 00:00:19,879] So that was great, because first time ever we saw customers use our product and then
[00:00:19,879 -> 00:00:26,280] the end users using the product or feeling the effect of the product, which is great.
[00:00:00,000 -> 00:00:05,320] And then about six months in, we got the first major customer.
[00:00:05,320 -> 00:00:08,960] That was Headspace, and they were trying the product out.
[00:00:08,960 -> 00:00:16,480] There was also an ex-Facebook team that loved the product, and they missed the tools inside
[00:00:16,480 -> 00:00:17,480] Facebook, right?
[00:00:17,480 -> 00:00:23,280] So it was good to have that kind of validation, and then that subsequently ended up in a contract
[00:00:23,280 -> 00:00:24,739] and so on.
[00:00:00,000 -> 00:00:08,560] And subsequently, one of the good things is the ex-Facebook, ex-Uber, ex-Google, ex-Airbnb,
[00:00:08,560 -> 00:00:13,560] these folks have used tools like this before and they miss the tools.
[00:00:13,560 -> 00:00:17,399] And so when they see Statsync, it kind of resonates really well.
[00:00:17,399 -> 00:00:22,600] And I feel like it's not only do they miss the infrastructure, missing the tool, they
[00:00:22,600 -> 00:00:26,879] also have conviction of what the tool can help them achieve.
[00:00:26,879 -> 00:00:27,879] Yeah.
[00:00:00,000 -> 00:00:08,519] And that is important though, because having an intuitive feeling for how profound these
[00:00:08,519 -> 00:00:15,119] tools can have an impact on your product culture, product shipping culture, the velocity with
[00:00:15,119 -> 00:00:20,000] which you ship, even to the element of engineering happiness.
[00:00:20,000 -> 00:00:26,519] Because instead of debating product features on the merits of your debate, you actually
[00:00:26,519 -> 00:00:29,079] are able to test it out.
[00:00:00,000 -> 00:00:04,559] Everyone can have ideas, everyone can have autonomy to test something out in production
[00:00:04,559 -> 00:00:07,839] and then if the impact is there, then it stays.
[00:00:07,839 -> 00:00:12,480] If the impact is not there, you wind it down and then you move on.
[00:00:12,480 -> 00:00:16,719] It's such a powerful way of running or building software.
[00:00:16,719 -> 00:00:18,760] And the cultural impact doesn't stop with that.
[00:00:18,760 -> 00:00:20,879] I think it continues on.
[00:00:20,879 -> 00:00:27,760] I remember when I used to set up goals, and the goals for the end of the half used to
[00:00:27,760 -> 00:00:29,879] be like shipping goals.
[00:00:00,000 -> 00:00:03,439] Oh, by the end of this half, I'm going to ship these features.
[00:00:03,439 -> 00:00:06,759] And then I remember at Facebook, we never talked about shipping things.
[00:00:06,759 -> 00:00:09,439] It's always about what impact did it have?
[00:00:09,439 -> 00:00:11,880] Did it actually meaningfully...
[00:00:11,880 -> 00:00:15,679] If you ship garbage, then you just do garbage work.
[00:00:15,679 -> 00:00:16,679] It doesn't matter, right?
[00:00:16,679 -> 00:00:21,800] So I think it's important to like, okay, did we add value to our users, to the customers,
[00:00:21,800 -> 00:00:25,300] and how can we quantify that becomes important.
[00:00:00,000 -> 00:00:05,040] I remember seeing this number in the Trustworthy Online Control experiment.
[00:00:05,040 -> 00:00:11,000] 90% of business ideas are either negative or insignificant.
[00:00:11,000 -> 00:00:16,000] I think we saw very similar numbers at Facebook too.
[00:00:16,000 -> 00:00:23,320] I feel like unless you're really good in product sense, I think there should be a level of
[00:00:23,320 -> 00:00:25,640] humility to have.
[00:00:00,000 -> 00:00:03,520] Maybe we don't know everything that we don't know about how customers are going to use
[00:00:03,520 -> 00:00:05,240] our product.
[00:00:05,240 -> 00:00:10,119] And I think a good way to think about it is like, you know, one third of the features
[00:00:10,119 -> 00:00:14,279] you believe you ship are going to be positive for your metrics.
[00:00:14,279 -> 00:00:16,079] One third are going to be neutral.
[00:00:16,079 -> 00:00:19,160] And then about a third is actually hurting.
[00:00:19,160 -> 00:00:22,719] But if you don't know which one is the third, then you don't know.
[00:00:22,719 -> 00:00:23,719] We don't.
[00:00:23,719 -> 00:00:24,719] We never know.
[00:00:24,719 -> 00:00:25,719] Right.
[00:00:00,000 -> 00:00:09,199] Another Twitter from Navelle, the famous Angelis founder, he developed a mental model about
[00:00:09,199 -> 00:00:16,079] the complexity of decisions and his sense is just humans are very bad at making complex
[00:00:16,079 -> 00:00:17,079] decisions.
[00:00:17,079 -> 00:00:20,440] Yeah, especially if there's an element of subjective, like, you know, look, I came up
[00:00:20,440 -> 00:00:24,399] with the idea and I feel so personally invested in the idea.
[00:00:00,000 -> 00:00:07,139] It becomes very hard without data to be able to actually make the right decision.
[00:00:07,139 -> 00:00:12,279] With the current economic climate, it also becomes extremely important.
[00:00:12,279 -> 00:00:15,359] Which team should I double down on?
[00:00:15,359 -> 00:00:17,019] Which team should I wind down?
[00:00:17,019 -> 00:00:18,660] Which project should I wind down?
[00:00:18,660 -> 00:00:19,660] It's important.
[00:00:19,660 -> 00:00:23,120] If you just go and wind down the projects that are actually beneficial to the product,
[00:00:23,120 -> 00:00:24,519] that's actually worse.
[00:00:24,519 -> 00:00:28,480] It never hurts to understand the impact of the effort that you're putting in.
[00:00:00,000 -> 00:00:06,000] I feel like A-B testing is the ultimate tool for achieving intellectual honesty.
[00:00:06,000 -> 00:00:07,000] I think so.
[00:00:07,000 -> 00:00:13,519] Now, one thing I would say is like, you know, A-B testing is the, I think, the state of
[00:00:13,519 -> 00:00:17,960] the art, the way to identify which ones work, which ones don't.
[00:00:17,960 -> 00:00:23,500] Now, but the problem with A-B testing is that it is a time-consuming process.
[00:00:23,500 -> 00:00:27,660] The process of A-B testing comes up with like, first you have to come up with a hypothesis,
[00:00:00,000 -> 00:00:02,799] And then you have to build a variance for validating
[00:00:02,799 -> 00:00:03,879] those hypotheses.
[00:00:03,879 -> 00:00:05,559] Then you have to ship those variants.
[00:00:05,559 -> 00:00:08,560] And then you have to allocate samples, isolate the experiment,
[00:00:08,560 -> 00:00:10,919] run the experiment for, I don't know, two, three weeks,
[00:00:10,919 -> 00:00:13,119] depending on how many samples you have.
[00:00:13,119 -> 00:00:16,000] And then you have to have your data science team go back
[00:00:16,000 -> 00:00:19,359] and analyze it and then verify it,
[00:00:19,359 -> 00:00:22,960] all to whether you validate or invalidate the hypothesis
[00:00:22,960 -> 00:00:24,120] that you originally had.
[00:00:24,120 -> 00:00:25,839] That entire process takes so long
[00:00:25,839 -> 00:00:28,839] that most people don't run experiments as many times
[00:00:00,000 -> 00:00:04,879] they should because then what they do is they save the big decisions for experiments and
[00:00:04,879 -> 00:00:07,639] the rest of them they rely on product intuition.
[00:00:07,639 -> 00:00:08,960] Or the convenient decisions.
[00:00:08,960 -> 00:00:09,960] Yeah, right.
[00:00:09,960 -> 00:00:17,780] And so I think it becomes important for tools to make it so simple.
[00:00:17,780 -> 00:00:18,879] It should be automatic.
[00:00:18,879 -> 00:00:24,239] The whole idea of every code feature release should automatically be subscribed into an
[00:00:24,239 -> 00:00:25,239] A-B test.
[00:00:00,000 -> 00:00:02,540] And the tools should take the work or the burden
[00:00:02,540 -> 00:00:07,099] of analyzing those and giving you back numbers
[00:00:07,099 -> 00:00:09,939] that you can then use to make product decisions.
[00:00:09,939 -> 00:00:12,099] Is that the biggest selling point for StatsIg?
[00:00:12,099 -> 00:00:15,199] Yeah, so the idea behind StatsIg is like,
[00:00:15,199 -> 00:00:17,519] we think that A-B testing is great,
[00:00:17,519 -> 00:00:20,179] but A-B testing is such a time-consuming
[00:00:20,179 -> 00:00:23,320] and manual process that most people don't run
[00:00:23,320 -> 00:00:25,219] as many A-B tests as they should.
[00:00:25,219 -> 00:00:28,379] And so it is important for a tool like StatsIg
[00:00:00,000 -> 00:00:02,899] to come in and say, look, you focus on building features
[00:00:02,899 -> 00:00:04,599] because that's what you're really good at.
[00:00:04,599 -> 00:00:06,400] That's what you should be spending time on.
[00:00:06,400 -> 00:00:09,960] Let the tools take over the idea of, OK,
[00:00:09,960 -> 00:00:12,720] any time we see a rollout.
[00:00:12,720 -> 00:00:14,720] And that generates an opportunity for us
[00:00:14,720 -> 00:00:16,500] to go in and understand, OK, here's
[00:00:16,500 -> 00:00:20,239] a split in an otherwise statistically random sample
[00:00:20,239 -> 00:00:22,800] that we can take the people that are exposed to the feature
[00:00:22,800 -> 00:00:26,239] as your treatment and the people that are not exposed
[00:00:26,239 -> 00:00:28,359] to the feature as control.
[00:00:28,359 -> 00:00:29,600] Let's compare.
[00:00:00,000 -> 00:00:02,600] Let's compare all the metrics, the hundreds of metrics,
[00:00:02,600 -> 00:00:05,519] and then see if there's any statistical differences
[00:00:05,519 -> 00:00:06,719] between those metrics.
[00:00:06,719 -> 00:00:08,640] And that is useful information for you, right?
[00:00:08,640 -> 00:00:12,199] So that gives you, because why would you build a feature
[00:00:12,199 -> 00:00:13,039] in the first place?
[00:00:13,039 -> 00:00:14,580] You would build a feature because you
[00:00:14,580 -> 00:00:17,519] believe that feature is good for your users, customers,
[00:00:17,519 -> 00:00:19,160] or business, or anything.
[00:00:19,160 -> 00:00:20,960] Something about that feature is good.
[00:00:20,960 -> 00:00:22,320] And that's your hypothesis.
[00:00:22,320 -> 00:00:23,320] Let's validate that.
[00:00:23,320 -> 00:00:25,280] So I have two questions.
[00:00:25,280 -> 00:00:27,399] I think both are linked to your market size
[00:00:27,399 -> 00:00:29,679] or your total addressable market.
[00:00:00,000 -> 00:00:07,559] The first question is, how many of your current customers are ex-Facebook, ex-Google, and
[00:00:07,559 -> 00:00:11,679] ex-Uber because they have all used the advertising platform?
[00:00:11,679 -> 00:00:17,359] But I found it is difficult for people that never used this kind of advertising platform
[00:00:17,359 -> 00:00:20,120] to realize the value behind it.
[00:00:20,120 -> 00:00:27,039] Yeah, so this is a really interesting question because whenever we talk to someone new, we
[00:00:00,000 -> 00:00:02,399] we can quickly tell if we're selling,
[00:00:03,399 -> 00:00:05,839] you know, in the realm of selling, right?
[00:00:05,839 -> 00:00:09,560] And are we selling a Tylenol or are we selling a vitamin?
[00:00:09,560 -> 00:00:10,480] I'll explain what that is.
[00:00:10,480 -> 00:00:13,519] Like, you know, Tylenol is a painkiller.
[00:00:13,519 -> 00:00:15,880] So imagine someone comes to you and says like,
[00:00:15,880 -> 00:00:18,239] look, I have a headache and I need a Tylenol.
[00:00:18,239 -> 00:00:21,320] It's easy for you to like, look, here's a Tylenol.
[00:00:21,320 -> 00:00:24,960] It solves for your pain and I can sell it to you.
[00:00:24,960 -> 00:00:26,800] Whereas a vitamin is a little bit different
[00:00:26,800 -> 00:00:29,039] because you have to first convince people
[00:00:00,000 -> 00:00:02,419] that it is important for you to have the vitamin.
[00:00:02,419 -> 00:00:04,160] But the moment you try a vitamin,
[00:00:04,160 -> 00:00:06,280] then you feel the benefit of it
[00:00:06,280 -> 00:00:08,720] and you're never gonna not want it.
[00:00:08,720 -> 00:00:13,720] And so when we talk to the ex-Facebook,
[00:00:14,640 -> 00:00:17,359] ex-Uber, ex-Airbnb, it is very clear.
[00:00:17,359 -> 00:00:21,039] Like they realize the value intuitively
[00:00:21,039 -> 00:00:22,320] of a tool like StatSig
[00:00:22,320 -> 00:00:25,000] and it becomes a much easier conversation.
[00:00:25,000 -> 00:00:27,839] And then when we talk to the other set of folks,
[00:00:27,839 -> 00:00:29,960] there is an element of like for us,
[00:00:00,000 -> 00:00:02,980] We have a lot of awareness to build.
[00:00:02,980 -> 00:00:06,740] We need to be out there, do some content,
[00:00:06,740 -> 00:00:08,560] build some right blog posts,
[00:00:08,560 -> 00:00:13,560] and go to conferences and talk to educate,
[00:00:13,880 -> 00:00:16,620] or to build awareness for a different way
[00:00:16,620 -> 00:00:20,239] of building, measuring, and then using the data
[00:00:20,239 -> 00:00:22,559] to inform your product decisions.
[00:00:22,559 -> 00:00:24,600] And that is a large market,
[00:00:24,600 -> 00:00:29,059] and that we are starting to slowly build up.
[00:00:00,000 -> 00:00:08,160] But for us so far, the success has come from the people that already understand the value
[00:00:08,160 -> 00:00:09,160] of the tool.
[00:00:09,160 -> 00:00:14,000] So, we're super early, we're only like 18 months in, so right now we're kind of like
[00:00:14,000 -> 00:00:19,039] in the, you know, we're happy just serving the Tylenol market.
[00:00:19,039 -> 00:00:22,399] Eventually we want to address the vitamin market.
[00:00:22,399 -> 00:00:28,640] Yeah, I think it's just a very hard problem to solve.
[00:00:00,000 -> 00:00:02,799] people to realize the value of Lightermaze.
[00:00:02,799 -> 00:00:04,960] Yeah, no it is.
[00:00:04,960 -> 00:00:07,080] And there are companies that have done that really, really well.
[00:00:07,080 -> 00:00:10,800] I think, you know, even just feature flagging, right?
[00:00:10,800 -> 00:00:16,280] Some of our competitors have done a pretty great job of convincing people that feature
[00:00:16,280 -> 00:00:19,480] flagging is a great way to build products.
[00:00:19,480 -> 00:00:24,839] It decouples code shipments, code releases from feature releases.
[00:00:24,839 -> 00:00:27,079] You don't have to tie those two things together.
[00:00:27,079 -> 00:00:28,280] That's a very powerful concept.
[00:00:00,000 -> 00:00:05,059] And so the people before us have done a pretty good job of bringing awareness.
[00:00:05,059 -> 00:00:08,980] And now it's on us to build the awareness to the next level, which is like, it's not
[00:00:08,980 -> 00:00:11,980] just enough if you just put features behind a feature flag.
[00:00:11,980 -> 00:00:15,759] It is important for you to understand how each feature is performing.
[00:00:15,759 -> 00:00:20,179] Is it beneficial for your customers, your users, your business?
[00:00:20,179 -> 00:00:22,379] You talk about the competitors.
[00:00:22,379 -> 00:00:27,059] What is the biggest difference between you and other like Optimize and I think there
[00:00:27,059 -> 00:00:29,460] are a couple, a split?
[00:00:00,000 -> 00:00:08,560] Yeah, so like I said, one of the things that I have observed was that whenever people talk
[00:00:08,560 -> 00:00:13,119] about product experimentation, the state of the art is A-B testing.
[00:00:13,119 -> 00:00:15,240] That's where it stops.
[00:00:15,240 -> 00:00:21,440] Whereas what we all learned inside Facebook is like A-B testing is great, but it favors
[00:00:21,440 -> 00:00:24,519] precision over decision.
[00:00:00,000 -> 00:00:06,240] And so what ends up happening is people obsess so much about the precision of the A-B test
[00:00:06,240 -> 00:00:10,800] and in practice what ends up happening is you need to be making a lot more product decisions.
[00:00:10,800 -> 00:00:16,000] And so where Statsit comes in is like, you know, automates the entirety of running an
[00:00:16,000 -> 00:00:23,480] A-B test so much so that we believe that our customers are now running ten times more experiments
[00:00:23,480 -> 00:00:24,480] than they were running before.
[00:00:24,480 -> 00:00:25,480] I see.
[00:00:25,480 -> 00:00:27,679] The efficiency of running experiments.
[00:00:27,679 -> 00:00:28,679] Simplicity even.
[00:00:28,679 -> 00:00:29,679] Yes.
[00:00:00,000 -> 00:00:08,400] So the biggest differentiators are how simple our tool is for you to just get those metrics right away.
[00:00:08,400 -> 00:00:09,400] Two lines of code, right?
[00:00:09,400 -> 00:00:10,400] Well, yeah.
[00:00:10,400 -> 00:00:13,800] And then everything else flows from there.
[00:00:13,800 -> 00:00:21,600] The second part is you don't need to occupy your data science team to constantly be analyzing and making product decisions.
[00:00:21,600 -> 00:00:27,000] The engineers, the product managers, and the designers can make these decisions using StatsIg.
[00:00:00,000 -> 00:00:06,160] And what it does is like, it's pretty important because it's hard enough to find good data scientists.
[00:00:06,160 -> 00:00:08,560] Good data scientists are...
[00:00:08,560 -> 00:00:10,560] See, there's not very many.
[00:00:10,560 -> 00:00:11,919] It's very difficult.
[00:00:11,919 -> 00:00:17,120] And then what happens is most of the companies that we talk to hire these great data scientists
[00:00:17,120 -> 00:00:18,879] and then what do they put them on?
[00:00:18,879 -> 00:00:22,559] They put them on, go analyze this A-B test.
[00:00:22,559 -> 00:00:28,879] And what ends up happening is these teams of data scientists are looking backwards
[00:00:00,000 -> 00:00:07,879] and analyzing and diagnosing and running queries on an experiment and then making like, okay,
[00:00:07,879 -> 00:00:10,480] here's a report and make your product decision.
[00:00:10,480 -> 00:00:14,480] Instead, those people, I mean, obviously, I don't think people enjoy doing that.
[00:00:14,480 -> 00:00:15,960] They should be looking forward.
[00:00:15,960 -> 00:00:20,960] They should be like analyzing, okay, what should the product be going forward into versus
[00:00:20,960 -> 00:00:22,640] like looking backwards.
[00:00:22,640 -> 00:00:28,800] And so, what StatsIQ does is also relieves these data scientists from the grunt work
[00:00:00,000 -> 00:00:04,000] and lets them do creative work, which is what everybody wants to do.
[00:00:04,000 -> 00:00:07,000] Yeah, as a data scientist, I really thank you for...
[00:00:07,000 -> 00:00:09,000] Yeah, yeah, I think it's important.
[00:00:09,000 -> 00:00:11,000] We see so many of our customers like,
[00:00:11,000 -> 00:00:14,000] you have such an amazing data science team, but what are they doing?
[00:00:14,000 -> 00:00:16,000] They're doing grunt work.
[00:00:16,000 -> 00:00:18,000] Yeah, I think this is a very good angle,
[00:00:18,000 -> 00:00:21,000] like looking backwards versus looking forwards.
[00:00:21,000 -> 00:00:26,000] I think all the great data scientists want to help shape the future of the company.
[00:00:26,000 -> 00:00:29,000] Shape the future, strategy, what the product should evolve into,
[00:00:00,000 -> 00:00:02,000] Not what the product was already done.
[00:00:02,000 -> 00:00:04,000] Better decisions.
[00:00:04,000 -> 00:00:08,000] Looking at the past is only taking information for the future.
[00:00:08,000 -> 00:00:09,000] Correct.
[00:00:09,000 -> 00:00:13,000] The second question is also about total addressable market.
[00:00:13,000 -> 00:00:19,000] We know at Facebook it's very easy to do A-B testing because of the traffic size.
[00:00:19,000 -> 00:00:23,000] You have billions of people using the app.
[00:00:23,000 -> 00:00:27,000] But it's not the case for most companies.
[00:00:00,000 -> 00:00:07,480] Yeah, this is a common myth. The myth is that you need to have large sample sizes like Facebook
[00:00:07,480 -> 00:00:14,359] in order to run experiments. And I think that's not true at all. See, if you're looking for
[00:00:14,359 -> 00:00:21,780] 0.01% improvements in your MAU or DAU metric or your revenue metric, then you do need millions
[00:00:21,780 -> 00:00:27,679] of billions of samples. Obviously, you know this better than anyone else, which is in
[00:00:00,000 -> 00:00:09,439] In order to get sample size or the statistical power, you have two factors going.
[00:00:09,439 -> 00:00:16,500] There's the minimum detectable effect and then the baseline conversion rate as well
[00:00:16,500 -> 00:00:18,280] as the sample size.
[00:00:18,280 -> 00:00:24,000] It turns out that the minimum detectable effect has a much higher bearing on your statistical
[00:00:24,000 -> 00:00:25,000] power.
[00:00:00,000 -> 00:00:06,360] So when small companies with only a few thousands of samples, what they're looking for is 10%
[00:00:06,360 -> 00:00:08,039] wins, 20% wins.
[00:00:08,039 -> 00:00:09,039] They're not looking for.01%.
[00:00:09,039 -> 00:00:13,699] If you're a small company with only a thousand samples, if you're looking for.1%, I would
[00:00:13,699 -> 00:00:15,599] say you should stop.
[00:00:15,599 -> 00:00:18,199] You should go look for larger wins.
[00:00:18,199 -> 00:00:22,120] And so when you're looking for 10, 20% wins, you don't need millions of samples.
[00:00:22,120 -> 00:00:23,260] That's true.
[00:00:23,260 -> 00:00:28,679] And so we have to bust this myth because this comes up a lot of times.
[00:00:00,000 -> 00:00:02,600] 39% of our customers are our B2B companies.
[00:00:02,600 -> 00:00:04,320] And B2B companies, they don't have,
[00:00:04,320 -> 00:00:06,280] when they generally talk to us, they're like,
[00:00:06,280 -> 00:00:10,279] oh, we don't have that many samples.
[00:00:10,279 -> 00:00:11,720] And how do we run experiments?
[00:00:11,720 -> 00:00:13,359] You can definitely run experiments.
[00:00:13,359 -> 00:00:15,320] And these are all still valid.
[00:00:15,320 -> 00:00:17,960] I would still say, look, you should still
[00:00:17,960 -> 00:00:20,800] put every feature behind the feature flag
[00:00:20,800 -> 00:00:23,600] and then still validate the impact of those features.
[00:00:23,600 -> 00:00:25,600] Because sometimes what happens is
[00:00:25,600 -> 00:00:27,800] you introduce a bug without really noticing.
[00:00:00,000 -> 00:00:03,560] And those bugs have this massive MDE.
[00:00:03,560 -> 00:00:08,800] And those MDEs will manifest itself as a change
[00:00:08,800 -> 00:00:11,359] with statistical significance instantly.
[00:00:11,359 -> 00:00:13,519] You don't have to wait for two weeks.
[00:00:13,519 -> 00:00:15,279] You'll get back that right away.
[00:00:15,279 -> 00:00:17,879] And you should be looking at that and capturing those,
[00:00:17,879 -> 00:00:20,160] and then fixing the bugs, and not
[00:00:20,160 -> 00:00:23,760] wait until your experiment is done to look at those.
[00:00:23,760 -> 00:00:27,079] So I generally say it would be beneficial
[00:00:27,079 -> 00:00:29,280] if the entire data science community comes together
[00:00:00,000 -> 00:00:04,540] It actually busts this myth that you need to have lots of samples.
[00:00:04,540 -> 00:00:10,419] I remember on Facebook I always tell the engineers, don't look at the funnel metrics, rather than
[00:00:10,419 -> 00:00:11,419] at MAGs.
[00:00:11,419 -> 00:00:15,220] At first, look at adoption at different layers.
[00:00:15,220 -> 00:00:17,899] And that often tells you more information.
[00:00:17,899 -> 00:00:19,620] Oh, absolutely.
[00:00:19,620 -> 00:00:23,219] That's another good technique where you look at the top of the funnel, and the correlating
[00:00:00,000 -> 00:00:07,080] metrics, things that have less inertia, things that can move right away.
[00:00:07,080 -> 00:00:12,199] It's also a pretty good way to pick the right metric for how you measure the success of
[00:00:12,199 -> 00:00:13,939] your own product.
[00:00:13,939 -> 00:00:18,440] Sometimes if you pick too deep of a funnel metric, say if you pick the bottom of the
[00:00:18,440 -> 00:00:22,960] funnel metric, it's very hard to move those metrics and you don't know if the work you're
[00:00:22,960 -> 00:00:24,960] doing is actually impacting.
[00:00:24,960 -> 00:00:29,539] Sometimes people pick like 30 day average metrics and then those metrics have inertia
[00:00:00,000 -> 00:00:04,200] then what happens is like, you know, whatever changes you're making don't immediately manifest.
[00:00:04,200 -> 00:00:06,280] You have to wait a long time for it to see.
[00:00:06,839 -> 00:00:10,679] So my general philosophy is like, you know, pick top of the funnel metrics,
[00:00:11,119 -> 00:00:13,919] generally the ones that are correlated to the ones that you want to move.
[00:00:14,279 -> 00:00:16,440] And if they're moving in the right direction, then that's a good sign.
