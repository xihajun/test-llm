[00:00:00,000 -> 00:00:03,919] 拆GPT的出現對我來說無疑是狗突然會說人話
[00:00:03,919 -> 00:00:05,799] 而且這件事情是Nubian已經做到的
[00:00:05,799 -> 00:00:07,280] 拆GPT只是沒有把它開放出來
[00:00:07,280 -> 00:00:10,039] 如果你的算力和你的信息明顯會不如拆GPT
[00:00:10,039 -> 00:00:11,800] 你的態度然後你的ego
[00:00:11,800 -> 00:00:16,000] 然後你的這個心地你的理性都不如拆GPT
[00:00:16,000 -> 00:00:18,920] 你的判斷力和你的思維能力再不如拆GPT的話
[00:00:18,920 -> 00:00:19,920] 那你還剩下什麼呢
[00:00:19,920 -> 00:00:21,519] 大家好歡迎回到課堂筆電政
[00:00:21,519 -> 00:00:23,359] 今天我們聊拆GPT
[00:00:23,359 -> 00:00:26,039] 已經有很多小夥伴忽悠我去聊下GPT了
[00:00:26,039 -> 00:00:27,960] 其實我寫了一個文檔
[00:00:27,960 -> 00:00:29,600] 這個文檔我在一直更新
[00:00:00,000 -> 00:00:02,399] 现在我上次看的时候已经有18000次了
[00:00:02,399 -> 00:00:05,200] 把我对拆机配器的各种各样的理解
[00:00:05,440 -> 00:00:07,000] 尤其是对技术的理解
[00:00:07,000 -> 00:00:09,000] 它到底是什么能颠覆什么
[00:00:09,160 -> 00:00:10,199] 怎么应用等等
[00:00:10,199 -> 00:00:11,960] 全都写在了那个文档里边
[00:00:12,119 -> 00:00:14,320] 我在这里把这个文档的RM放在这
[00:00:14,320 -> 00:00:16,600] 欢迎大家去看也欢迎大家去收藏
[00:00:16,879 -> 00:00:19,280] 那个文档我会提出非常强烈的观点
[00:00:19,280 -> 00:00:20,800] 我不会说那些正确的废话
[00:00:20,800 -> 00:00:22,839] 我其实说的是非常强烈的观点
[00:00:22,839 -> 00:00:24,640] 它到底有什么不一样的能力
[00:00:24,920 -> 00:00:26,879] 和之前的人工智能有什么不一样
[00:00:27,160 -> 00:00:28,399] 它到底能颠覆什么
[00:00:00,000 -> 00:00:01,720] 为什么能颠覆这些东西
[00:00:01,760 -> 00:00:03,200] 国产大模型到底有没有前途
[00:00:03,200 -> 00:00:05,320] 我也会是有一个非常明确的结论
[00:00:05,559 -> 00:00:08,320] 但是正因为有这些非常强烈的观点
[00:00:08,359 -> 00:00:10,560] 我非常清楚这些观点的来源
[00:00:10,599 -> 00:00:13,720] 所以说当我遇到了一个我没有见过的东西
[00:00:13,720 -> 00:00:15,320] 或者说当我知道我中间的一个
[00:00:15,320 -> 00:00:16,679] factor被改变的时候
[00:00:16,719 -> 00:00:19,039] 这些观点都很有可能是变化的
[00:00:19,199 -> 00:00:21,960] 这就叫所谓的strong opinions weakly held
[00:00:22,000 -> 00:00:24,600] 总之那个是我推荐大家去看的
[00:00:24,839 -> 00:00:27,719] 我觉得聊拆这边是一个很长的话题
[00:00:27,760 -> 00:00:29,960] 所以说我们今天就简单上一下结论
[00:00:00,000 -> 00:00:02,399] 如果你是一个结论党的话到这就可以结束了
[00:00:02,399 -> 00:00:05,400] 是我聊完这些结论以后会马上去讲一下
[00:00:05,400 -> 00:00:08,699] 为什么做一个结论党对于自己的认知是没有帮助的
[00:00:08,699 -> 00:00:10,699] 那好我们直接就上结论吧
[00:00:10,699 -> 00:00:15,000] 第一,Chai GPT和过去的Machine Learning是有一个极大的范式不同的
[00:00:15,000 -> 00:00:17,800] 过去的Machine Learning只能寻找对应关系
[00:00:17,800 -> 00:00:19,899] 而Chai GPT能理解
[00:00:19,899 -> 00:00:21,699] 如果用一个例子的话
[00:00:21,699 -> 00:00:27,199] 是2017年朱松纯教授讲人工智能与智能的关系的时候
[00:00:00,000 -> 00:00:03,080] 提出來的一個鸚鵡和烏鴉的例子
[00:00:03,080 -> 00:00:05,919] 鸚鵡只會模仿人說話
[00:00:05,919 -> 00:00:07,919] 但是它其實是在模仿這些話
[00:00:07,919 -> 00:00:10,080] 它根本就不知道這些話什麼意思
[00:00:10,080 -> 00:00:13,599] 但是烏鴉是對這個世界有理解有思考的
[00:00:13,599 -> 00:00:15,800] 這其實是一個很神奇的比喻
[00:00:15,800 -> 00:00:18,440] 而且在那篇五年前的文章裡邊
[00:00:18,440 -> 00:00:20,359] 現在可能快六年前的文章裡邊
[00:00:20,359 -> 00:00:21,719] 朱松成教授就說
[00:00:21,719 -> 00:00:23,600] 我們所有的人工智能學者
[00:00:23,600 -> 00:00:26,879] 應該以烏鴉為圖騰去尋找烏鴉的智能
[00:00:26,879 -> 00:00:28,440] 今天Chachapiti出來了
[00:00:00,000 -> 00:00:01,919] 它就是具备了无压的智能
[00:00:01,919 -> 00:00:03,799] 这在我看来是非常非常颠覆
[00:00:03,799 -> 00:00:05,240] 和非常非常不可想象的
[00:00:05,240 -> 00:00:07,960] 因为我之前的工作跟Machine Learning是太相关了
[00:00:07,960 -> 00:00:10,199] 我自己的频道还有那么多视频去讲Machine Learning
[00:00:10,199 -> 00:00:13,119] 尤其是讲Machine Learning在这里边的Limitation
[00:00:13,119 -> 00:00:16,000] 因为它只能做寻找对应关系这件事
[00:00:16,000 -> 00:00:16,879] 所以
[00:00:16,879 -> 00:00:19,399] XGBT的出现对我来说无疑是
[00:00:19,399 -> 00:00:21,320] 狗突然会说人话了
[00:00:21,320 -> 00:00:22,719] 就是我真的我看到XGBT
[00:00:22,719 -> 00:00:24,519] 我就觉得人工智能有理解能力
[00:00:24,519 -> 00:00:26,519] 就好像狗会说人话一样
[00:00:26,519 -> 00:00:28,480] 但是我觉得我们的人类就是这样子
[00:00:00,000 -> 00:00:01,600] 今天听到的狗说人话
[00:00:01,600 -> 00:00:03,520] 可能三天后也觉得这是一件
[00:00:03,520 -> 00:00:04,519] 习以为常的事情了
[00:00:04,519 -> 00:00:05,519] 不就是
[00:00:05,519 -> 00:00:07,320] 这是一个非常非常颠覆的事情
[00:00:07,320 -> 00:00:10,279] 我回头会专门出文章去讲这些
[00:00:10,279 -> 00:00:11,640] 我今天就只把结论放上来
[00:00:11,640 -> 00:00:12,679] 大家不用着急
[00:00:12,679 -> 00:00:14,759] 当然了最好是看我的那个文章
[00:00:14,759 -> 00:00:15,359] 好
[00:00:16,039 -> 00:00:17,120] 第二就是Chachabitty
[00:00:17,120 -> 00:00:18,559] 它会颠覆什么样的工作
[00:00:18,559 -> 00:00:20,640] 它会颠覆所有搬砖类的工作
[00:00:20,640 -> 00:00:22,760] 除了真正的搬砖它不能颠覆
[00:00:22,760 -> 00:00:24,879] 但是如果说你是一个在电脑前面
[00:00:24,879 -> 00:00:26,559] 你形容你的工作是搬砖
[00:00:26,559 -> 00:00:29,239] 那Chachabitty大概率是会取代你的工作的
[00:00:00,000 -> 00:00:02,799] 取代这样的搬砖类的工作可能有一个好处
[00:00:02,799 -> 00:00:05,400] 就是它可以极大地减少内卷
[00:00:05,400 -> 00:00:06,200] 什么意思呢
[00:00:06,200 -> 00:00:08,320] 因为你搬砖这个东西就很明显
[00:00:08,320 -> 00:00:09,679] 很容易被其他人取代
[00:00:09,679 -> 00:00:12,560] 那大家在搬砖类的工作上就只能内卷
[00:00:12,560 -> 00:00:13,560] 只能拼996
[00:00:13,560 -> 00:00:14,560] 那你现在拼不了
[00:00:14,560 -> 00:00:16,640] 拼996肯定是拼不过拆GPT的
[00:00:16,640 -> 00:00:18,839] 那大家会努力的去思考
[00:00:18,839 -> 00:00:20,320] 如果我不内卷还可以干嘛
[00:00:20,320 -> 00:00:23,000] 所以说它取代搬砖减少内卷
[00:00:23,000 -> 00:00:26,199] 这是它会对人类工作的颠覆
[00:00:26,199 -> 00:00:28,160] 就是它不是颠覆一类工作
[00:00:28,160 -> 00:00:29,239] 只要你是搬砖类
[00:00:00,000 -> 00:00:01,320] 它几乎都可以颠覆的
[00:00:01,320 -> 00:00:04,320] 第三个是这个讲到了刚才
[00:00:04,320 -> 00:00:05,280] Touch TV的颠覆
[00:00:05,280 -> 00:00:07,160] 接下来我们就讲它怎么颠覆
[00:00:07,160 -> 00:00:10,199] 或者说OpenAI开放了什么是最重要的
[00:00:10,199 -> 00:00:13,720] OpenAI很明显是对这里边非常谨慎小心
[00:00:13,720 -> 00:00:16,679] 它现在不开放很重要的能力
[00:00:16,679 -> 00:00:18,600] 在我看来不是因为它不能做
[00:00:18,600 -> 00:00:20,320] 他们的工程能力是非常强的
[00:00:20,320 -> 00:00:21,719] 它想做到这样的开放
[00:00:21,719 -> 00:00:23,199] 可能一晚上就可以做到了
[00:00:23,199 -> 00:00:25,839] 当然它是担心这样的开放
[00:00:25,839 -> 00:00:28,600] 对人类社会所造成的冲击而没有开放
[00:00:00,000 -> 00:00:01,560] 但是迟早也会开放的
[00:00:01,840 -> 00:00:04,120] 它开放的就是prompting的长度
[00:00:04,519 -> 00:00:06,839] 尤其是记忆你prompting的长度
[00:00:06,960 -> 00:00:09,599] 以及它调用其他工具的接口
[00:00:09,720 -> 00:00:10,320] 什么意思呢
[00:00:10,320 -> 00:00:12,759] 大家其实要知道就是
[00:00:12,759 -> 00:00:15,000] New Beam它并不是什么
[00:00:15,000 -> 00:00:17,679] 拆GPT背后的模型去有什么更改
[00:00:17,679 -> 00:00:19,719] 而是GPT 3.5放在那儿
[00:00:19,719 -> 00:00:21,079] 模型参数从来不变
[00:00:21,120 -> 00:00:22,800] 你的这些东西也从来没有进到
[00:00:22,800 -> 00:00:24,839] GPT 3.5的这个参数里面
[00:00:25,000 -> 00:00:27,239] 然后我们在这拆GPT的基础之上
[00:00:27,239 -> 00:00:29,160] 给它搜索的能力
[00:00:00,000 -> 00:00:02,480] 然后再给了他一堆instruction
[00:00:02,480 -> 00:00:04,679] 给了他一堆instruct prompting
[00:00:04,679 -> 00:00:06,599] 去教他怎么去使用搜索
[00:00:06,599 -> 00:00:08,439] 然后你的搜索什么是好的搜索
[00:00:08,439 -> 00:00:10,400] 什么是不好的搜索的情况下
[00:00:10,400 -> 00:00:12,599] ChatGPT学会了使用搜索
[00:00:12,599 -> 00:00:13,839] 然后就变成了NeoBing
[00:00:13,839 -> 00:00:15,199] 大家应该有感觉到这种关系
[00:00:15,199 -> 00:00:16,800] 所以说他只要把这块
[00:00:16,800 -> 00:00:19,480] NeoBing和ChatGPT之间的这些东西开放出来
[00:00:19,480 -> 00:00:20,960] 我可以去教他一堆话
[00:00:20,960 -> 00:00:21,839] 让他记住
[00:00:21,839 -> 00:00:23,879] 并且让他可以调用数据
[00:00:23,879 -> 00:00:25,039] 或者调用搜索
[00:00:25,039 -> 00:00:26,160] 调用其他的能力
[00:00:26,160 -> 00:00:28,879] 尤其是使用自然语言的方式去调用的情况下
[00:00:00,000 -> 00:00:04,160] 它就已经可以100%的颠覆很多很多的东西了
[00:00:04,160 -> 00:00:05,200] 我举一个例子吧
[00:00:05,200 -> 00:00:08,160] 假设你是一个非常厉害的程序员
[00:00:08,160 -> 00:00:10,960] 你现在看XGBT编出来的东西还有很多问题
[00:00:10,960 -> 00:00:13,759] 但是已经有一些地方可以用了
[00:00:13,759 -> 00:00:17,559] 这个时候你给XGBT开放了所有的代码
[00:00:17,559 -> 00:00:19,679] 让他可以去搜索这里的所有代码
[00:00:19,679 -> 00:00:23,039] 然后你让XGBT能记住你一万句话
[00:00:23,039 -> 00:00:25,039] 写着一万句话的全中可调
[00:00:25,039 -> 00:00:28,399] 你可以去教XGBT一万句话
[00:00:00,000 -> 00:00:02,839] 那你想想这一万句话交出来的这个XGBT
[00:00:02,839 -> 00:00:04,480] 它的编程能力会有多强
[00:00:04,480 -> 00:00:07,120] 你想想你用一万句话如果去教一个人的话
[00:00:07,120 -> 00:00:08,679] 那个人会变成什么样子
[00:00:08,679 -> 00:00:11,720] XGBT又有理性又有理解
[00:00:11,720 -> 00:00:15,679] 然后又有这个强大的算力和无尽的知识
[00:00:15,679 -> 00:00:17,480] 然后他去学真的去听
[00:00:17,480 -> 00:00:19,280] 你只要教他什么他就学什么
[00:00:19,280 -> 00:00:22,079] 这一万句话之后他会变成一个多么强大的程序员
[00:00:22,079 -> 00:00:24,160] 而且这件事情是NewBin已经做到的
[00:00:24,160 -> 00:00:25,920] XGBT只是没有把它开放出来而已
[00:00:25,920 -> 00:00:28,440] 讲完它的开放我们就要回归它的本质
[00:00:28,440 -> 00:00:29,960] 它到底是一个什么东西
[00:00:00,000 -> 00:00:06,160] ChatGPT是一个人类调用算力和数据接近最完美的形态
[00:00:06,160 -> 00:00:07,160] 我再重复一遍
[00:00:07,160 -> 00:00:12,919] ChatGPT是人类调用算力和数据接近最完美的形态
[00:00:12,919 -> 00:00:14,480] 其实这个观点一点都不新鲜
[00:00:14,480 -> 00:00:17,920] 就是很多人说ChatGPT是自然语言编程机
[00:00:17,920 -> 00:00:19,600] 或者自然语言计算机
[00:00:19,600 -> 00:00:22,600] 我们的计算机发展史其实是算力在进步
[00:00:22,600 -> 00:00:24,839] 数据在增加和进步
[00:00:24,839 -> 00:00:25,839] 就不断的被生成
[00:00:25,839 -> 00:00:28,280] 比如说你在抖音上刷视频的数据
[00:00:28,280 -> 00:00:29,480] 视频本身是数据
[00:00:00,000 -> 00:00:01,720] 你刷的行为也是数据
[00:00:01,720 -> 00:00:03,200] 我们积累了很多数据以后
[00:00:03,200 -> 00:00:05,200] 就对这个世界了解的越来越多
[00:00:05,200 -> 00:00:06,040] 算力越来越多
[00:00:06,040 -> 00:00:08,279] 我们就可以去更好的去利用这些
[00:00:08,279 -> 00:00:09,800] 更好的去算很多东西
[00:00:10,480 -> 00:00:11,480] 之前的编程
[00:00:11,480 -> 00:00:12,800] 零和一机器
[00:00:12,800 -> 00:00:14,439] 会编这些东西我都不说了
[00:00:14,439 -> 00:00:15,480] 到了高级语言
[00:00:15,480 -> 00:00:16,800] 但是其实在网上还有
[00:00:16,800 -> 00:00:19,160] 比如说虚拟机是对硬件的抽象
[00:00:19,160 -> 00:00:21,640] 云服务是对整个Infra的抽象
[00:00:21,640 -> 00:00:24,559] API又是对背后的Infra
[00:00:24,559 -> 00:00:27,839] 加上很多Computing和存储的抽象
[00:00:27,839 -> 00:00:28,960] 其实我们用API
[00:00:00,000 -> 00:00:01,080] 已经可以解决很多
[00:00:01,080 -> 00:00:03,960] 尤其 serverless 的 API 已经可以解决很多东西了
[00:00:03,960 -> 00:00:08,839] 那接下来 XGBT 也就是对所有的计算的过程
[00:00:08,839 -> 00:00:11,199] 调用 IT 的这个过程的终极抽象
[00:00:11,199 -> 00:00:13,160] 那你就跟 XGBT 经常
[00:00:13,160 -> 00:00:14,759] 你现在还有很多步骤
[00:00:14,759 -> 00:00:17,000] 你跟他说我要写一段这个
[00:00:17,000 -> 00:00:18,760] 你告诉我那个公式是什么样子
[00:00:18,760 -> 00:00:20,839] 他已经可以很好的帮助你这一点了
[00:00:20,839 -> 00:00:21,719] 但是到最后
[00:00:21,719 -> 00:00:23,879] 如果 XGBT 真的去理解这个系统
[00:00:23,879 -> 00:00:24,879] 理解这些数据库
[00:00:24,879 -> 00:00:26,559] 你就可以跟他直接要一个结果
[00:00:26,559 -> 00:00:27,800] 他可以给你一个结果
[00:00:00,000 -> 00:00:04,200] 像我们所谓很让我们难受的这些 SQL Market 取数的工作
[00:00:04,200 -> 00:00:07,000] Chai GPT 真的可以很好的就完成它
[00:00:07,000 -> 00:00:13,199] 所以说记住 Chai GPT 是人类调用算力和存储的最终极形态
[00:00:13,199 -> 00:00:15,439] 最后一个问题就是 Chai GPT 有多难模仿
[00:00:15,439 -> 00:00:17,399] 其实这个问题是两个问题
[00:00:17,399 -> 00:00:19,399] 我觉得 Chai GPT 我们一定要理解
[00:00:19,399 -> 00:00:23,000] GPT 3.5 是拥有理解能力的大语言模型
[00:00:23,000 -> 00:00:25,000] 它是最难模仿的
[00:00:25,000 -> 00:00:29,480] 它具有的理解能力是我们现在很多人都不知道怎么发生
[00:00:00,000 -> 00:00:01,800] 但是它就发生了的东西
[00:00:01,800 -> 00:00:03,279] 文章里有一些猜想
[00:00:03,279 -> 00:00:05,360] 这里就不展开了
[00:00:05,360 -> 00:00:07,280] 接下来比较难的东西是什么
[00:00:07,280 -> 00:00:08,919] 是把这个大模型
[00:00:08,919 -> 00:00:11,160] 乔教成人类喜欢的模式
[00:00:11,160 -> 00:00:12,400] 那就是他做的
[00:00:12,400 -> 00:00:14,480] 这个reinforced learning with human feedback
[00:00:14,480 -> 00:00:17,079] RLHF的这件事情
[00:00:17,079 -> 00:00:18,719] 当然就是这个instructor范式
[00:00:18,719 -> 00:00:19,839] 然后几步对吧
[00:00:19,839 -> 00:00:20,879] 到了Chai GPT
[00:00:20,879 -> 00:00:23,920] Chai GPT这个对话形式是很容易实现的
[00:00:23,920 -> 00:00:25,679] 所以说我们可以看到
[00:00:25,679 -> 00:00:27,440] 市面上已经有很多人
[00:00:27,440 -> 00:00:29,199] 快速的可以用一个
[00:00:00,000 -> 00:00:02,359] 不管是开源的还是自研的还是怎样的
[00:00:02,359 -> 00:00:04,440] 这个大宇元模型做一个生成式的任务
[00:00:04,440 -> 00:00:06,240] 套上这个对话机器人的形式
[00:00:06,240 -> 00:00:08,039] 就可以去跟你进行对话
[00:00:08,039 -> 00:00:10,960] 当然大家就觉得对出来这个话非常奇怪
[00:00:10,960 -> 00:00:12,679] 它背后是没有压的能力的
[00:00:12,679 -> 00:00:15,519] 我在未来的视频也会展开
[00:00:15,519 -> 00:00:18,519] 就是如何去判定一个跟你对话的机器人
[00:00:18,519 -> 00:00:19,960] 到底有没有无压的能力
[00:00:19,960 -> 00:00:21,719] 也会是一个非常重要的事情
[00:00:22,160 -> 00:00:24,960] 我今天的录制时间是3月12号
[00:00:24,960 -> 00:00:27,239] 我争取在3月16号文心盈演出来之前
[00:00:27,239 -> 00:00:27,960] 把这个发出来
[00:00:00,000 -> 00:00:01,919] 就是文心言我 fully trust
[00:00:01,919 -> 00:00:03,520] 他可以做到一个对话
[00:00:03,520 -> 00:00:05,440] 然后他这个对话也会让你觉得
[00:00:05,440 -> 00:00:06,599] 有的地方 make sense
[00:00:06,599 -> 00:00:07,799] 有的地方不 make sense
[00:00:07,960 -> 00:00:11,599] 但是其实就是一个真正的智者和一个智障
[00:00:11,679 -> 00:00:15,000] 在一开始几句话是很难发现区别的
[00:00:15,039 -> 00:00:18,519] 可是背后的智能是很难很难做出来的
[00:00:18,760 -> 00:00:20,559] 我不是说文心言是智障
[00:00:20,559 -> 00:00:21,160] 一定是智障
[00:00:21,160 -> 00:00:24,039] 但是我觉得他很有可能是一个智障
[00:00:24,039 -> 00:00:26,120] 但是模仿出来可以对话的样子
[00:00:26,199 -> 00:00:28,679] 好的我们回来继续讲这里边的复刻
[00:00:00,000 -> 00:00:03,000] 第一就是有可能已经能复刻出来了
[00:00:03,000 -> 00:00:05,360] 其他大型模型也具备理解能力了
[00:00:05,360 -> 00:00:06,480] 这个就没得可说
[00:00:06,480 -> 00:00:07,559] 第二种可能性
[00:00:07,559 -> 00:00:09,839] 而且我觉得它是我默认的可能性
[00:00:09,839 -> 00:00:12,199] 它非常可能就是它很难复刻
[00:00:12,199 -> 00:00:15,800] 它是有一个强工程能力门槛的事情
[00:00:15,800 -> 00:00:16,800] 举一个例子
[00:00:16,800 -> 00:00:18,600] 比如说我们国家的材料科学
[00:00:18,600 -> 00:00:22,280] 芯片和飞机引擎都是受限于材料科学
[00:00:22,280 -> 00:00:24,960] 那为什么材料科学这件事情就很难模仿
[00:00:24,960 -> 00:00:26,920] 因为它是一个很吃工程能力的
[00:00:26,920 -> 00:00:29,199] 我之前一年多前采访
[00:00:00,000 -> 00:00:02,799] 在Intel工作的芯片工程师
[00:00:03,080 -> 00:00:06,799] 他的工作就是要从结晶的晶体上
[00:00:06,799 -> 00:00:08,439] 去做下一代的芯片
[00:00:08,599 -> 00:00:10,919] 但是结晶的这个晶体
[00:00:10,919 -> 00:00:13,039] 你这一步差了0.1%的杂质
[00:00:13,160 -> 00:00:15,720] 那一步可能就差10% 20%的产出
[00:00:15,960 -> 00:00:18,160] 这么多步骤结合到一起的时候
[00:00:18,280 -> 00:00:19,960] 你最后可能你的良品率
[00:00:19,960 -> 00:00:22,280] 就是它的10% 1%
[00:00:22,399 -> 00:00:25,600] 那你的成本就完全不可接受
[00:00:25,600 -> 00:00:25,879] 对吧
[00:00:25,879 -> 00:00:29,399] 所以说工程能力是一个很重要的东西
[00:00:00,000 -> 00:00:01,399] 且很难被复制
[00:00:01,399 -> 00:00:02,399] 不幸的是
[00:00:02,399 -> 00:00:07,200] OpenAI就是有一个强工程能力壁垒的这样的一个机构
[00:00:07,200 -> 00:00:09,000] 它里边的人才密度是非常高的
[00:00:09,000 -> 00:00:10,199] 在碰撞的过程中
[00:00:10,199 -> 00:00:12,599] 他们解决了很多很多的工程瓶颈
[00:00:12,599 -> 00:00:16,000] 而且大语言模型本来就是一个非常吃工程能力的东西
[00:00:16,000 -> 00:00:18,199] 我们现在有无数的人去讨论
[00:00:18,199 -> 00:00:19,600] 怎么怎么样子去用NLP
[00:00:19,600 -> 00:00:21,000] 怎么怎么样子用大语言模型
[00:00:21,000 -> 00:00:22,000] 这里边的原理是什么
[00:00:22,000 -> 00:00:23,000] 我们应该怎么做出来
[00:00:23,000 -> 00:00:27,600] 但是有几个人真的能像OpenAI做好文本清理那一步
[00:00:00,000 -> 00:00:03,600] 大家真的能把真正的世界的文本
[00:00:03,600 -> 00:00:06,080] 很好的清理到一个数据集
[00:00:06,080 -> 00:00:07,080] 去喂给大陆人模型
[00:00:07,080 -> 00:00:08,800] 我觉得真正能做好这一步的人
[00:00:08,800 -> 00:00:09,919] 反而是非常非常少的
[00:00:09,919 -> 00:00:11,279] 以及我没有听到谁
[00:00:11,279 -> 00:00:12,919] 很认真地去讨论这一步
[00:00:12,919 -> 00:00:15,000] 如果大家真的很认真的要去复刻
[00:00:15,000 -> 00:00:15,800] Chats with Tea
[00:00:15,800 -> 00:00:17,199] 起码先把第一步做好吧
[00:00:17,879 -> 00:00:19,519] 那在这种情况下
[00:00:19,519 -> 00:00:22,199] 叠加leadership是缺乏技术判断力的
[00:00:22,199 -> 00:00:23,920] 叠加我们的工程师
[00:00:23,920 -> 00:00:26,199] 尤其有原理性思考的工程师的人才密度
[00:00:26,199 -> 00:00:27,039] 是不够的
[00:00:27,039 -> 00:00:29,480] 再叠加我们的商业社会
[00:00:00,000 -> 00:00:04,559] 或者说我们这样一个很复杂的环境和我们的人的思考模式
[00:00:04,559 -> 00:00:06,080] 大家一定要认真严肃对待
[00:00:06,080 -> 00:00:08,199] Chai GPT是很难复刻的这一现实
[00:00:08,560 -> 00:00:10,199] 如果观众里有一些不懂的同学
[00:00:10,199 -> 00:00:13,839] 他们会说过去的ML我们都复制的不是很好吗
[00:00:13,839 -> 00:00:15,320] 中国的AI不是很领先吗
[00:00:15,359 -> 00:00:17,000] 其实不是这样子的
[00:00:17,000 -> 00:00:19,839] 过去的AI的绝大多数的日本人都是开源的
[00:00:19,839 -> 00:00:21,000] 模型都是开源的
[00:00:21,000 -> 00:00:24,679] 因为它的真正的竞争力在数据和对商业的结合上
[00:00:24,839 -> 00:00:27,039] 但是Chai GPT再说一遍
[00:00:27,039 -> 00:00:29,239] 它是一个强工程门槛的东西
[00:00:00,000 -> 00:00:02,759] 它的那个训练它的模型是必然的
[00:00:02,759 -> 00:00:05,440] 它的那个训练也是一个很难很难的
[00:00:05,440 -> 00:00:07,679] 它也不会开源给你死雷小新
[00:00:07,679 -> 00:00:11,080] 它哪怕把CHPT的这个原理全都给你了
[00:00:11,080 -> 00:00:13,720] 你要想拿着CHPT去重新复刻一遍
[00:00:13,720 -> 00:00:16,160] 就像我给你一个材料的recipe
[00:00:16,160 -> 00:00:18,079] 我们每个人都能买到飞机引擎
[00:00:18,079 -> 00:00:19,199] 不是每个人都能买到吧
[00:00:19,199 -> 00:00:20,679] 但是我们能买到飞机引擎
[00:00:20,679 -> 00:00:24,960] 我们知道这个东西的原理和它的成型的样子
[00:00:24,960 -> 00:00:26,800] 但是你没有办法做出来
[00:00:26,800 -> 00:00:28,280] 大概就是这样一件事情
[00:00:00,000 -> 00:00:02,680] 好的以上就是我的几个主要结论
[00:00:02,960 -> 00:00:04,480] 但是你知道这些结论
[00:00:04,480 -> 00:00:06,080] 不知道这些结论是怎么来的
[00:00:06,080 -> 00:00:06,960] 意义其实不大
[00:00:06,960 -> 00:00:09,199] 因为你换到另外一个人说另外一个结论
[00:00:09,199 -> 00:00:10,480] 你就没有办法比较了
[00:00:10,519 -> 00:00:12,160] 你只会说这个人咖啡大
[00:00:12,160 -> 00:00:13,759] 那个人咖啡大或者怎么样
[00:00:13,759 -> 00:00:16,079] 这个人听着和我的想法比较一致
[00:00:16,120 -> 00:00:17,519] 我就选择相信这个人
[00:00:17,559 -> 00:00:19,559] 你是很难形成这里的判断力的
[00:00:19,719 -> 00:00:21,480] 真正的我的思辨过程
[00:00:21,480 -> 00:00:22,640] 我对技术的理解
[00:00:22,640 -> 00:00:25,399] 我对技术的总结和为什么这些技术的总结
[00:00:25,399 -> 00:00:26,760] 叠加这些东西
[00:00:26,879 -> 00:00:28,559] 可以让我得到这样的结论
[00:00:00,000 -> 00:00:02,200] 我都在我的文档里面说了
[00:00:02,200 -> 00:00:04,599] 大家有兴趣的话可以去看
[00:00:04,599 -> 00:00:07,799] 但是总之我的建议就是不要去做结论党
[00:00:07,799 -> 00:00:09,400] 不要去做省留党
[00:00:09,400 -> 00:00:13,400] 你真正有价值的结论没价值
[00:00:13,400 -> 00:00:14,400] 你真的就是他
[00:00:14,400 -> 00:00:17,800] 你只是在寻找一个和你听着有点关系的东西
[00:00:17,800 -> 00:00:22,199] 但是他怎么得到这个的才是你最应该关心的
[00:00:22,199 -> 00:00:24,800] 尤其在这个ChinaGPT马上就要取代绝大多数
[00:00:24,800 -> 00:00:26,899] 观众类公司的一个年代了
[00:00:26,899 -> 00:00:28,699] 你如果还停留在找结论的话
[00:00:00,000 -> 00:00:02,680] 那你的思维能力一定是不如拆GPT的
[00:00:02,879 -> 00:00:05,799] 如果你的算力和你的信息明显会不如拆GPT
[00:00:05,839 -> 00:00:07,559] 你的态度 然后你的ego
[00:00:07,559 -> 00:00:11,759] 然后你的心地 你的理性都不如拆GPT
[00:00:11,800 -> 00:00:13,439] 你的判断力和你的思维能力
[00:00:13,439 -> 00:00:14,640] 再不如拆GPT的话
[00:00:14,640 -> 00:00:15,839] 那你还剩下什么呢
[00:00:15,880 -> 00:00:18,359] 好的 灌输焦虑就灌输到这吧
[00:00:18,399 -> 00:00:19,719] 我反正是一点都不焦虑
[00:00:19,719 -> 00:00:21,640] 我特别希望拆GPT的到来
[00:00:21,640 -> 00:00:24,320] 我迫不及待等待它赶紧到来的那一天
[00:00:24,519 -> 00:00:26,280] 那这期视频就到这里
[00:00:26,320 -> 00:00:27,839] 我们下期再见 拜拜
