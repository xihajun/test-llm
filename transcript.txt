[00:00:00,000 -> 00:00:02,160] 难道不是希望做一个比Google更好的吗
[00:00:02,720 -> 00:00:04,320] 不 我十分钟做一个Google
[00:00:05,080 -> 00:00:07,240] 所以降临派到时候
[00:00:07,240 -> 00:00:09,039] 到时候应该降临了
[00:00:09,039 -> 00:00:10,880] 要禁止我去降临派了
[00:00:10,880 -> 00:00:11,759] 群主
[00:00:11,759 -> 00:00:12,919] 你们禁临派了
[00:00:12,919 -> 00:00:13,919] 另外一个就是说
[00:00:13,919 -> 00:00:15,480] 那我们应该怎么看待
[00:00:15,480 -> 00:00:16,199] HotSession这个问题
[00:00:16,199 -> 00:00:18,359] 就没有HotSession就真的是更好吗
[00:00:21,280 -> 00:00:23,679] 对 我这有两个问题先留着
[00:00:23,679 -> 00:00:24,640] 要不先问吧
[00:00:24,640 -> 00:00:25,199] 你说
[00:00:25,199 -> 00:00:26,199] 第一个就是
[00:00:26,199 -> 00:00:27,199] Conversation是这样的
[00:00:27,199 -> 00:00:28,120] Instruct
[00:00:00,000 -> 00:00:03,040] instruction 为什么需要强化学习这一
[00:00:03,040 -> 00:00:05,599] 第二个就是它的那一步
[00:00:05,599 -> 00:00:07,080] 它的alignment这一步
[00:00:07,080 -> 00:00:10,560] 现在是用 reinforcement learning的方式
[00:00:10,560 -> 00:00:13,080] 但是是否可以用其他方式也有些
[00:00:13,080 -> 00:00:15,599] 我觉得第二个问题稍微更好回答一点
[00:00:15,599 -> 00:00:18,239] 就是你如果align的只是这个format
[00:00:18,239 -> 00:00:19,559] 或者说想用的时候
[00:00:19,559 -> 00:00:21,239] 就其实并不需要
[00:00:21,239 -> 00:00:24,000] 毕竟并不见得需要所谓的强化学习
[00:00:24,000 -> 00:00:25,600] 因为强化学习更多的是
[00:00:25,600 -> 00:00:28,640] 你需要让这个模型去探索这些东西
[00:00:00,000 -> 00:00:01,600] 那如果你已经知道答案的话
[00:00:01,600 -> 00:00:02,759] 你为什么要让他探索呢
[00:00:02,759 -> 00:00:03,640] 对对吧
[00:00:03,640 -> 00:00:03,919] 嗯
[00:00:03,919 -> 00:00:04,480] 对
[00:00:04,480 -> 00:00:06,559] 然后你第一个问题是
[00:00:06,559 -> 00:00:07,480] instruct
[00:00:07,480 -> 00:00:08,560] 就是instructivity
[00:00:08,560 -> 00:00:09,720] 是用reinforcement learning
[00:00:09,720 -> 00:00:10,439] 但是为什么呢
[00:00:11,080 -> 00:00:12,560] 我觉得这个可能也是跟
[00:00:12,560 -> 00:00:15,359] 就是他需要在conversation上有behave
[00:00:15,359 -> 00:00:17,160] 有有有相关的
[00:00:17,160 -> 00:00:18,719] 但这个我觉得我们可以再
[00:00:18,719 -> 00:00:19,280] 再聊聊这个
[00:00:19,280 -> 00:00:20,960] 我记得前一阵子那个Stanford
[00:00:20,960 -> 00:00:22,920] 出了一个paper是
[00:00:23,120 -> 00:00:23,800] 就是
[00:00:24,160 -> 00:00:25,960] HF但是不用
[00:00:26,120 -> 00:00:27,800] 但是不用RL的这种方式
[00:00:27,800 -> 00:00:29,399] 其实业内武汉也有一些讨论
[00:00:00,000 -> 00:00:06,000] 就是说HF是不是就alignment是否一定要用RL的方式去做
[00:00:06,000 -> 00:00:08,320] 对 是否一定要用RL的这个方式
[00:00:08,320 -> 00:00:10,640] 我好奇就是你怎么看待这个话题的讨论
[00:00:11,320 -> 00:00:14,960] 我觉得alignment首先是一个比较broad的一个
[00:00:14,960 -> 00:00:17,399] 就是到底是什么叫alignment对吧
[00:00:17,399 -> 00:00:18,640] 就和什么东西align
[00:00:18,640 -> 00:00:22,000] 或者说和一个东西align是不是代表和另外一个东西就不align
[00:00:22,000 -> 00:00:25,120] 然后这我觉得就有点哲学的讨论
[00:00:00,000 -> 00:00:04,480] 那说我觉得RL更多的是我们觉得现在大家发现的一个
[00:00:04,480 -> 00:00:09,359] 能够更像人的一种比较有效的方法
[00:00:09,359 -> 00:00:11,199] 但未来是不是这是唯一的方法
[00:00:11,199 -> 00:00:12,800] 我觉得这也是不确定
[00:00:12,800 -> 00:00:17,199] 我更区分强化学习和普通的
[00:00:17,199 -> 00:00:20,239] 我们所说的这种就是微调模型或者这种
[00:00:20,239 -> 00:00:21,839] 或者说single turn模型
[00:00:21,839 -> 00:00:22,760] 就是最重要的一个
[00:00:22,760 -> 00:00:24,519] 我觉得那几个部分
[00:00:24,519 -> 00:00:28,160] 一个是你需不需要让模型去做一个sequence of decision
[00:00:28,160 -> 00:00:29,679] 然后你才知道这个reward
[00:00:00,000 -> 00:00:02,879] 如果你每一课都知道准确的reward的话
[00:00:02,879 -> 00:00:05,759] 那你不见得需要用强化学习
[00:00:05,759 -> 00:00:08,480] 你只有需要做一个sequence的时候你才需要reward
[00:00:08,480 -> 00:00:10,320] 第二个 这是我个人的观点
[00:00:10,320 -> 00:00:12,160] 我其实很少听到别人去这么说
[00:00:12,160 -> 00:00:14,960] 我也非常想知道是不是正确的
[00:00:14,960 -> 00:00:17,039] 但是我目前自己是这么理解的
[00:00:17,039 -> 00:00:19,440] 就是你需要做强化学习的时候
[00:00:19,440 -> 00:00:21,519] 你很有可能你得到reward
[00:00:21,519 -> 00:00:23,679] 它到模型上而不differentiable
[00:00:23,679 -> 00:00:24,960] 我举个例子
[00:00:24,960 -> 00:00:26,480] 比如说我们现在训练一个模型
[00:00:26,480 -> 00:00:28,800] 任何一个loss function
[00:00:00,000 -> 00:00:01,600] 它本身是defensible
[00:00:01,600 -> 00:00:04,559] 所以你可以根据这个loss去把
[00:00:04,559 -> 00:00:07,679] 这个weights update到每一个parameter到多少是知道的
[00:00:07,679 -> 00:00:08,080] 对
[00:00:08,080 -> 00:00:10,720] 因为你就是forward pass过去
[00:00:10,720 -> 00:00:11,839] 你获得了这个loss
[00:00:11,839 -> 00:00:15,919] 然后你这个整个模型和loss function都是可导的
[00:00:15,919 -> 00:00:18,719] 所以你才知道模型应该update成什么
[00:00:18,719 -> 00:00:20,440] 而强化学习当中
[00:00:20,440 -> 00:00:22,839] 你很多时候这个reward它不是可导的
[00:00:22,839 -> 00:00:23,320] 是的
[00:00:23,320 -> 00:00:25,039] 这个东西0或者1
[00:00:25,039 -> 00:00:25,280] 对
[00:00:25,280 -> 00:00:26,120] 成功没成功
[00:00:26,120 -> 00:00:26,399] 对
[00:00:26,399 -> 00:00:28,600] 或者甚至这个reward大小都是一个
[00:00:00,000 -> 00:00:03,000] 我们所谓说超参数 Hyperparameter
[00:00:03,000 -> 00:00:05,679] 那我们就没有办法
[00:00:05,679 -> 00:00:07,559] 如果你遇见了一个情况
[00:00:07,559 -> 00:00:09,119] 你的reward是不可打的
[00:00:09,119 -> 00:00:10,519] 你的loss方面是不可打的
[00:00:10,519 -> 00:00:10,759] 对
[00:00:10,759 -> 00:00:11,679] 那你是不是说
[00:00:11,679 -> 00:00:15,199] 我觉得就更适合去用强化学习去这个思路
[00:00:15,199 -> 00:00:16,640] 但同时同理来说
[00:00:16,640 -> 00:00:17,839] 如果是可打的话
[00:00:17,839 -> 00:00:22,440] 那你也不见得一定是要用强化学习去完成这件事情
[00:00:22,440 -> 00:00:24,160] 那我好奇一下
[00:00:24,160 -> 00:00:26,519] 我不知道这个是不是相关会我理解对不对
[00:00:00,000 -> 00:00:03,960] 比如说你刚才讲到一系列的决策
[00:00:03,960 -> 00:00:06,040] 就让我很容易想到像Auto-GPT
[00:00:06,040 -> 00:00:09,359] 这样的就是涉及一系列的行为
[00:00:09,359 -> 00:00:11,279] 就我们所谓的Agent
[00:00:11,279 -> 00:00:14,400] 现在我们看到一些Agent的一些实现
[00:00:14,400 -> 00:00:16,640] 虽然说大家可以当然有各种各样的问题
[00:00:16,640 -> 00:00:20,839] 但是它的确是在一系列的action之间
[00:00:20,839 -> 00:00:22,320] 在不断的去做自我调整
[00:00:22,320 -> 00:00:25,440] 这个属不属于你刚才所说的这种情况
[00:00:25,440 -> 00:00:26,640] 如果属于的话
[00:00:00,000 -> 00:00:04,360] 其实像什么BJHI什么OGBT
[00:00:04,360 -> 00:00:10,320] 其实他们也并没有涉及到RHF这个方向上的这种变动
[00:00:10,320 -> 00:00:14,000] 所以就我好奇就是你怎么看待这一块
[00:00:14,000 -> 00:00:16,359] 首先我觉得你这个说的特别好
[00:00:16,359 -> 00:00:19,359] 就是这些agent首先一定是做一个sequence of decision
[00:00:19,359 -> 00:00:20,719] 互相相互depend
[00:00:20,719 -> 00:00:26,079] 这个一定是一个很好的用来做强化学习的一个环境场景
[00:00:26,079 -> 00:00:28,199] 那你其实就要设计一下这个
[00:00:00,000 -> 00:00:03,560] 因为强化学有几个必须要完成的条件
[00:00:03,560 -> 00:00:05,440] 就比如说这个环境是什么样
[00:00:05,440 -> 00:00:06,759] 它的状态是什么样
[00:00:06,759 -> 00:00:08,800] 它们的状态之间是怎么样变化的
[00:00:08,800 -> 00:00:10,199] 或者是它的reward应该是什么
[00:00:10,199 -> 00:00:11,759] 如果你能把这些设计好
[00:00:11,759 -> 00:00:15,199] 你其实真是可以把这些feedback还到给模型的
[00:00:15,199 -> 00:00:17,280] 那你就回到我们刚刚那个话题
[00:00:17,280 -> 00:00:18,719] 就是如果你要还到这个模型
[00:00:18,719 -> 00:00:20,839] 它是不是还是变了一个更好的一个模型了
[00:00:20,839 -> 00:00:24,559] 我认为你通过这个RLHL的过程当中
[00:00:24,559 -> 00:00:26,199] 你的模型发生了一些变化
[00:00:26,199 -> 00:00:27,800] 那它就可以变得更好
[00:00:27,800 -> 00:00:28,239] 比如像
[00:00:00,000 -> 00:00:01,679] 但是可以这样agree
[00:00:01,679 -> 00:00:04,040] 就是模型不是必须要发生变化
[00:00:04,440 -> 00:00:06,599] 模型在不发生变化的情况下
[00:00:06,599 -> 00:00:09,759] 也能让它通过这两步达到
[00:00:09,759 -> 00:00:13,039] 就是完成任务的能力变得更强
[00:00:13,599 -> 00:00:15,160] 就是模型可以变化
[00:00:15,160 -> 00:00:16,480] 但是它不是一个必要条件
[00:00:16,800 -> 00:00:17,879] 我觉得可以这么说
[00:00:17,879 -> 00:00:19,160] 那这个如果我们
[00:00:19,160 -> 00:00:20,039] 你如果想拼一下
[00:00:20,039 -> 00:00:23,800] 我们可以聊关于所谓不同种类的
[00:00:23,800 -> 00:00:26,199] fine tuning他们的做法不一样
[00:00:26,199 -> 00:00:27,120] 我觉得你说的这个东西
[00:00:27,120 -> 00:00:29,719] 可能更适合某一类 特类的
[00:00:00,000 -> 00:00:02,240] 我其实同意你那天吃饭的时候说的一个点
[00:00:02,240 -> 00:00:04,679] 就是你如果说纯参数的update
[00:00:04,679 -> 00:00:06,360] 就是和我们现在的做法
[00:00:06,360 -> 00:00:08,560] 我们现在最多就能发挥60%的效果
[00:00:08,560 -> 00:00:10,439] 我觉得让模型发生变化
[00:00:10,439 -> 00:00:12,960] 肯定最后是一个最optimal的方式
[00:00:12,960 -> 00:00:14,919] 但是我会觉得
[00:00:14,919 -> 00:00:18,879] 就是为了保证它现在的general
[00:00:18,879 -> 00:00:21,239] 所以说模型不发生变化
[00:00:21,239 -> 00:00:23,600] 而通过第二步去提高它的能力
[00:00:23,600 -> 00:00:25,160] 是一个更general的方式
[00:00:25,160 -> 00:00:27,559] 以及它已经远远超过其之前的能力了
[00:00:27,559 -> 00:00:28,960] 所以说这已经够了
[00:00:00,000 -> 00:00:03,720] 那其实你要完成你这个目标
[00:00:03,720 -> 00:00:06,280] 你模型的参数可以变化
[00:00:06,280 -> 00:00:10,560] 但是有可能每一个人会得获得自己一部分可调参数
[00:00:10,560 -> 00:00:12,720] 就是整个模型不变
[00:00:12,720 -> 00:00:16,719] 但是模型的每一层当中加一小块是可以调整的
[00:00:16,719 -> 00:00:19,600] 或者是最后一层加一小块是可以调整的
[00:00:19,600 -> 00:00:21,600] 那其实就跟你的这个例子是一样的
[00:00:21,600 -> 00:00:23,879] 虽然它是一个参数的形式存在
[00:00:23,879 -> 00:00:26,399] 但是它并不影响本身大模型的表现
[00:00:26,399 -> 00:00:26,679] 对
[00:00:26,679 -> 00:00:27,480] 我觉得这也是另外一个
[00:00:27,480 -> 00:00:28,280] 也可以
[00:00:00,000 -> 00:00:01,800] 我再把那个再说
[00:00:01,800 -> 00:00:03,160] 或者说这样吧
[00:00:03,160 -> 00:00:04,000] 你刚刚说的是
[00:00:04,000 -> 00:00:07,559] 我想下来两件事
[00:00:07,559 -> 00:00:08,800] 我抽象理解的话
[00:00:08,800 -> 00:00:11,039] 你刚刚说的是给模型做加法
[00:00:11,039 -> 00:00:13,359] 或者说减了以后加对吧
[00:00:13,359 -> 00:00:14,720] 就是它的参数有改变
[00:00:14,720 -> 00:00:16,039] 我说的是一个乘法
[00:00:16,039 -> 00:00:18,120] 就是你这个layer没有变
[00:00:18,120 -> 00:00:19,920] 但是你又加了一层layer
[00:00:19,920 -> 00:00:21,239] 然后乘以后
[00:00:21,239 -> 00:00:23,079] 你的模型的performance发生了变化
[00:00:23,079 -> 00:00:24,079] 我大概是这个意思
[00:00:00,000 -> 00:00:06,360] 就是alignment overloaded这一层
[00:00:06,360 -> 00:00:10,720] 它是其实是一个你可以把它理解成为另外一个matrix
[00:00:10,720 -> 00:00:13,720] 然后我通过alignment成了这个matrix
[00:00:13,720 -> 00:00:15,519] 然后我得到了更好的效果
[00:00:15,519 -> 00:00:18,480] 我不需要去改变模型本身的参数
[00:00:18,480 -> 00:00:19,440] 大概是这个意思
[00:00:19,440 -> 00:00:22,160] 你的意思就是说我说的这些东西
[00:00:22,160 -> 00:00:26,000] 每一个都是说在特定任务上加上了这个东西使得它成
[00:00:26,000 -> 00:00:27,800] 但是你说的是
[00:00:00,000 -> 00:00:05,280] 可能有一系列模型和一系列的这个alignment的方法
[00:00:05,280 -> 00:00:07,280] 然后他们可以互相就
[00:00:07,280 -> 00:00:09,759] 对或者就一个GPT
[00:00:09,759 -> 00:00:12,400] 因为GPT它这个本身的这个matrix太牛逼了
[00:00:12,400 -> 00:00:16,640] 然后我不同的任务我就去研究针对不同任务最optimal的matrix
[00:00:16,640 -> 00:00:19,359] 但是1乘n不就像和1加n没什么区别
[00:00:20,800 -> 00:00:22,039] 不是一个模型
[00:00:22,039 -> 00:00:25,000] 就是这模型里边的1751个参数不变
[00:00:25,000 -> 00:00:28,960] 而这里边可能我有一个10个亿的参数
[00:00:28,960 -> 00:00:29,760] 500万的参数
[00:00:00,000 -> 00:00:03,279] 然后就让它的task的效果变好了
[00:00:03,720 -> 00:00:06,559] 我只需要去focus在改变这里的参数就行了
[00:00:06,559 -> 00:00:09,759] 而不需要去对那1750一个参数做任何改变
[00:00:09,759 -> 00:00:12,039] 对对对 那这个其实我们就是说
[00:00:12,039 -> 00:00:16,399] 那这其实说到就是全参数微调和部分参数微调
[00:00:16,399 -> 00:00:19,120] 像你说这种更多是就有专门有一个部分
[00:00:19,120 -> 00:00:21,719] 有一个area of work叫adapter
[00:00:21,719 -> 00:00:26,000] 其实就是这个模型它可以插入一部分参数进去
[00:00:26,000 -> 00:00:28,519] 那你不用管它是插入在模型的中间
[00:00:00,000 -> 00:00:01,600] 还是在最后还是前面
[00:00:01,600 -> 00:00:02,680] 其实就是插入一部分
[00:00:02,680 -> 00:00:03,839] 然后它是可以替换的
[00:00:03,839 -> 00:00:06,280] 但其实你本身是一个基础模型
[00:00:06,280 -> 00:00:07,280] 或者一个底层模型
[00:00:07,280 -> 00:00:10,960] 你可以不断的插入不同的这种adapter
[00:00:10,960 -> 00:00:11,800] 但是这是很困难
[00:00:11,800 -> 00:00:13,119] 这差 这数值差也有区别
[00:00:13,119 -> 00:00:13,560] 对对对对
[00:00:13,560 -> 00:00:14,240] 是怎么差
[00:00:14,240 -> 00:00:16,480] 然后甚至说你插入的是参数
[00:00:16,480 -> 00:00:17,839] 还插入的是粒子
[00:00:17,839 -> 00:00:18,280] 对
[00:00:18,280 -> 00:00:21,000] 都是可以 这都是不同的一种思路
[00:00:21,000 -> 00:00:21,480] 或者是
[00:00:21,480 -> 00:00:22,760] 这个是在pre-train阶段吗
[00:00:22,760 -> 00:00:23,000] 还是
[00:00:23,000 -> 00:00:24,079] 这是在应用阶段的时候
[00:00:24,079 -> 00:00:24,559] 应用阶段
[00:00:24,559 -> 00:00:27,640] 就是你可以训练一个插入的adapter参数
[00:00:27,640 -> 00:00:29,199] 就像是比如说我这个模型
[00:00:00,000 -> 00:00:03,480] 本来有就是一个trillion的parameter
[00:00:03,480 -> 00:00:05,639] 那我可能就插入了一个一个billion
[00:00:05,639 -> 00:00:07,040] 或者half a billion的parameter
[00:00:07,040 -> 00:00:09,400] 但它分布在这个模型不同的位置
[00:00:09,400 -> 00:00:10,640] 但我模型知道
[00:00:10,640 -> 00:00:13,039] 哦 这些东西实际上它是可被替换的
[00:00:13,039 -> 00:00:14,640] 那你可以拿走
[00:00:14,640 -> 00:00:17,320] 你也可以换上另外一个去不断去替换
[00:00:17,320 -> 00:00:19,600] 那你们也可以说是我插入的是参数
[00:00:19,600 -> 00:00:20,640] 那有可能呢
[00:00:20,640 -> 00:00:23,679] 我只在最前头插入了一段文字
[00:00:23,679 -> 00:00:25,800] 那其实就有点像prompt
[00:00:25,800 -> 00:00:27,399] 或者说我们所谓说的in context
[00:00:00,000 -> 00:00:02,560] 或者再往前就说叫prefix tuning
[00:00:02,560 -> 00:00:06,400] 就是说我能不能在我说的之前加一段
[00:00:06,400 -> 00:00:10,080] 不管是文字就是hard prefix或者soft prefix
[00:00:10,080 -> 00:00:11,400] 就我增加一个vector
[00:00:11,400 -> 00:00:14,279] 那在我的文字当中那会不会有提高
[00:00:14,279 -> 00:00:16,480] 其实这就是这都是像你说的
[00:00:16,480 -> 00:00:18,719] 就是不改变大模型的本身的情况下
[00:00:18,719 -> 00:00:21,920] 去再让这个模型应用在不同的任务上
[00:00:21,920 -> 00:00:23,480] 好我先说观点二
[00:00:23,480 -> 00:00:26,320] 然后再打一个跟针对你刚才说的这东西比方
[00:00:26,320 -> 00:00:27,440] 观点二就是
[00:00:00,000 -> 00:00:06,480] 如果说我们说的开放reinforcement learning这一层出现了以后
[00:00:06,480 -> 00:00:07,599] 我觉得他们很快就没了
[00:00:07,599 -> 00:00:12,080] 因为这个sequence其实你在Lineman那一步是更好被调节的
[00:00:12,080 -> 00:00:15,359] 而且更可就是我们现在是
[00:00:15,359 -> 00:00:20,079] 如果大家去读InstructGPT或ChaiGPT的paper
[00:00:20,079 -> 00:00:23,719] 他们做的流程其实是非常透明和容易复制的
[00:00:23,719 -> 00:00:25,879] 就是你去对话
[00:00:25,879 -> 00:00:26,800] 然后人去打标
[00:00:26,800 -> 00:00:29,320] 打完标了以后去告诉他什么好什么不好
[00:00:00,000 -> 00:00:00,720] 然后去train
[00:00:01,000 -> 00:00:04,719] 然后人这个环节完全可以被机器去替代
[00:00:04,719 -> 00:00:06,759] 就是但是任务不一样了
[00:00:06,759 -> 00:00:09,880] 就是比如说我需要编出来一个什么样的系统
[00:00:09,880 -> 00:00:11,599] 比如说我要编Google的一个ranker
[00:00:11,599 -> 00:00:11,839] 对吧
[00:00:11,839 -> 00:00:14,320] 然后我要让他的ranking performance特别好
[00:00:14,320 -> 00:00:17,559] 那我是可以把这个任务拆解成几个重要的步骤
[00:00:17,559 -> 00:00:21,480] 然后就直接用这个结果去feedback给GPT
[00:00:21,480 -> 00:00:22,760] 然后让他去做就行了
[00:00:22,879 -> 00:00:26,440] 那最后他应该我觉得就是假设他的能力是到位的话
[00:00:26,440 -> 00:00:28,280] 他应该可以做出一个Google
[00:00:00,000 -> 00:00:02,000] 会不断的调整自己
[00:00:02,000 -> 00:00:04,120] 使得自己的ranking的效果会越来越好
[00:00:04,120 -> 00:00:06,879] 通过一些这online的一些feedback来讲
[00:00:06,879 -> 00:00:09,880] 对 问题是这feedback明确的情况下
[00:00:09,880 -> 00:00:11,560] 它的反馈效率
[00:00:11,560 -> 00:00:13,880] 远远不是人给他提供feedback能比的
[00:00:13,880 -> 00:00:15,679] 他可能10分钟就做出来了
[00:00:16,960 -> 00:00:21,399] 那你这门我想就是解释一下
[00:00:21,399 -> 00:00:23,039] 就是你觉得这个feedback是
[00:00:23,039 -> 00:00:25,079] 机器怎么提供这样的feedback
[00:00:25,079 -> 00:00:26,160] 就是你说的呀
[00:00:26,160 -> 00:00:27,719] 就是你们当时做的那个
[00:00:00,000 -> 00:00:03,200] 就是和其他就是和就是去对比嘛
[00:00:03,200 -> 00:00:05,759] 去benchmark其他的其他的那个
[00:00:05,759 -> 00:00:07,280] 就是我直接benchmark google
[00:00:07,280 -> 00:00:08,800] 然后你什么时候达到google performance
[00:00:08,800 -> 00:00:09,560] 然后就行了
[00:00:09,560 -> 00:00:12,080] ok 那这个确实那你
[00:00:12,080 -> 00:00:14,599] 但是你希望去做这个强化学习
[00:00:14,599 -> 00:00:16,960] 你难道不是希望做一个比google更好的吗
[00:00:17,399 -> 00:00:19,120] 不我10分钟做一个google
[00:00:19,120 -> 00:00:20,199] 哦那你
[00:00:20,199 -> 00:00:21,199] 我10分钟做一个google
[00:00:21,199 -> 00:00:23,679] 然后我也可以去想方设法定义比google更好
[00:00:23,679 -> 00:00:25,079] 然后去给他提供feedback
[00:00:25,079 -> 00:00:27,199] 但是问题是我这10分钟做一个google
[00:00:00,000 -> 00:00:03,660] 不是说我的模型在已经人写好的情况下去
[00:00:03,660 -> 00:00:05,700] 去达到这样的performance
[00:00:05,700 -> 00:00:08,699] 而是他从零开始让他重新开始写代码
[00:00:08,699 -> 00:00:09,339] 对吧
[00:00:09,339 -> 00:00:11,699] 你就给我写代码写出来一个Google
[00:00:11,699 -> 00:00:13,380] 然后他10分钟可以做到这件事
[00:00:14,179 -> 00:00:15,939] 我觉得未来是有可能的
[00:00:15,939 -> 00:00:17,660] 这就是我觉得AGI可怕之处
[00:00:17,660 -> 00:00:18,980] 就是这就是为什么我觉得
[00:00:18,980 -> 00:00:22,460] Chai GP OpenAI不敢随便开放reinforcement learning这一步
[00:00:22,460 -> 00:00:24,500] 因为其实我刚说的还是个简单的
[00:00:24,500 -> 00:00:26,699] 10分钟取代Google大家觉得是可行的
[00:00:26,699 -> 00:00:29,420] 但是如果说我是一个更高的目标
[00:00:00,000 -> 00:00:01,120] 如果我能想象出来
[00:00:01,120 -> 00:00:03,160] 且我能给他提供reward function的话
[00:00:03,200 -> 00:00:05,400] 他也可以好好的做到这件事
[00:00:05,919 -> 00:00:09,800] 我觉得从理想上讲肯定是这样的
[00:00:09,800 -> 00:00:09,960] 对
[00:00:09,960 -> 00:00:13,000] 这个让我想到Silicon Valley那个
[00:00:13,039 -> 00:00:14,240] 那个电视剧里面
[00:00:14,240 -> 00:00:14,800] 对吧
[00:00:14,800 -> 00:00:16,679] 就是从一个小的
[00:00:16,719 -> 00:00:18,800] 这个你只optimize for a small goal
[00:00:18,800 -> 00:00:20,679] but ultimately就是他发现你
[00:00:20,679 -> 00:00:23,440] 其实你反正要optimize他
[00:00:23,440 -> 00:00:25,399] 其实花了一天的时间
[00:00:25,399 -> 00:00:27,079] 破解了世界上最难被的加密算法
[00:00:27,079 -> 00:00:28,519] 我当时在那个文章里面有说
[00:00:00,000 -> 00:00:02,279] 我就说skynet这种情况应该不会出现
[00:00:02,279 -> 00:00:05,759] 因为AGI它们就是模型它没有一个自己的
[00:00:05,759 -> 00:00:06,599] motivation
[00:00:06,599 -> 00:00:08,279] 它没有一个intrinsic function
[00:00:08,279 -> 00:00:09,919] 它自己没有目标
[00:00:09,919 -> 00:00:13,560] 但是你让它去破解世界上加密算法
[00:00:13,560 -> 00:00:16,120] 它有可能把它高效的破解出来了
[00:00:16,120 -> 00:00:17,480] 这个是存在可能性的
[00:00:17,480 -> 00:00:20,359] 其实这个就其实这个是rule backed out
[00:00:20,359 -> 00:00:22,480] 我一开始问你的这个问题
[00:00:22,480 -> 00:00:24,879] 就是你觉得开放了LHF
[00:00:24,879 -> 00:00:26,079] 是可以使得我们做哪些
[00:00:26,079 -> 00:00:27,399] 我们现在做不了的这个事情
[00:00:00,000 -> 00:00:02,819] 而刚刚之所以讲到这个Auto-GBT其实也是
[00:00:02,819 -> 00:00:04,259] 我们都知道它只是一个demo
[00:00:04,259 -> 00:00:07,900] 但它提供了一个让更多人看到的一种可能性
[00:00:07,900 -> 00:00:09,660] 其实在这个可能性中
[00:00:09,660 -> 00:00:11,660] 其实大家就在讲说把它做到落地
[00:00:11,660 -> 00:00:14,300] 就是有可能有哪一些这个Bottleneck
[00:00:14,300 -> 00:00:16,699] 其中一个就是怎么样能够让他去
[00:00:16,699 -> 00:00:18,500] 自己知道怎么去收
[00:00:18,500 -> 00:00:20,260] 自己知道怎么去收敛中的一个东西
[00:00:20,260 -> 00:00:24,140] 其实就是刚才如意讲到的那个它的sequence里边
[00:00:24,140 -> 00:00:26,820] 就是sequence里边怎么样去帮他去优化
[00:00:00,000 -> 00:00:03,720] 能够帮他在每一个这个过程中去提供这个这个reward
[00:00:03,720 -> 00:00:07,599] 而似乎刚刚大家在想说要能够帮他很好做到这一点
[00:00:07,599 -> 00:00:11,279] 是要在可能在LHF这个环节去实现的
[00:00:11,279 -> 00:00:13,320] 我去打那个比方啊
[00:00:13,320 -> 00:00:14,359] 我先把那个比方打完
[00:00:14,359 -> 00:00:17,239] 就是跟你说的这个东西的关系是什么
[00:00:17,239 -> 00:00:23,039] 就是pre-train那个大模型能力这么多
[00:00:23,039 -> 00:00:26,239] 我们通过alignment发挥了这么多
[00:00:26,239 -> 00:00:29,719] 然后通过不同的prompt又发挥了这么多
[00:00:00,000 -> 00:00:04,919] 然后现在AutoGPT是在通过prompt去探索这里边的天花板
[00:00:04,919 -> 00:00:07,639] 然后呢最多就到这而已
[00:00:07,639 -> 00:00:10,080] 但是你开放了reinforcement learning之后
[00:00:10,080 -> 00:00:11,919] 你的天花板可能可以到这
[00:00:13,000 -> 00:00:14,560] 对大概就是这种感觉
[00:00:14,560 -> 00:00:17,679] 我的比方就是说你就把它比方成人
[00:00:17,679 -> 00:00:20,199] 这个人你给他读了世界上所有书
[00:00:20,199 -> 00:00:21,480] 请了最好的老师
[00:00:21,480 -> 00:00:23,199] 教了一个特别聪明的人
[00:00:23,480 -> 00:00:26,000] 这个时候如果用你的那个改参数的方式
[00:00:26,000 -> 00:00:27,640] 你可以给他再加点芯片
[00:00:27,640 -> 00:00:29,280] 然后给他一些特定的知识
[00:00:00,000 -> 00:00:02,000] 那它是可以变得更聪明更有效率
[00:00:02,000 -> 00:00:03,600] 但是另外的做法就是
[00:00:03,600 -> 00:00:05,200] 你给它一个很聪明的人
[00:00:05,200 -> 00:00:08,640] 然后去让他去研究一些任务
[00:00:08,640 -> 00:00:10,839] 然后去因为他有特别强的算力
[00:00:10,839 -> 00:00:12,240] 所以说他在一个任务上
[00:00:12,240 -> 00:00:14,599] 通过不同的调教和反馈
[00:00:14,599 -> 00:00:15,599] 第一手的反馈
[00:00:15,599 -> 00:00:17,559] 他能在这个任务上变得特别牛逼
[00:00:17,559 -> 00:00:19,800] 但是我们现在是说有一个人
[00:00:19,800 -> 00:00:21,519] 你只教他怎么跟人家对话
[00:00:21,519 -> 00:00:23,519] 然后在之后说不同的人给你对话
[00:00:23,519 -> 00:00:24,640] 你给他不同的反应
[00:00:24,640 -> 00:00:26,760] 我们在这上面已经看到了
[00:00:26,760 -> 00:00:28,320] 这个模型的这么多潜力了
[00:00:00,000 -> 00:00:02,500] 在開放前面的一層
[00:00:02,500 -> 00:00:04,299] 我只想到開放前面的那一層
[00:00:04,299 -> 00:00:06,459] 就很可怕了已經
[00:00:06,459 -> 00:00:09,259] 所以降臨派
[00:00:09,259 -> 00:00:11,460] 到時候一旦降臨了
[00:00:11,460 -> 00:00:14,300] 要記得我是降臨派的群主
[00:00:14,300 -> 00:00:15,500] 你不是靜音派的
[00:00:15,500 -> 00:00:16,800] 可以可以
[00:00:16,800 -> 00:00:18,719] 好呀好呀
[00:00:18,719 -> 00:00:22,640] 我們聊到了就是
[00:00:22,640 -> 00:00:24,260] OpenEducate一長
[00:00:24,260 -> 00:00:25,760] 現在所有的人都要死
[00:00:25,760 -> 00:00:29,420] 如果開放了這種強化學習
[00:00:00,000 -> 00:00:03,200] 到底对这些应用上面到底有什么样的影响
[00:00:03,200 -> 00:00:04,320] 会开放什么样的新机会
[00:00:04,320 -> 00:00:04,960] 对正好
[00:00:04,960 -> 00:00:05,480] 对正好
[00:00:05,480 -> 00:00:06,360] 因为我想起来
[00:00:06,360 -> 00:00:09,039] 之前我在脑子里边ping的一个地方
[00:00:09,039 -> 00:00:10,960] 就是你提到这个
[00:00:10,960 -> 00:00:12,199] 如意现在做的公司
[00:00:12,199 -> 00:00:14,359] 你们是选择自己
[00:00:14,359 -> 00:00:16,519] build一个自己的模型
[00:00:16,519 -> 00:00:17,039] 所以可以
[00:00:17,039 -> 00:00:18,079] 我觉得可以讲讲这个
[00:00:18,079 -> 00:00:21,399] 因为这个也是现在大家经常会讨论的一个东西
[00:00:21,399 -> 00:00:22,440] 就是你们怎么做
[00:00:22,440 -> 00:00:25,000] 我到底是自己build一个模型
[00:00:25,000 -> 00:00:27,800] 还是说用现有模型的一个决策
[00:00:00,000 -> 00:00:03,200] 是因为你们看到现有的模型哪一些限制
[00:00:03,200 -> 00:00:05,400] 那going forward就好像我们刚才提到的
[00:00:05,400 -> 00:00:08,320] 如果我们要准备好说
[00:00:08,320 -> 00:00:10,800] 那这个模型现在的能力
[00:00:10,800 -> 00:00:12,839] 还有很多我们不知道的这个地方
[00:00:12,839 -> 00:00:17,239] 你觉得我相信很多听众也是创业公司的小伙伴
[00:00:17,239 -> 00:00:17,480] 对吧
[00:00:17,480 -> 00:00:19,440] 我一方面希望现在能够尽快落地
[00:00:19,440 -> 00:00:23,559] 那另一方面我要为了未来可能的底层模型的变化
[00:00:23,559 -> 00:00:24,760] 需要做哪一些准备
[00:00:24,760 -> 00:00:26,120] 你的思考是怎么样
[00:00:00,000 -> 00:00:04,759] OK 我觉得我先说一下我们为什么自己做自己的这些模型
[00:00:05,000 -> 00:00:06,440] 就是说当然了
[00:00:06,440 -> 00:00:15,679] 就是一方面我们想说就是如何让这个模型中获得专业专门是这种客服方面的知识
[00:00:15,679 -> 00:00:18,280] 就是这些agent上面是如何回答这些问题的
[00:00:18,280 -> 00:00:20,199] 我们其实做了很多这种评估
[00:00:20,199 -> 00:00:22,679] 就是说如果同样这个问题
[00:00:22,679 -> 00:00:25,160] 这用户提交了这么一个问题
[00:00:25,160 -> 00:00:27,559] 那就是他们的用户提交这么一个问题
[00:00:27,559 -> 00:00:29,079] 那模型应该去如何回答
[00:00:00,000 -> 00:00:02,600] 很多时候模型就算我们提供了一些
[00:00:02,600 -> 00:00:04,160] 比如说文章在其中
[00:00:04,599 -> 00:00:06,320] 文章有可能是详细
[00:00:06,320 -> 00:00:07,639] 有可能就缺少一些步骤
[00:00:07,639 -> 00:00:10,640] 那么他和agent
[00:00:10,640 -> 00:00:12,839] 就是说那些真正客服的那些人
[00:00:12,839 -> 00:00:14,640] 他们去完成那些事情是不一样的
[00:00:14,919 -> 00:00:19,199] 更多的是文章是让客户自己去troubleshoot自己
[00:00:19,199 -> 00:00:20,039] 或者怎么去做
[00:00:20,039 -> 00:00:22,800] 而并不是说agent应该去完成什么事情
[00:00:22,800 -> 00:00:25,839] 而agent他会回答我做了什么事
[00:00:25,839 -> 00:00:27,800] 然后你应该去做xyz
[00:00:00,000 -> 00:00:04,679] 这件事情上本身是模型没有的这方面知识
[00:00:04,679 -> 00:00:07,040] 那么如何把这方面知识放到模型当中
[00:00:07,040 -> 00:00:08,720] 让模型去生成的答案当中
[00:00:08,720 -> 00:00:10,679] 更像是一个agent去回答的东西
[00:00:10,679 -> 00:00:13,320] 这更像是我们的一个目标
[00:00:13,640 -> 00:00:14,839] 在做这方面的时候
[00:00:14,839 -> 00:00:16,559] 其中还一个就是说
[00:00:17,280 -> 00:00:22,359] 我们希望因为Chats B得回答的时候更多有些
[00:00:23,640 -> 00:00:27,320] 如果控制它的format去回答问题的方法
[00:00:27,320 -> 00:00:28,120] 我觉得这也是
[00:00:00,000 -> 00:00:04,919] 你当然也可以说我们可以通过更多的prompt去tuning去完成这些事情
[00:00:05,160 -> 00:00:10,599] 但如果你想有一个更加抗比较一致的体验的话
[00:00:10,599 -> 00:00:13,720] 觉得训练自己的模型会提高一些更加一致体验
[00:00:13,720 -> 00:00:16,920] 当然了做这件事情肯定是有它的代价的
[00:00:17,600 -> 00:00:21,120] 最明显的我目前看到几个明显的代价
[00:00:21,120 -> 00:00:26,000] 就除去所谓的工程成本或者说什么其他的成本
[00:00:26,000 -> 00:00:27,399] 包含这些都不提的话
[00:00:00,000 -> 00:00:02,200] 最明显的一个代价
[00:00:02,200 -> 00:00:04,400] 我认为就是文字质量
[00:00:04,400 -> 00:00:08,679] 这Chad GPT或者说其他大模型
[00:00:08,679 -> 00:00:13,160] 他们在大模型当中体现出一个特别和别人不一样
[00:00:13,160 -> 00:00:15,439] 就是他们的文字质量特别的高
[00:00:15,439 -> 00:00:18,239] 特别的像是一个人去说的东西
[00:00:18,239 -> 00:00:19,920] 而一些小的模型
[00:00:19,920 -> 00:00:22,320] 你去让他去完成一些短的东西
[00:00:22,320 -> 00:00:23,879] 或者说让去生成一些东西
[00:00:23,879 -> 00:00:25,079] 他会比较容易
[00:00:25,079 -> 00:00:27,239] 但是一旦生成比较长的时候
[00:00:27,239 -> 00:00:29,600] 他的文字质量或者说用词方法
[00:00:00,000 -> 00:00:02,960] 就会没有大模型用的那么好
[00:00:02,960 -> 00:00:05,040] 就是更大的模型用的那么好
[00:00:05,040 -> 00:00:07,719] 所以这是我发现的一个比较大的一个挑战
[00:00:07,719 -> 00:00:10,199] 其中一些做法可能也就是说
[00:00:10,199 -> 00:00:14,880] 像GPT模型学习或者是训练一个更大的模型
[00:00:14,880 -> 00:00:19,000] 然后从大的模型往小的模型去destill
[00:00:19,000 -> 00:00:21,199] 这些都是一些这样这方面的一些思路
[00:00:21,199 -> 00:00:27,239] 需要用大模型然后destill到小模型
[00:00:00,000 -> 00:00:03,480] 就是说明你的主要成本不是在训练模型上
[00:00:03,480 -> 00:00:05,000] 而是在使用模型inference上
[00:00:05,000 -> 00:00:08,800] 对主要成本将会是叫hosting cost
[00:00:08,800 -> 00:00:12,679] 就是因为这个大家可以算一算
[00:00:12,679 -> 00:00:16,879] 就是说一个模型它需要多少的GPU的RAM
[00:00:16,879 -> 00:00:22,000] 然后你其实如果要让这个模型一直能够available
[00:00:22,000 -> 00:00:27,239] 并且比如说你还要接受更长的sequence的话
[00:00:00,000 -> 00:00:03,759] 那可能他本身就是一个需要不是一个很便宜的机器
[00:00:03,759 -> 00:00:09,080] 如果一个56B和1B的可能这个成本就差的非常多了
[00:00:09,080 -> 00:00:09,599] 对
[00:00:10,080 -> 00:00:10,640] 对
[00:00:10,640 -> 00:00:14,679] 那这个说起来也就是说OpenAI他做的很聪明的一点
[00:00:14,679 -> 00:00:17,399] 就是我现在也觉得很神奇的地方
[00:00:17,399 -> 00:00:20,559] 就是他们能够把他们的hosting cost压的这么低
[00:00:20,760 -> 00:00:23,480] 让我觉得他们是我甚至怀疑他们
[00:00:23,920 -> 00:00:26,559] 你用OpenAI OpenAI要付给你钱使用的
[00:00:26,559 -> 00:00:27,399] 就是他们的
[00:00:27,399 -> 00:00:27,879] 对
[00:00:00,000 -> 00:00:04,559] 就是他们的host这些模型的成本实际上是相当高的
[00:00:05,000 -> 00:00:06,519] 但是对就两方面
[00:00:06,519 -> 00:00:08,480] 一方面它确实能压到很低
[00:00:08,480 -> 00:00:13,480] 比如说3.5就降价降了10倍
[00:00:13,480 -> 00:00:18,079] 对然后但是4很明显大家看到包括各种performance的问题
[00:00:18,079 -> 00:00:20,280] 肯定跟他的这个对
[00:00:20,280 -> 00:00:22,600] 这其实有一些大家说法就是说
[00:00:22,600 -> 00:00:24,640] 比如用甚至用GPS4的时候
[00:00:24,640 -> 00:00:25,960] 它虽然是一个大模型
[00:00:25,960 -> 00:00:28,359] 但是它最后它是生成的部分的时候
[00:00:00,000 -> 00:00:02,399] 他是用一些小模型尝试去做生成
[00:00:02,399 -> 00:00:05,440] 然后如果他叫speculative decoding
[00:00:05,440 -> 00:00:07,480] 这当然也是别人的一些猜测
[00:00:07,480 -> 00:00:11,919] 就是用一些小模型去做后面的几个步骤去做decoding的steps
[00:00:11,919 -> 00:00:13,880] 那去节省这个成本
[00:00:13,880 -> 00:00:18,559] 然后有些人也认为就是这个模型的效果变得更加的不好
[00:00:18,559 -> 00:00:24,120] 也是因为这个他让小的模型去speculate了更多
[00:00:24,120 -> 00:00:26,199] 而不是像原来那么保守
[00:00:26,199 -> 00:00:28,960] 都有这当时就是还是那句话
[00:00:00,000 -> 00:00:01,120] 就是大家都是猜测
[00:00:01,120 -> 00:00:06,200] 不过非常让人觉得震撼的就是他能够用这么便宜的价格
[00:00:06,200 -> 00:00:07,000] 就算是GPC
[00:00:07,000 -> 00:00:11,000] 我觉得也是一个非常便宜的价格去完成这件事情
[00:00:11,000 -> 00:00:13,439] 对因为他NPI收费也并不是这么贵
[00:00:13,439 -> 00:00:15,000] 对那即使他那么便宜
[00:00:15,000 -> 00:00:18,120] 你们仍然选择自己做
[00:00:18,120 -> 00:00:23,719] 对其实你刚才提到的为什么选自己做中的一个
[00:00:23,719 -> 00:00:26,640] 你提到一个是consistency
[00:00:26,640 -> 00:00:27,719] 然后另外一个
[00:00:00,000 -> 00:00:02,560] 我刚想到其实特别想跟大家讨论的
[00:00:02,560 -> 00:00:04,879] 其实就是大家经常诟病大模型的
[00:00:04,879 -> 00:00:08,000] 这个Hallucination这个问题
[00:00:08,000 -> 00:00:10,800] 当然我觉得支持者
[00:00:10,800 -> 00:00:13,199] 我说支持这个Hallucination是个硬伤的人
[00:00:13,199 -> 00:00:16,079] 总是能找出各种支持他的对吧
[00:00:16,079 -> 00:00:19,199] 他们在很多场景下他的不稳定
[00:00:19,199 -> 00:00:21,519] 然后会多么的mission critical
[00:00:21,519 -> 00:00:23,600] 但是我就好奇
[00:00:23,600 -> 00:00:26,320] 我相信柯代表在这方面肯定很有想法
[00:00:26,320 -> 00:00:28,559] 因为我经常在想说一个就是说
[00:00:00,000 -> 00:00:04,960] 首先这个是不是模型的一个硬伤
[00:00:04,960 -> 00:00:07,480] 就是它在不论是模型层面
[00:00:07,480 -> 00:00:08,320] 我现在应该怎么说
[00:00:08,320 -> 00:00:09,839] 就首先一个假设吧
[00:00:09,839 -> 00:00:10,519] 我们就不讨论那个
[00:00:10,519 -> 00:00:13,560] 就假设这个就是个统计模型的一个硬伤
[00:00:13,560 -> 00:00:17,280] 我们就是没有办法100%来去消灭这个
[00:00:17,280 -> 00:00:18,559] Hallucination
[00:00:18,559 -> 00:00:20,679] 那么一个说从应用的角度
[00:00:20,679 -> 00:00:22,519] 我们是否可以在应用的层面
[00:00:22,519 -> 00:00:25,079] 其实可以把它降到它所在的那个
[00:00:25,079 -> 00:00:27,160] 场景里边其实一个可执行
[00:00:27,160 -> 00:00:29,960] 就是一个商用上已经make sense的一个地步
[00:00:00,000 -> 00:00:01,399] 另外一個就是說
[00:00:01,399 -> 00:00:02,919] 那我們應該怎麼看待
[00:00:02,919 -> 00:00:03,799] hallucination這個問題
[00:00:03,799 -> 00:00:04,559] 就沒有hallucination
[00:00:04,559 -> 00:00:05,879] 就真的是更好嗎
[00:00:05,879 -> 00:00:08,279] 因為如果從人的角度來說
[00:00:08,279 -> 00:00:09,119] 我們覺得很多時候
[00:00:09,119 -> 00:00:10,160] 可能hallucination也是
[00:00:10,160 -> 00:00:11,519] 說實在人也有hallucination
[00:00:11,519 -> 00:00:12,960] 甚至可能hallucination也是
[00:00:12,960 -> 00:00:15,160] 我們有creativity
[00:00:15,160 -> 00:00:16,239] 就是創造力的
[00:00:16,239 -> 00:00:19,320] 是不是它也是一個硬幣的兩面
[00:00:19,320 -> 00:00:20,719] 那我們應該怎麼看待
[00:00:20,719 -> 00:00:23,960] 這個hallucination的這個問題
[00:00:23,960 -> 00:00:24,679] 所以
[00:00:24,679 -> 00:00:25,559] 要我可以嗎 現在
[00:00:25,559 -> 00:00:27,079] 你先說吧
[00:00:27,079 -> 00:00:28,359] 就是我剛笑
[00:00:00,000 -> 00:00:02,000] 就是因為人帶著hallucination太多了
[00:00:02,000 -> 00:00:04,599] 我覺得99%說
[00:00:05,299 -> 00:00:07,400] GPT hallucination很有問題的人
[00:00:07,400 -> 00:00:08,800] 他們都在hallucination
[00:00:09,199 -> 00:00:11,400] 對 然後我自己還開玩笑說
[00:00:11,400 -> 00:00:13,599] 我這個項鏈戴在身上了以後
[00:00:13,599 -> 00:00:15,199] 可以prompting能力加5
[00:00:15,199 -> 00:00:16,500] 但是hallucination加10
[00:00:16,899 -> 00:00:18,000] 但是因為過於自信
[00:00:18,000 -> 00:00:19,600] 所以反而會顯得
[00:00:20,000 -> 00:00:21,399] 就是流量也可以增加
[00:00:21,399 -> 00:00:22,699] 因為hallucination太多
[00:00:23,199 -> 00:00:24,899] 包括我有一個很具體的例子
[00:00:24,899 -> 00:00:27,600] 就是我當時膝蓋踢球斷了
[00:00:27,899 -> 00:00:29,199] 然後其實第一次的時候
[00:00:00,000 -> 00:00:01,219] 只是一個小的撕裂
[00:00:01,219 -> 00:00:02,220] 然後去找醫生
[00:00:02,220 -> 00:00:04,219] 醫生說拍個X光
[00:00:04,219 -> 00:00:05,459] 沒事 骨頭沒事
[00:00:05,459 -> 00:00:07,459] 那你就休息兩個月就可以好
[00:00:07,459 -> 00:00:08,419] 我休息了兩個月
[00:00:08,419 -> 00:00:10,259] 然後再去踢5分鐘又斷了
[00:00:10,259 -> 00:00:10,759] 為什麼
[00:00:10,759 -> 00:00:12,759] 因為你需要拍那個MCR
[00:00:12,759 -> 00:00:14,460] 反而就是褐色公證
[00:00:14,460 -> 00:00:15,759] 你才能看到韌帶的情況
[00:00:15,759 -> 00:00:17,059] 韌帶其實當時已經撕裂了
[00:00:17,059 -> 00:00:19,559] 那我應該休息半年以上才行
[00:00:19,559 -> 00:00:21,059] 這就是一個醫生hallucinate
[00:00:21,059 -> 00:00:22,519] 然後導致了問題
[00:00:22,519 -> 00:00:24,760] 然後他也沒有任何的責任
[00:00:24,760 -> 00:00:27,519] 就是人類說什麼mission critical
[00:00:27,519 -> 00:00:28,519] 但是我覺得就是
[00:00:00,000 -> 00:00:02,500] 你如果看这个mission critical的上面各种各样的人
[00:00:02,520 -> 00:00:03,859] 能保证他们不hallucination吗
[00:00:03,879 -> 00:00:04,700] 我觉得没有
[00:00:05,139 -> 00:00:07,700] 所以我觉得那我们先不如说一下
[00:00:07,700 -> 00:00:09,539] 你认为什么叫hallucination
[00:00:10,460 -> 00:00:12,580] 就是他ok
[00:00:13,140 -> 00:00:14,339] 这是个好问题
[00:00:14,339 -> 00:00:19,460] hallucination是我的这个答案是错的
[00:00:19,460 -> 00:00:21,100] 或者说不是最好的情况下
[00:00:21,100 -> 00:00:22,379] 我自我不知道
[00:00:22,379 -> 00:00:23,780] 我以为它是对的
[00:00:23,780 -> 00:00:24,899] 或者说是最好的
[00:00:25,179 -> 00:00:29,300] 那我们传统的积极学习的模型都会经常会出现
[00:00:00,000 -> 00:00:05,599] 比如说我们给了一个很高的confidence level
[00:00:05,599 -> 00:00:06,679] 然后是错的
[00:00:06,679 -> 00:00:08,279] 你认为这个是hallucination吗
[00:00:08,279 -> 00:00:09,000] 是
[00:00:09,720 -> 00:00:11,640] 还是你认为这只是犯了一个错误
[00:00:12,759 -> 00:00:13,880] 就是统计上的错误
[00:00:13,880 -> 00:00:14,919] 比如说我们举个例子
[00:00:15,400 -> 00:00:18,199] 这个东西是一个student t-test
[00:00:18,600 -> 00:00:20,640] 它的一个p-value达到这个东西
[00:00:20,640 -> 00:00:21,920] 但它实际上并不是
[00:00:21,920 -> 00:00:22,800] 明白了
[00:00:22,800 -> 00:00:25,160] 那你认为你做的这个决定
[00:00:25,160 -> 00:00:27,320] 说他们俩你reject或者accept
[00:00:00,000 -> 00:00:02,879] 这个non-hypothesis是不是hallucination
[00:00:02,879 -> 00:00:03,359] 不是
[00:00:03,799 -> 00:00:07,000] 不是因为它是带着就是它的p-value
[00:00:07,000 -> 00:00:07,440] 对吧
[00:00:07,440 -> 00:00:08,720] p-value的意义就是说
[00:00:08,720 -> 00:00:09,759] 就是代表你很
[00:00:09,759 -> 00:00:10,759] 那就是
[00:00:10,759 -> 00:00:12,080] 但你根据这个p-value
[00:00:12,080 -> 00:00:13,679] 你很有信心
[00:00:14,400 -> 00:00:15,960] 然后你做出了一个错误的决定
[00:00:15,960 -> 00:00:17,440] 你认为你在hallucinate吗
[00:00:17,600 -> 00:00:18,079] 不觉得
[00:00:18,079 -> 00:00:20,399] 我觉得我的信心是带probability的
[00:00:20,399 -> 00:00:22,879] 就是我接受它出1%的错误
[00:00:22,879 -> 00:00:24,440] 模型也是这么觉得的
[00:00:24,559 -> 00:00:25,000] 对
[00:00:25,000 -> 00:00:25,920] 对啊
[00:00:25,920 -> 00:00:28,920] 模型在每一次生成一个新的token的时候
[00:00:00,000 -> 00:00:02,879] 它其实都会生成一个概率对吗
[00:00:02,879 -> 00:00:04,360] 就是说我有多少个概率
[00:00:04,360 -> 00:00:06,599] 生成的下一个token是这个token
[00:00:06,599 -> 00:00:08,160] 然后他犯了错误
[00:00:08,160 -> 00:00:10,160] 你为什么称他叫hallucination
[00:00:10,160 -> 00:00:12,439] 我觉得孟大军他是很自信的
[00:00:12,439 -> 00:00:12,720] 对吧
[00:00:12,720 -> 00:00:13,880] 很一本正经的
[00:00:13,880 -> 00:00:14,160] 很好的问题
[00:00:14,160 -> 00:00:14,880] 我没有这么想过
[00:00:14,880 -> 00:00:16,320] 但是我现在想的话就是说
[00:00:16,320 -> 00:00:17,120] 模型知道
[00:00:17,120 -> 00:00:18,000] 但是人不知道
[00:00:18,000 -> 00:00:18,960] 人很多
[00:00:18,960 -> 00:00:21,679] 就是人为什么我们会用hallucination去称这件事情
[00:00:21,679 -> 00:00:23,280] 因为它其实是一个人的行为
[00:00:23,280 -> 00:00:24,640] 人为什么会有这种行为呢
[00:00:24,640 -> 00:00:28,280] 因为人很多时候把opinion和facts给混淆起来
[00:00:00,000 -> 00:00:03,000] 把自己错误的观点非常有自信的给说出来
[00:00:03,000 -> 00:00:06,759] 然后他在用这个观点去说的时候
[00:00:06,759 -> 00:00:10,480] 其实imply的是我对这件事情的probability非常之高
[00:00:10,480 -> 00:00:11,839] 实际上可能非常之低
[00:00:11,839 -> 00:00:15,000] 但是Chai TPT在跟人进行alignment的时候
[00:00:15,000 -> 00:00:18,800] 他在学习人类说话的时候
[00:00:18,800 -> 00:00:22,320] 他哪怕对这件事情assign的事实概率是30%
[00:00:22,320 -> 00:00:25,879] 但是他的语气给你的感受是90%
[00:00:25,879 -> 00:00:28,320] 那你觉得如果他同样说这句话
[00:00:00,000 -> 00:00:02,839] 但是他说我其实可能没有那么确定
[00:00:02,839 -> 00:00:04,719] 我觉得加一个maybe
[00:00:04,719 -> 00:00:06,679] 你会觉得这个就不是hallucination了吗
[00:00:07,360 -> 00:00:09,560] 我觉得从这个就是这个词
[00:00:09,560 -> 00:00:11,679] 我没有精确定义的情况下
[00:00:11,679 -> 00:00:15,080] 我觉得我可以接受它不是hallucination
[00:00:15,080 -> 00:00:16,120] 明白你的意思
[00:00:16,120 -> 00:00:18,039] 以及我从头到尾我都觉得
[00:00:18,039 -> 00:00:20,359] CHPT表现出来很多hallucination
[00:00:20,359 -> 00:00:22,199] 比我见到的人真的是少太多了
[00:00:22,199 -> 00:00:23,719] 就是对啊
[00:00:23,719 -> 00:00:25,280] 就比如说李彦宏说什么
[00:00:25,280 -> 00:00:26,440] 我们两个月可以做出来
[00:00:26,440 -> 00:00:27,480] 我就还是两个周
[00:00:27,480 -> 00:00:28,280] 我就他说就
[00:00:00,000 -> 00:00:02,000] 就是人類的
[00:00:02,000 -> 00:00:04,000] 或者我看網上的新聞
[00:00:04,000 -> 00:00:05,000] 對吧 天啊
[00:00:05,000 -> 00:00:07,000] 我覺得我們可以舉幾個
[00:00:07,000 -> 00:00:09,000] 就是關於這個所謂
[00:00:09,000 -> 00:00:10,000] hallucination的幾個
[00:00:10,000 -> 00:00:11,000] 比較常見的例子
[00:00:11,000 -> 00:00:12,000] 我覺得一個就是
[00:00:12,000 -> 00:00:13,000] 比如說我們這樣
[00:00:13,000 -> 00:00:14,000] 但是我是站在就是
[00:00:14,000 -> 00:00:16,000] hallucination不是那麼重要
[00:00:16,000 -> 00:00:19,000] 和這是一個可以被技術解決
[00:00:19,000 -> 00:00:21,000] 就是你去適配不同的situation
[00:00:21,000 -> 00:00:24,000] 去做不同的解決方案的這一派
[00:00:24,000 -> 00:00:25,000] 所以你可能你跟我
[00:00:25,000 -> 00:00:26,000] 我沒有
[00:00:26,000 -> 00:00:27,000] 我其實沒有什麼特別的觀點
[00:00:27,000 -> 00:00:29,000] 就是說hallucination好還是不好
[00:00:00,000 -> 00:00:02,680] 或者是能不能消灭
[00:00:02,680 -> 00:00:05,040] 其实我并没有一个特别确切的观点
[00:00:05,040 -> 00:00:06,080] 我只是有一些
[00:00:06,080 -> 00:00:09,439] 当然了我们讨论的就是探索这些想法
[00:00:09,439 -> 00:00:11,960] 我自己看到就是我们认为为什么说
[00:00:11,960 -> 00:00:13,119] HelloSense非常的明显
[00:00:13,119 -> 00:00:14,080] 举个例子
[00:00:14,080 -> 00:00:14,759] 就是比如说
[00:00:14,759 -> 00:00:17,079] AutoGPT或者LineChain在完成这件事情的时候
[00:00:17,079 -> 00:00:19,480] 它会去生成这个东西的
[00:00:19,480 -> 00:00:23,359] 这个这个tool或者这个function所需要的input
[00:00:23,359 -> 00:00:23,719] 对吧
[00:00:23,719 -> 00:00:25,559] 它需要的输入应该是什么
[00:00:25,559 -> 00:00:27,600] 很多时候它有的时候不能说很多时候
[00:00:27,600 -> 00:00:29,800] 就是有过人们发现过
[00:00:00,000 -> 00:00:02,500] 就是他会生成一些完全不存在的东西
[00:00:02,500 -> 00:00:04,299] 比如说医疗的paper
[00:00:04,299 -> 00:00:05,299] 根本就没有这个paper
[00:00:05,299 -> 00:00:07,000] 对就是什么johndoe到
[00:00:07,000 -> 00:00:08,000] 直播到com
[00:00:08,000 -> 00:00:10,900] 他会就真的是直接就说是johndoe
[00:00:10,900 -> 00:00:13,900] 然后就他不会说不是这是一种类型
[00:00:13,900 -> 00:00:15,099] 还有一种就是像你说的
[00:00:15,300 -> 00:00:18,000] 那种我在描述一件事情的时候
[00:00:18,500 -> 00:00:19,899] 不光我的观点是错的
[00:00:20,100 -> 00:00:21,399] 我还说了一些论据
[00:00:21,399 -> 00:00:22,699] 这些论据本身是不存在的
[00:00:22,699 -> 00:00:25,100] 对那这种也是合乳死年史对吧
[00:00:25,100 -> 00:00:27,100] 然后我觉得你说的第二种
[00:00:27,100 -> 00:00:28,899] 更像是人也会发生了一些问题
[00:00:28,899 -> 00:00:29,899] 就像记忆的时候
[00:00:00,000 -> 00:00:01,919] 我记得好像有这么一篇文章
[00:00:01,919 -> 00:00:03,080] 去说这件事情
[00:00:03,080 -> 00:00:04,080] 但实际上并没有
[00:00:04,080 -> 00:00:05,960] 我只是把两篇文章混在一起了
[00:00:05,960 -> 00:00:06,480] 对
[00:00:06,480 -> 00:00:07,400] 但他我非常
[00:00:07,400 -> 00:00:08,560] 那我就说
[00:00:08,560 -> 00:00:09,880] 就因为这么一篇文章
[00:00:09,880 -> 00:00:11,359] Hello, CNN这个词本来就是一个
[00:00:11,359 -> 00:00:12,199] Human Behavior
[00:00:12,199 -> 00:00:14,279] 然后Behavior就是我看到的
[00:00:14,279 -> 00:00:15,839] 是出现小人了是吧
[00:00:15,839 -> 00:00:16,320] 对
[00:00:16,320 -> 00:00:17,000] 就是这样的
[00:00:17,000 -> 00:00:17,800] 然后我
[00:00:18,039 -> 00:00:19,160] 他这个词就是
[00:00:19,160 -> 00:00:21,199] 我看到的出现小人了
[00:00:21,199 -> 00:00:21,480] 对
[00:00:21,480 -> 00:00:22,839] 就这个意思
[00:00:22,839 -> 00:00:23,440] 嗯
[00:00:23,800 -> 00:00:25,679] 那这个东西到底就是
[00:00:25,679 -> 00:00:28,440] 我觉得可能大家觉得它不好的原因
[00:00:00,000 -> 00:00:03,359] 就是因为我们对机器的期待并不希望他们做一些
[00:00:03,359 -> 00:00:04,719] creativity task
[00:00:04,719 -> 00:00:06,919] 我们对机器的期待和习惯
[00:00:06,919 -> 00:00:11,320] 对我们和他们的习惯更多的是说机器就是有确定性的
[00:00:11,320 -> 00:00:12,640] 他在做的这些事情
[00:00:12,640 -> 00:00:15,439] 我用电脑去完成这件事情
[00:00:15,439 -> 00:00:17,600] 就是因为人也可以做
[00:00:17,600 -> 00:00:19,079] 但你做的效率更高
[00:00:19,079 -> 00:00:20,320] 而且你不会犯错
[00:00:20,320 -> 00:00:22,160] 所以我才让你去用电脑
[00:00:22,160 -> 00:00:25,399] 这可能是我们之前使用一些机器或者工具的习惯
[00:00:25,399 -> 00:00:26,600] 这导致我发现
[00:00:26,600 -> 00:00:28,280] 我现在用到这个工具
[00:00:00,000 -> 00:00:02,799] 我有一定概率這個工具會失敗
[00:00:02,799 -> 00:00:05,500] 變成了一件不是很好接受的事情
