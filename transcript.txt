[00:00:00,000 -> 00:00:02,560] 來分享他們的見解和經驗
[00:00:02,560 -> 00:00:06,400] 討論ChairGBT在不同領域中的應用和挑戰
[00:00:06,400 -> 00:00:09,480] 以及未來發展方向的可能性
[00:00:09,480 -> 00:00:13,039] 在這裡我希望大家能夠積極參與
[00:00:13,039 -> 00:00:15,039] 共同學習與交流
[00:00:15,039 -> 00:00:17,039] 謝謝大家
[00:00:17,039 -> 00:00:19,039] 開個玩笑
[00:00:19,039 -> 00:00:21,920] 那麼我剛剛念的就是ChairGBT
[00:00:21,920 -> 00:00:24,879] 為這一場分享會做的開場準備
[00:00:24,879 -> 00:00:28,679] 接下來就是我真人的開場白了
[00:00:00,000 -> 00:00:01,800] 就是我叫周恩傑
[00:00:01,800 -> 00:00:04,299] 騰訊IEG讀書協會
[00:00:04,299 -> 00:00:10,199] 終於就是邀請到了一位非常不錯的一個業內大咖
[00:00:10,199 -> 00:00:11,199] 來為我們做分享
[00:00:11,199 -> 00:00:12,699] 這是我們第一次嘗試
[00:00:12,699 -> 00:00:20,699] 那麼孫一正是我們騰訊IEG數據增長數據科學的副總監
[00:00:20,699 -> 00:00:22,199] 然後在B站上面的話
[00:00:22,199 -> 00:00:25,800] 會也有一個帳號叫科代表立正
[00:00:25,800 -> 00:00:29,300] 那麼大家可以就叫科代表立正
[00:00:00,000 -> 00:00:06,599] 然後非常感謝大佬能為我們做一個這樣的非常寶貴的分享
[00:00:06,599 -> 00:00:11,000] 然後話不多說就是我們即將開始
[00:00:11,000 -> 00:00:15,000] 那麼在會前的話我們說一下現場的一些流程跟秩序
[00:00:15,000 -> 00:00:18,600] 就是我們中間有十分鐘的中場休息
[00:00:18,600 -> 00:00:21,399] 這個的話由講師來自由去安排
[00:00:21,399 -> 00:00:25,199] 然後結束的時間也可以有一個QA環節
[00:00:25,199 -> 00:00:27,199] 就是大家有感興趣的問題的話
[00:00:00,000 -> 00:00:03,799] 可以自由的跟我們講師做一個互動
[00:00:05,400 -> 00:00:07,559] 然後茶水間的話有水和咖啡
[00:00:07,559 -> 00:00:09,800] 大家可以就是去自取
[00:00:10,160 -> 00:00:12,800] 會場期間的話也可以親身的走動
[00:00:13,400 -> 00:00:13,900] OK
[00:00:14,400 -> 00:00:14,900] 好
[00:00:15,300 -> 00:00:15,800] 有請
[00:00:20,800 -> 00:00:21,500] 大家好
[00:00:21,600 -> 00:00:23,559] 對我剛剛在座位下面聽
[00:00:23,559 -> 00:00:25,899] 我想我怎麼有精神分裂了嗎
[00:00:25,899 -> 00:00:28,300] 怎麼我請他們專家們
[00:00:28,300 -> 00:00:29,960] 然後我一個人
[00:00:00,000 -> 00:00:03,140] 但是你說話得罪了一些人
[00:00:03,140 -> 00:00:05,500] 你剛剛說了一個終於請來了一位
[00:00:05,500 -> 00:00:09,580] 就顯得好像之前請的那些同學不是很行的樣子
[00:00:09,580 -> 00:00:13,419] 所以說不知道陳水扁聽的情商高還是你的情商高一點
[00:00:13,419 -> 00:00:16,859] 我今天想這個就是
[00:00:16,859 -> 00:00:19,820] 在群裡也有分享我寫的那個文章嘛
[00:00:19,820 -> 00:00:21,660] 其實我因為自己有一個頻道
[00:00:21,660 -> 00:00:22,940] 然後自己平時也愛寫作
[00:00:22,940 -> 00:00:24,379] 然後包括寫了一些文章
[00:00:24,379 -> 00:00:27,019] 我覺得科普校的東西其實我已經講得差不多了
[00:00:00,000 -> 00:00:04,080] 就是我觉得如果花大家的时间到了现场
[00:00:04,120 -> 00:00:07,440] 我再把我之前说的那些东西重新再给大家讲一遍
[00:00:07,480 -> 00:00:08,839] 我自己也觉得挺无聊的
[00:00:08,839 -> 00:00:11,359] 然后大家其实可以回去看
[00:00:11,400 -> 00:00:14,960] 也没有必要在这再去听我去讲一遍
[00:00:15,080 -> 00:00:18,679] 但是我也相信很多同学是不会去看那个文章的
[00:00:18,679 -> 00:00:22,640] 所以说我们想照顾一下大家的同学们的感觉
[00:00:22,800 -> 00:00:25,239] 那我想就是拆GPT是一个什么形式呢
[00:00:25,440 -> 00:00:27,719] 拆GPT它是背后有一个大模型
[00:00:27,760 -> 00:00:29,280] 然后这个大模型有很多知识
[00:00:00,000 -> 00:00:01,360] 然後他有很多能力
[00:00:01,360 -> 00:00:03,399] 大家通過對話的方式去激活
[00:00:03,399 -> 00:00:05,240] 然後得到你想要的東西對不對
[00:00:05,240 -> 00:00:07,599] 我今天在這就辦一個ChatsPT
[00:00:07,599 -> 00:00:09,519] 就是我會稍稍講一下
[00:00:09,519 -> 00:00:13,119] 就是我有一個更短的那個
[00:00:13,119 -> 00:00:15,800] 一個英文的就是PPT
[00:00:15,800 -> 00:00:19,359] 我會大概花10到15分鐘的時間講一下那個PPT
[00:00:19,359 -> 00:00:20,480] 然後講完了以後呢
[00:00:20,480 -> 00:00:21,879] 我就坐在這裡了
[00:00:21,879 -> 00:00:23,120] 然後我就是一個ChatsPT了
[00:00:23,120 -> 00:00:25,199] 然後請大家來問我問題
[00:00:25,199 -> 00:00:26,719] 包括我那個文檔裡邊
[00:00:00,000 -> 00:00:02,359] 我其實也有說在這個
[00:00:02,359 -> 00:00:03,399] Chai GPT 紀元
[00:00:03,399 -> 00:00:04,799] 現在叫AGI紀元吧
[00:00:04,799 -> 00:00:06,960] 提問的能力會是一個非常非常重要的能力
[00:00:06,960 -> 00:00:09,480] 所以說我們期待大家提一些問題
[00:00:09,480 -> 00:00:11,880] 不管這個問題是什麼樣子的
[00:00:11,880 -> 00:00:13,839] 都我相信啊
[00:00:13,839 -> 00:00:15,240] 就是只要你提問
[00:00:15,240 -> 00:00:16,199] 你今天肯定是賺的
[00:00:16,199 -> 00:00:17,039] 就是你不提問的話
[00:00:17,039 -> 00:00:18,480] 這個問題在你的肚子裡邊
[00:00:18,480 -> 00:00:19,839] 然後你現在提出來了
[00:00:19,839 -> 00:00:22,239] 你起碼可以幫助你自己把這個問題想得清楚一點吧
[00:00:22,239 -> 00:00:23,800] 就是我的回答不一定有用
[00:00:23,800 -> 00:00:26,000] 但是你去提問了這件事情
[00:00:26,000 -> 00:00:28,559] 對你來說應該都是有好處的
[00:00:28,559 -> 00:00:29,760] 然後再多說一點點
[00:00:00,000 -> 00:00:04,799] 就是我也是之前我在這個公司裡邊也試過這樣的形式啊
[00:00:04,799 -> 00:00:07,200] 我在IEG講過一個DS的課
[00:00:07,200 -> 00:00:12,599] 然後我在KM講過一個就是直播講了一個寫作的課
[00:00:12,599 -> 00:00:19,199] 那兩個課我都試圖說我先把寫的東西給大家寫清楚
[00:00:19,199 -> 00:00:21,600] 然後大家之前看一下
[00:00:21,600 -> 00:00:24,000] 我甚至在這個課堂為了照顧大家不看
[00:00:24,000 -> 00:00:27,699] 我說我們在這個課的一開始的10分鐘15分鐘
[00:00:27,699 -> 00:00:29,399] 我們一起把這個文章讀完
[00:00:00,000 -> 00:00:01,000] 讀完了以後呢
[00:00:01,000 -> 00:00:03,299] 然後我們再根據文章的內容去提問
[00:00:03,299 -> 00:00:04,700] 我就不需要從頭過一遍了
[00:00:04,700 -> 00:00:06,900] 結果後來就得到了很多差評
[00:00:06,900 -> 00:00:08,500] 就大家說這個人怎麼這樣子
[00:00:08,500 -> 00:00:11,099] 就是他不講這個文章
[00:00:11,099 -> 00:00:12,300] 然後就讓我們直接問
[00:00:12,300 -> 00:00:13,300] 我們也不知道問什麼
[00:00:13,300 -> 00:00:17,100] 所以我就稍稍再多說一點點
[00:00:17,100 -> 00:00:19,600] 就是我為什麼就是在課前
[00:00:19,600 -> 00:00:21,399] 大家如果看時間的話就是兩分鐘
[00:00:21,399 -> 00:00:22,699] 可能上課之前兩分鐘
[00:00:22,699 -> 00:00:23,899] 我突然想起來了一件事
[00:00:23,899 -> 00:00:25,399] 就是我自己的
[00:00:25,399 -> 00:00:27,699] 我自己的這個視頻裡面
[00:00:00,000 -> 00:00:03,799] 我覺得頻道裡面我覺得有這樣一個視頻是很有意思的
[00:00:03,799 -> 00:00:06,000] 或者說我挺推薦大家看的
[00:00:06,000 -> 00:00:09,000] 他也被很多大佬們推薦過
[00:00:09,000 -> 00:00:10,800] 甚至就是他的播放量不高啊
[00:00:10,800 -> 00:00:13,099] 但是比如說兩周之前
[00:00:13,099 -> 00:00:16,000] 創夢天地就是在那個A1的
[00:00:16,000 -> 00:00:19,500] 就是那個A洞的一個公司
[00:00:19,500 -> 00:00:21,600] 他們CEO找我聊天
[00:00:21,600 -> 00:00:23,399] 然後說他從來不發朋友圈
[00:00:23,399 -> 00:00:26,800] 但是他分享了這個視頻給他們的員工看
[00:00:26,800 -> 00:00:28,399] 和發在他的朋友圈裡面
[00:00:00,000 -> 00:00:01,639] 就是他還是有點意思的
[00:00:01,639 -> 00:00:02,439] 他有點的
[00:00:02,439 -> 00:00:03,759] 他有點意思呢
[00:00:03,759 -> 00:00:07,559] 就是說我們在面對自己的每一個行為的時候
[00:00:07,599 -> 00:00:11,800] 其實都可以抱著一個固定思維或者成長思維去看這件事
[00:00:12,039 -> 00:00:14,720] 比如說我們如果說是一個固定思維去
[00:00:15,000 -> 00:00:16,760] 看自己的這個提問會覺得啊
[00:00:16,760 -> 00:00:18,440] 我是不是問了一個特別蠢的問題
[00:00:18,559 -> 00:00:20,760] 然後是不是別人會覺得我水平不行
[00:00:20,960 -> 00:00:22,920] 但是其實沒有人會關注的
[00:00:22,920 -> 00:00:23,879] 沒有人會記得的
[00:00:23,920 -> 00:00:25,879] 就是如果大家去想一下
[00:00:25,960 -> 00:00:27,079] 你之前開了會
[00:00:27,120 -> 00:00:28,079] 如果有一個人問的問題
[00:00:28,079 -> 00:00:29,480] 你還記不記得這個問題是誰問的
[00:00:00,000 -> 00:00:02,120] 不管他多麼的出彩和多麼的愚蠢
[00:00:02,120 -> 00:00:03,319] 你最後都不會記得的
[00:00:04,000 -> 00:00:05,759] 但成長性思維就是說
[00:00:05,759 -> 00:00:06,719] 我來了
[00:00:06,759 -> 00:00:07,919] 我花時間在這了
[00:00:07,919 -> 00:00:09,919] 我用了這個機會去問了一個問題
[00:00:09,919 -> 00:00:11,480] 然後這個問題對我來說是有用的
[00:00:11,480 -> 00:00:12,679] 那你就得到了成長
[00:00:12,759 -> 00:00:16,640] 這其實是一個我覺得起碼可以讓自己更快樂一點的方式
[00:00:17,000 -> 00:00:21,160] 然後第二個是我接下來的那個視頻的副標題
[00:00:21,199 -> 00:00:23,800] 那個不是接下來那個PPT的副標題
[00:00:24,000 -> 00:00:28,280] 那個PPT的副標題是我們三年之內是不是要去做一個喬久石
[00:00:28,480 -> 00:00:29,920] 我是認真的思考了這件事情
[00:00:00,000 -> 00:00:02,279] 所以说就是我去采访了乔九师
[00:00:02,279 -> 00:00:05,000] 然后去了解这个行业的各种各样的
[00:00:05,280 -> 00:00:06,599] 他到底是要干什么呀
[00:00:06,599 -> 00:00:08,240] 怎么样子做一个好的乔九师
[00:00:08,519 -> 00:00:11,400] 我是真的在很认真的去想这件事情啊
[00:00:11,400 -> 00:00:14,560] 所以说大家也可以就是
[00:00:16,199 -> 00:00:19,640] 就是现在昨天我还听了一个沐瑶的播客
[00:00:19,879 -> 00:00:23,199] 他说他观察到现在很多人在聊ChattyPT
[00:00:23,199 -> 00:00:24,920] 是一个看热闹的心态
[00:00:25,320 -> 00:00:27,359] 就是无论你是焦虑也好
[00:00:27,359 -> 00:00:28,839] 或者说是想了解也罢
[00:00:00,000 -> 00:00:02,799] 或者说是看自己对自己有什么意义
[00:00:02,799 -> 00:00:04,799] 其实还是抱着一个看热闹的心态
[00:00:04,799 -> 00:00:08,279] 没有觉得这个东西真真切切的跟自己那么有关
[00:00:08,279 -> 00:00:09,439] 而且在看热闹的过程中
[00:00:09,439 -> 00:00:12,960] 大家也会借题发挥的去抒发一些自己的焦虑啊
[00:00:12,960 -> 00:00:13,919] 对现实的不满啊
[00:00:13,919 -> 00:00:15,080] 或者说是一些想法
[00:00:15,880 -> 00:00:16,920] 但是这不是一个热闹
[00:00:16,920 -> 00:00:18,519] 这个是真实发生的事儿
[00:00:19,359 -> 00:00:20,960] 我反正是去
[00:00:21,160 -> 00:00:24,399] 就是我会真的去认真的思考这件事情的
[00:00:24,399 -> 00:00:25,199] 所以说
[00:00:26,000 -> 00:00:26,800] 也希望吧
[00:00:26,800 -> 00:00:28,160] 就是今天这个机会
[00:00:00,000 -> 00:00:02,299] 大家可以打破一點這種看熱鬧的心態
[00:00:02,299 -> 00:00:05,259] 去認真的去思考和認真的
[00:00:05,259 -> 00:00:06,540] 不管你的思考是什麼
[00:00:06,540 -> 00:00:08,460] 我覺得行動總比不行動好
[00:00:08,859 -> 00:00:10,539] 第三個呢是
[00:00:10,539 -> 00:00:12,859] 我在會場之前
[00:00:12,859 -> 00:00:14,019] 我記錯時間了
[00:00:14,019 -> 00:00:15,140] 我以為是三點鐘開始
[00:00:15,140 -> 00:00:16,339] 所以說我就很早就來了
[00:00:16,339 -> 00:00:17,899] 然後很早就來了就在那無聊
[00:00:17,899 -> 00:00:19,579] 然後就看這個第三個視頻
[00:00:19,579 -> 00:00:21,140] 這段視頻我沒有看完
[00:00:21,140 -> 00:00:23,059] 但是他說的這個內容
[00:00:23,059 -> 00:00:25,019] 我覺得就非常非常的有共鳴
[00:00:25,019 -> 00:00:28,019] 他說的他當中間引用了一句話
[00:00:00,000 -> 00:00:03,480] 就是說某個另外的大咖說的一句話是
[00:00:03,480 -> 00:00:06,879] 一百個說話的人不如一個思考的人
[00:00:06,879 -> 00:00:10,320] 但是一千個思考的人不如一個看見的人
[00:00:10,320 -> 00:00:14,039] 就是他覺得一個思想者能做的最重要的事情
[00:00:14,039 -> 00:00:15,919] 就是看到這個世界真實的情況
[00:00:15,919 -> 00:00:18,800] 並且把這個真實的情況給人風不動地說出來
[00:00:20,679 -> 00:00:22,000] 我在努力做這件事
[00:00:22,000 -> 00:00:24,719] 但是我也希望大家就是和我一起做這件事
[00:00:24,719 -> 00:00:27,519] 我希望今天的這個對話吧
[00:00:27,519 -> 00:00:28,879] 或者說是提問
[00:00:00,000 -> 00:00:03,080] 大家可以和我一起來去做這件事情
[00:00:04,719 -> 00:00:06,360] 這就是一個簡單的開場
[00:00:11,919 -> 00:00:13,000] 這是那個文章
[00:00:14,919 -> 00:00:15,400] 好的
[00:00:15,400 -> 00:00:17,000] 這個是PPT
[00:00:17,000 -> 00:00:22,079] 這個PPT的意義就是給大家來一個簡單的科普和介紹一下我的思考框架
[00:00:22,079 -> 00:00:23,440] 這些東西都會發在
[00:00:23,440 -> 00:00:26,640] 就是在那個群裡面是發了文章的
[00:00:00,000 -> 00:00:03,600] 然後文章的第一個第一行就是一個PPT的鏈接
[00:00:03,600 -> 00:00:04,799] 所以說大家都有這個PPT了
[00:00:06,200 -> 00:00:08,800] 對就是五個關於ChatshipT的問題
[00:00:09,199 -> 00:00:12,160] 那麼為什麼放英文
[00:00:12,160 -> 00:00:15,400] 因為很多時候發現討論還是需要在跟
[00:00:15,900 -> 00:00:17,500] 在一個英文的語境下討論
[00:00:17,899 -> 00:00:18,800] 對
[00:00:19,800 -> 00:00:21,399] 五個問題
[00:00:22,000 -> 00:00:23,500] 其實就這樣五個
[00:00:23,500 -> 00:00:25,699] 第一個就是我們要理解ChatshipT
[00:00:25,699 -> 00:00:26,500] 它到底是什麼
[00:00:26,500 -> 00:00:28,800] 它到底是一個平平無奇的技術
[00:00:00,000 -> 00:00:02,200] 還是一個真正不一樣的技術
[00:00:02,200 -> 00:00:04,040] 而且這個問題其實是很多大佬
[00:00:04,040 -> 00:00:05,759] 和大佬之間是有分歧的
[00:00:05,759 -> 00:00:08,000] 就是樂坤等等吧
[00:00:08,000 -> 00:00:09,839] 包括我自己認識的很多大佬
[00:00:09,839 -> 00:00:12,160] 他們都會對這件事情有不同的理解
[00:00:12,160 -> 00:00:13,839] 可是明顯大家也看到了
[00:00:13,839 -> 00:00:15,000] 就是越來越多的大佬
[00:00:15,000 -> 00:00:17,480] 會覺得它是一個很不一樣的技術突破
[00:00:17,480 -> 00:00:18,600] 所以說這個問題
[00:00:18,600 -> 00:00:20,800] 我們要有一個自己的判斷
[00:00:20,800 -> 00:00:21,640] 那第二個問題呢
[00:00:21,640 -> 00:00:23,559] 就是如何理解它的意義
[00:00:23,559 -> 00:00:26,199] 就是大家會打很多類比
[00:00:26,199 -> 00:00:27,000] 當然這些類比
[00:00:27,000 -> 00:00:28,800] 你如果不是真的理解它的意義的話
[00:00:28,800 -> 00:00:29,960] 類比是沒有意義的
[00:00:00,000 -> 00:00:01,600] 类比本身是没有意义的
[00:00:01,600 -> 00:00:04,480] 就比如说有的人会说这是第四次工业革命
[00:00:04,480 -> 00:00:06,480] 但是我们这句话听了100遍了对吧
[00:00:06,480 -> 00:00:08,359] 就是什么web story是第四次工业革命
[00:00:08,359 -> 00:00:09,439] VR是第四次工业革命
[00:00:09,439 -> 00:00:11,519] 然后所有东西都是第四次工业革命
[00:00:11,519 -> 00:00:13,759] 那到底这个是不是第四次工业革命
[00:00:13,759 -> 00:00:15,800] 或者说前三次工业革命到底是什么
[00:00:15,800 -> 00:00:18,800] 我觉得这个我们要去真正的去理解一下
[00:00:18,800 -> 00:00:22,359] 第三个呢就是它容不容易复现
[00:00:22,359 -> 00:00:23,960] 这个其实我不知道
[00:00:23,960 -> 00:00:26,039] 但是我会给大家说一下我的
[00:00:26,039 -> 00:00:29,000] 就是想这个问题的决策框架吧
[00:00:00,000 -> 00:00:02,000] 或者是決策數,就是我看到了什麼
[00:00:02,000 -> 00:00:03,339] 我會覺得它容易浮現
[00:00:03,339 -> 00:00:05,339] 但是我現在默認狀態是我覺得什麼
[00:00:05,339 -> 00:00:08,179] 第四個就是我們怎麼樣去使用它
[00:00:08,179 -> 00:00:11,720] 第五個就是人和Chai GPT到底有什麼區別
[00:00:13,720 -> 00:00:15,980] 這裡有一個最最基本的科普
[00:00:18,219 -> 00:00:20,519] 第一個就是Chai GPT它的原理是什麼
[00:00:20,519 -> 00:00:23,359] 它的原理是它的這個底層的大模型
[00:00:23,359 -> 00:00:27,440] 是一個叫Generative Autoregressive Large Language Model
[00:00:27,440 -> 00:00:29,440] 這幾個關鍵詞
[00:00:00,000 -> 00:00:01,639] Generative就是生成性
[00:00:01,639 -> 00:00:04,480] 生成性就是说它是在做一个生成任务
[00:00:04,480 -> 00:00:05,679] 我们看到了一个模型
[00:00:05,679 -> 00:00:07,120] 我们可以给它不同的任务
[00:00:07,120 -> 00:00:10,439] 比如说你把这个数据去进行分类
[00:00:10,439 -> 00:00:11,640] 这是一个分类模型
[00:00:11,640 -> 00:00:12,759] 你去理解它
[00:00:12,759 -> 00:00:14,000] 这是一个理解模型
[00:00:14,000 -> 00:00:16,239] 那ChHbt它是一个生成模型
[00:00:16,239 -> 00:00:18,239] 它所做的就是给你一堆数据以后
[00:00:18,239 -> 00:00:20,280] 你想方设法的去生成下一个词
[00:00:20,280 -> 00:00:21,280] 这个叫生成
[00:00:21,280 -> 00:00:24,199] 然后AutoRegressive是自回归
[00:00:24,199 -> 00:00:25,920] 就是说你生成了这个词以后
[00:00:25,920 -> 00:00:27,519] 你再把这些作为你的这个数据
[00:00:00,000 -> 00:00:02,960] 输入数据再去生成下一个词
[00:00:02,960 -> 00:00:05,480] 那这个大语言模型大家就知道了
[00:00:05,480 -> 00:00:06,839] 就是它数据量大参数量大
[00:00:06,839 -> 00:00:09,439] 它是一个非常大的关于语言的模型
[00:00:09,439 -> 00:00:10,800] 现在它有了多模态
[00:00:10,800 -> 00:00:12,359] 但是它的怎么说呢
[00:00:12,359 -> 00:00:15,000] 就是从历史或者它的重点来看的话
[00:00:15,000 -> 00:00:16,280] 它仍然是一个大语言模型
[00:00:18,640 -> 00:00:21,719] 第二个我觉得比较需要普及科普的点
[00:00:21,719 -> 00:00:24,199] 就是这件事情挺重要的
[00:00:24,199 -> 00:00:26,000] 所以说我还是决定把它拿出来
[00:00:26,239 -> 00:00:27,280] 跟大家讲一下
[00:00:27,280 -> 00:00:29,079] 就是虽然是个比较技术的东西
[00:00:00,000 -> 00:00:01,879] 但是還是值得一講
[00:00:01,879 -> 00:00:03,299] 就是過去的模型
[00:00:03,299 -> 00:00:06,459] 過去的模型你要去改變這個模型本身
[00:00:06,459 -> 00:00:07,219] 你要幹什麼
[00:00:07,219 -> 00:00:10,380] 你要去就是一個過去的模型
[00:00:10,380 -> 00:00:12,800] 你要去讓它做一個新任務
[00:00:12,800 -> 00:00:18,600] 你就需要讓這個模型改變自己
[00:00:18,600 -> 00:00:20,600] 就是我們比如說一個模型
[00:00:20,600 -> 00:00:22,260] 一個圖像識別的模型吧
[00:00:22,260 -> 00:00:26,280] 他說我們可以把現在這些同學們的臉給認出來
[00:00:26,280 -> 00:00:28,100] 那如果來了一個新的人怎麼辦
[00:00:28,100 -> 00:00:29,640] 你還是要給他標籤對吧
[00:00:00,000 -> 00:00:01,300] 你还是要给他标签
[00:00:01,300 -> 00:00:03,600] 然后他如果学习了人脸的特征
[00:00:03,600 -> 00:00:05,700] 你让他去认猴子的脸
[00:00:05,700 -> 00:00:07,799] 他还要去学习猴子脸的特征
[00:00:07,799 -> 00:00:11,099] 然后你这个拿人脸的模型去认猴子脸
[00:00:11,099 -> 00:00:13,099] 很有可能你训练出来这个模型
[00:00:13,099 -> 00:00:14,699] 他认人脸又认不准了
[00:00:14,699 -> 00:00:17,000] 所以说它是一个非常专的模型
[00:00:17,000 -> 00:00:19,500] 那GPT它是一个
[00:00:19,500 -> 00:00:22,300] 它背后的这个GPT是一个很通用的模型
[00:00:22,300 -> 00:00:24,100] 那通用的模型和这个专有模型
[00:00:24,100 -> 00:00:25,800] 就必须要有一个技术突破
[00:00:25,800 -> 00:00:27,199] 这个技术突破就是这个
[00:00:00,000 -> 00:00:02,600] In Context Learning 叫上下文学习吧
[00:00:02,600 -> 00:00:05,799] 你这个上下文学习它的神奇之处呢
[00:00:05,799 -> 00:00:08,400] 就是在于你给了一个具备
[00:00:08,400 -> 00:00:10,199] In Context Learning 模型
[00:00:10,199 -> 00:00:11,400] 新的数据
[00:00:11,400 -> 00:00:13,199] 它不需要修改这个模型本身
[00:00:13,199 -> 00:00:15,000] 它就能在新的数据上表现更好
[00:00:15,000 -> 00:00:16,399] 我就不多展开了
[00:00:16,399 -> 00:00:17,600] 大家知道这件事就行了
[00:00:17,600 -> 00:00:19,199] 总之是一个很神奇的事情
[00:00:19,199 -> 00:00:21,800] 那第三个概念就是涌现
[00:00:21,800 -> 00:00:25,199] 涌现呢就是我们现在看到了
[00:00:25,199 -> 00:00:27,800] 这些大语言模型里边它的一个现象
[00:00:00,000 -> 00:00:03,799] 這個現象就是我們並不知道這個能力要出現
[00:00:03,799 -> 00:00:06,400] 我們沒有辦法預測它在什麼時候出現
[00:00:06,400 -> 00:00:09,400] 但是它變大的時候或者說質量變高的時候
[00:00:09,400 -> 00:00:10,400] 它突然就出現了
[00:00:10,400 -> 00:00:13,000] 這也是一個很神奇的現象
[00:00:15,000 -> 00:00:17,000] 第四個就是這個
[00:00:17,000 -> 00:00:20,399] 這個術語叫 Reinforcement Learning with Human Feedback
[00:00:20,399 -> 00:00:24,399] 就是基於人類反饋的強化學習
[00:00:24,399 -> 00:00:27,800] 那這個基於人類反饋的強化學習
[00:00:27,800 -> 00:00:29,399] 我覺得技術上都不展開了
[00:00:00,000 -> 00:00:01,199] 大家如果感兴趣的话
[00:00:01,199 -> 00:00:02,560] 可以去读这个PPT
[00:00:02,560 -> 00:00:03,560] 或者读我的文章
[00:00:03,560 -> 00:00:05,000] 总之呢就是
[00:00:05,000 -> 00:00:07,799] 底层的那个大模显影是我刚刚说的
[00:00:07,799 -> 00:00:08,880] 它是生成式的
[00:00:08,880 -> 00:00:10,080] 自会归的大圆模显影
[00:00:10,080 -> 00:00:10,480] 对吧
[00:00:10,480 -> 00:00:11,919] 我们通过训练
[00:00:11,919 -> 00:00:13,439] 涌现出来一堆东西
[00:00:13,439 -> 00:00:14,519] 但是这些东西呢
[00:00:14,519 -> 00:00:16,760] 可能还是不能很好的被人类使用
[00:00:16,760 -> 00:00:18,760] 那通过这样的一个机制
[00:00:18,760 -> 00:00:20,199] 通过了这样一个
[00:00:20,199 -> 00:00:23,320] 就是从人类反馈中
[00:00:23,320 -> 00:00:25,559] 去重新学习吧的机制
[00:00:25,559 -> 00:00:28,239] 它跟人类的偏好对齐了
[00:00:00,000 -> 00:00:02,960] 就是他不是他能听懂人类的对话了
[00:00:02,960 -> 00:00:04,040] 而是他的对话
[00:00:04,040 -> 00:00:07,440] 他说出的东西符合我们人类的理解了
[00:00:07,440 -> 00:00:09,560] 符合我们人类的这个理解范式了
[00:00:09,599 -> 00:00:11,560] 所以说他就对人类变得有用了
[00:00:13,279 -> 00:00:14,880] 嗯这个我就不展开了
[00:00:15,080 -> 00:00:16,079] 这个我也不展开了
[00:00:17,039 -> 00:00:18,600] 就是如果打个比方的话
[00:00:18,600 -> 00:00:21,000] 就是大家如果看过钢铁侠
[00:00:21,280 -> 00:00:22,079] GBT
[00:00:22,640 -> 00:00:25,960] GBT那个大模型是钢铁侠的那个
[00:00:26,640 -> 00:00:28,559] 就是那个发电机
[00:00:28,600 -> 00:00:29,960] 就是它中间那个核
[00:00:00,000 -> 00:00:01,439] 那個盒是超級難的對吧
[00:00:01,439 -> 00:00:03,399] 如果看鋼鐵俠就是它是
[00:00:03,399 -> 00:00:05,719] 很難複製的
[00:00:05,719 -> 00:00:07,280] 這是鋼鐵俠的獨有技術
[00:00:07,280 -> 00:00:09,919] 然後鋼鐵俠身上有各種各樣的武器系統
[00:00:09,919 -> 00:00:12,279] 這些武器系統看上去都很牛逼很炫酷
[00:00:12,279 -> 00:00:14,039] 但是都是人類已有的科技
[00:00:14,039 -> 00:00:15,960] 我們如果記得鋼鐵俠2的時候
[00:00:15,960 -> 00:00:17,039] 它有一個競爭對手
[00:00:17,039 -> 00:00:20,960] 也有各種各樣的可以發導彈和掃機關槍的東西
[00:00:20,960 -> 00:00:23,120] 武器系統其實是容易複製的
[00:00:23,120 -> 00:00:25,280] 但是這個內盒其實是非常非常重要的
[00:00:25,280 -> 00:00:28,000] 所以說我們在研究XHPT的時候
[00:00:28,000 -> 00:00:29,559] 要去關心它的這個內盒
[00:00:00,000 -> 00:00:02,759] 而不是关注它那些各种各样的武器系统
[00:00:05,599 -> 00:00:07,200] 更多的技术我也不展开了
[00:00:07,559 -> 00:00:11,759] 我就简单说一下我的这个我自己的观点吧
[00:00:11,759 -> 00:00:14,880] 我这就不去展开过多的其他的观点了
[00:00:14,880 -> 00:00:17,000] 我希望在大家在问的问题的时候
[00:00:17,199 -> 00:00:20,000] 去来问我
[00:00:20,559 -> 00:00:22,199] 第一个就是
[00:00:22,239 -> 00:00:25,320] ChaiCPT是不是只不过是一个更好的大语言模型
[00:00:25,359 -> 00:00:27,640] 就是它到底有没有和其他的大语言模型
[00:00:27,719 -> 00:00:28,640] 有本质的突破
[00:00:00,000 -> 00:00:01,600] 和过去的模型有本质的突破
[00:00:02,040 -> 00:00:03,839] 但是下面的这个子问题就是
[00:00:03,839 -> 00:00:05,679] 我们是不是一个更聪明的猴子
[00:00:06,040 -> 00:00:10,199] 这个是我觉得和其他的那些专家们
[00:00:10,439 -> 00:00:12,679] 我自己觉得很重要的一个不同
[00:00:12,679 -> 00:00:15,800] 就是如果按照那些专家们的逻辑
[00:00:15,800 -> 00:00:18,640] 去认为GPT只不过是一个更好的代言模型
[00:00:18,920 -> 00:00:20,760] 那根据类似的逻辑
[00:00:20,760 -> 00:00:23,000] 我们可能和猴子也没有那么大的不同
[00:00:23,160 -> 00:00:25,199] 而且这个是有科学依据的
[00:00:26,239 -> 00:00:28,239] 先说就是过去的模型是干什么的
[00:00:00,000 -> 00:00:03,399] 過去的模型他們尋找的是數據的對應關係
[00:00:03,399 -> 00:00:08,000] 所以說這個是一個簡單的illustration
[00:00:08,000 -> 00:00:08,800] 就是示範
[00:00:08,800 -> 00:00:11,000] 我們現在想訓練一個模型
[00:00:11,000 -> 00:00:14,199] 把藍色的點和紅色的點給區分開
[00:00:14,199 -> 00:00:16,679] 過去的模型如果你做的不開
[00:00:16,679 -> 00:00:17,480] 誇一刀切
[00:00:17,480 -> 00:00:19,079] 那肯定有時候很多誤傷
[00:00:19,079 -> 00:00:19,519] 對吧
[00:00:19,519 -> 00:00:21,320] 然後你這個綠色的
[00:00:21,320 -> 00:00:23,719] 你可以把它分得非常清楚
[00:00:23,719 -> 00:00:28,280] 但是如果我們再給他一個新的藍色和紅色的圖像的話
[00:00:28,280 -> 00:00:29,280] 他就分不清楚了
[00:00:00,000 -> 00:00:01,760] 所以说它这个叫overfit
[00:00:01,760 -> 00:00:03,120] 然后这个黑色的呢
[00:00:03,120 -> 00:00:05,240] 它就是一个比较有用的模型
[00:00:05,240 -> 00:00:08,000] 过去的这个深度学习
[00:00:08,000 -> 00:00:10,599] 反正机器学习深度学习做的事情
[00:00:10,599 -> 00:00:13,519] 就是在这个数据中想方设法的去找一个关系
[00:00:13,519 -> 00:00:16,199] 然后去满足一些条件优化一个东西
[00:00:16,199 -> 00:00:17,039] 但不管怎么样
[00:00:17,039 -> 00:00:19,039] 它就是寻找对应关系
[00:00:19,039 -> 00:00:20,399] 它可以寻找的很好
[00:00:20,399 -> 00:00:23,519] 就是这样做是你自己调出来的
[00:00:23,519 -> 00:00:26,079] 你把它调到了恰好的位置
[00:00:26,079 -> 00:00:28,039] 但是它其实是有能力给你
[00:00:00,000 -> 00:00:02,000] 把所有的關係找得非常非常清楚
[00:00:02,000 -> 00:00:03,080] 你給他一堆隨機數
[00:00:03,080 -> 00:00:04,839] 他也能給你找到對應關係
[00:00:06,639 -> 00:00:09,119] 可是找對應關係這件事情
[00:00:09,119 -> 00:00:11,080] 就是一個鸚鵡學舌的行為
[00:00:11,080 -> 00:00:12,960] 他只是把對應關係給到你
[00:00:12,960 -> 00:00:15,279] 他並沒有真正理解這個數據的意思
[00:00:15,720 -> 00:00:18,079] 但是GPT 3.5開始
[00:00:18,079 -> 00:00:19,600] 我們就發現了一個很不一樣的東西
[00:00:19,600 -> 00:00:22,000] 就是它有一個理解能力
[00:00:22,000 -> 00:00:23,719] 這個理解能力是在
[00:00:23,719 -> 00:00:26,160] 有一個朱松純教授
[00:00:26,160 -> 00:00:27,920] 他2017年的時候
[00:00:00,000 -> 00:00:03,200] 就是講人工智能的局限的時候講的
[00:00:03,200 -> 00:00:04,400] 就是他這個理解
[00:00:04,400 -> 00:00:07,440] 就是他就說過去的這個AI都是鸚鵡
[00:00:07,440 -> 00:00:09,240] 然後我們要尋找烏鴉的能力
[00:00:09,240 -> 00:00:10,599] 那烏鴉他幹了什麼呢
[00:00:10,839 -> 00:00:12,759] 烏鴉他在一個城市裡面
[00:00:13,000 -> 00:00:15,759] 他想打開堅果吃裡面的東西
[00:00:15,759 -> 00:00:17,120] 但是他發現掉到地上啊
[00:00:17,120 -> 00:00:18,399] 或者拿石頭砸都不行
[00:00:18,719 -> 00:00:20,399] 他就進一步發現
[00:00:20,399 -> 00:00:22,600] 汽車是能把堅果給壓開的
[00:00:22,719 -> 00:00:24,519] 紅綠燈是能控制汽車的
[00:00:24,519 -> 00:00:26,559] 但是汽車對我是有傷害的
[00:00:26,559 -> 00:00:27,559] 汽車可以壓死我
[00:00:27,679 -> 00:00:28,800] 那他怎麼辦呢
[00:00:00,000 -> 00:00:04,000] 他把这个坚果扔到了红绿灯的前面
[00:00:04,200 -> 00:00:07,719] 然后让汽车去把它压碎以后
[00:00:07,719 -> 00:00:09,919] 再等那个红绿灯红灯亮了
[00:00:09,919 -> 00:00:11,880] 汽车停了去把这个坚果给拿起来
[00:00:12,880 -> 00:00:14,119] 这件事情就让我们觉得
[00:00:14,119 -> 00:00:15,679] 乌鸦一定是有理解能力的
[00:00:15,800 -> 00:00:17,320] 不然的话他没有办法把这三个
[00:00:17,320 -> 00:00:18,719] 不相关的东西给串到一起
[00:00:18,719 -> 00:00:19,719] 然后完成这个任务
[00:00:20,160 -> 00:00:21,120] 拆GPT
[00:00:21,679 -> 00:00:23,280] 我文章里边我更详细的展开
[00:00:23,440 -> 00:00:24,920] 但是总之我们觉得拆GPT
[00:00:24,960 -> 00:00:27,079] 我觉得拆GPT是有这个能力的
[00:00:00,000 -> 00:00:06,599] 那接下來大家可能就會問到一個更哲學的問題
[00:00:06,599 -> 00:00:09,699] 或者說是那個更難回答的問題
[00:00:09,699 -> 00:00:11,900] 就是拆GPT有沒有意識
[00:00:11,900 -> 00:00:14,500] 對 包括左下角這個圖
[00:00:14,500 -> 00:00:16,500] 大家如果對哲學史比較了解的話
[00:00:16,500 -> 00:00:18,100] 就知道這是一個肛中之腦
[00:00:18,100 -> 00:00:20,600] 對吧 就是我們接下來會問的另外一個問題
[00:00:20,600 -> 00:00:22,600] 不是 就是在問拆GPT的時候
[00:00:22,600 -> 00:00:23,899] 大家不會問這個問題
[00:00:23,899 -> 00:00:26,199] 但是在哲學史上有一個沒有答案的問題
[00:00:00,000 -> 00:00:04,400] 就是我們是不是一個浴缸裡的大腦
[00:00:04,400 -> 00:00:07,719] 然後我們所有的這些東西都是我們大腦想像出來的
[00:00:07,719 -> 00:00:09,919] 而不是我們真實感受到的
[00:00:09,919 -> 00:00:13,800] 我們是不是一個計算機在不斷的給我們刺激
[00:00:13,800 -> 00:00:15,839] 包括《黑客帝國》等等的文學作品
[00:00:15,839 -> 00:00:18,440] 都是在這個觀點上出現的
[00:00:19,039 -> 00:00:20,559] 那為什麼這兩個是有聯繫的
[00:00:20,559 -> 00:00:24,920] 為什麼ChaiJPT的意識和肛中之腦這個問題是有聯繫的呢
[00:00:25,199 -> 00:00:27,960] 就是我們並不知道ChaiJPT有沒有意識
[00:00:00,000 -> 00:00:04,160] 这个是我跟很多大佬的很大的分析
[00:00:04,160 -> 00:00:06,320] 大佬们就是一些大佬们吧
[00:00:06,320 -> 00:00:07,599] 越来越少的大佬们
[00:00:07,599 -> 00:00:10,800] 他们会觉得我们知道CHAPT是怎么做出来的
[00:00:10,800 -> 00:00:11,279] 对吧
[00:00:11,279 -> 00:00:14,240] 他们的科学上没有什么特别大的突破
[00:00:14,240 -> 00:00:17,920] 我们知道它是一个自回归生成式的大圆模型
[00:00:17,920 -> 00:00:19,199] 那在这种范式下
[00:00:19,199 -> 00:00:20,480] 你是不可能产生意识的
[00:00:20,480 -> 00:00:21,839] 因为我们知道你是怎么造的
[00:00:21,839 -> 00:00:24,719] 但别忘了那个涌现的那件事情啊
[00:00:24,719 -> 00:00:26,559] 涌现是一个很重要的关键词
[00:00:26,559 -> 00:00:28,079] 还有另外一个
[00:00:28,079 -> 00:00:29,320] 还有另外一件事情
[00:00:00,000 -> 00:00:01,000] 就是
[00:00:02,879 -> 00:00:03,879] 這是個什麼
[00:00:04,740 -> 00:00:05,740] 先不管了
[00:00:11,720 -> 00:00:13,720] 就是在我們的認知科學裡面
[00:00:13,720 -> 00:00:15,220] Cognitive Science裡面
[00:00:16,179 -> 00:00:18,719] 其實第一我們
[00:00:18,719 -> 00:00:21,719] 並不知道意識是什麼怎麼回事
[00:00:22,600 -> 00:00:25,260] 大概是在就是21世紀剛開始的時候
[00:00:25,260 -> 00:00:26,500] 有一些
[00:00:26,500 -> 00:00:29,559] 有人提出來20世紀21世紀的20代
[00:00:00,000 -> 00:00:01,000] 科學問題吧
[00:00:01,000 -> 00:00:02,919] 可能大概一半是跟意識有關的
[00:00:02,919 -> 00:00:04,559] 就是我們為什麼
[00:00:04,559 -> 00:00:06,040] 這第一個是宇宙的起源
[00:00:06,040 -> 00:00:06,960] 宇宙是怎麼起源的
[00:00:06,960 -> 00:00:08,199] 這是未解之謎
[00:00:08,199 -> 00:00:09,519] 然後當然還有很多
[00:00:09,519 -> 00:00:11,000] 就是我們為什麼會做夢
[00:00:11,000 -> 00:00:13,080] 我們為什麼和其他的動物不一樣
[00:00:13,080 -> 00:00:14,679] 意識是什麼
[00:00:14,679 -> 00:00:15,960] 很多這些問題都是
[00:00:15,960 -> 00:00:16,839] 我們都是不知道的
[00:00:16,839 -> 00:00:19,440] 我們對人腦的理解是非常非常淺的
[00:00:19,440 -> 00:00:21,280] 那在這種情況下
[00:00:21,280 -> 00:00:23,480] 其實我們去非常怎麼說呢
[00:00:23,480 -> 00:00:24,920] 非常傲慢的去說
[00:00:24,920 -> 00:00:26,359] 因為我把你造出來
[00:00:26,359 -> 00:00:28,760] 所以說我覺得你肯定沒有意識
[00:00:00,000 -> 00:00:02,799] 我覺得這個邏輯不一定成立
[00:00:02,799 -> 00:00:05,599] 而且在這個認知科學裡面
[00:00:05,599 -> 00:00:08,439] 還有一本很著名的書
[00:00:08,439 -> 00:00:10,640] 就是The Path to Consciousness
[00:00:10,640 -> 00:00:12,000] 這就不展開了
[00:00:12,000 -> 00:00:14,480] 它其實也是很久很久以前寫的書
[00:00:14,480 -> 00:00:16,039] 那裡面就是說人腦
[00:00:16,039 -> 00:00:17,920] 其實和其他動物的大腦
[00:00:17,920 -> 00:00:19,079] 沒有本質的區別
[00:00:19,079 -> 00:00:20,760] 就是我們神經元更多
[00:00:20,760 -> 00:00:22,120] 我們皮層更豐富
[00:00:22,120 -> 00:00:23,679] 但是沒有一個數量級的差別
[00:00:23,679 -> 00:00:24,480] 就是確實多
[00:00:24,480 -> 00:00:27,760] 但是沒有多到就是差那麼大
[00:00:27,760 -> 00:00:28,920] 就是你很明顯一個猴子
[00:00:00,000 -> 00:00:02,240] 和一個螞蟻的大腦差的是非常非常大的
[00:00:02,240 -> 00:00:03,500] 但是我們和一個猴子的大腦
[00:00:03,500 -> 00:00:04,500] 其實差的沒有那麼大
[00:00:04,500 -> 00:00:06,000] 那在這種情況下
[00:00:06,000 -> 00:00:07,500] 我們的意識是怎麼來的呢
[00:00:07,500 -> 00:00:09,259] 然後那個書裡面就用了這個
[00:00:09,259 -> 00:00:10,259] 湧現這個詞
[00:00:10,259 -> 00:00:12,140] 他就覺得我們的意識是湧現出來的
[00:00:12,140 -> 00:00:13,720] 我們的智力是湧現出來的
[00:00:13,720 -> 00:00:16,179] 所以說我們不知道我們的大腦有沒有
[00:00:16,980 -> 00:00:18,600] 意識和我們的
[00:00:18,600 -> 00:00:20,679] 看似有的這個智力是怎麼來的
[00:00:20,679 -> 00:00:21,940] 我這就覺得
[00:00:21,940 -> 00:00:23,480] 第一我們不知道
[00:00:23,480 -> 00:00:24,780] Chagpity到底有沒有
[00:00:25,160 -> 00:00:26,320] 意識或者智力
[00:00:26,320 -> 00:00:27,620] 但是這個東西不重要
[00:00:27,620 -> 00:00:28,899] 這個東西一點都不重要
[00:00:00,000 -> 00:00:01,540] 因為為什麼呢
[00:00:01,540 -> 00:00:05,000] 因為在哲學史上其實就是有一個他心問題啊
[00:00:05,000 -> 00:00:07,200] 這兒我沒放
[00:00:07,200 -> 00:00:09,919] 但是我在文檔裡面有放一些拓展閱讀
[00:00:09,919 -> 00:00:11,320] 就大家如果感興趣的話
[00:00:11,320 -> 00:00:13,640] 就會看到它是貫穿哲學史
[00:00:13,640 -> 00:00:16,960] 所有的著名哲學家都會去說一嘴的這個問題
[00:00:16,960 -> 00:00:20,320] 就是我到底是不是NPC
[00:00:20,320 -> 00:00:21,839] 你們到底是不是NPC
[00:00:21,839 -> 00:00:24,000] 就是我見到了其他人到底是不是NPC
[00:00:24,000 -> 00:00:25,879] 我到底是不是一個剛中之腦
[00:00:25,879 -> 00:00:28,559] 這個問題在哲學史上是一個沒有答案
[00:00:00,000 -> 00:00:03,960] 但是每個人都會借題發揮去講很多重要哲學觀點的東西
[00:00:04,320 -> 00:00:07,040] 但是他對我們來說有用的答案是什麼呢
[00:00:07,040 -> 00:00:08,039] 就是也不重要
[00:00:08,039 -> 00:00:10,039] 我不需要去管你們是不是NPC
[00:00:10,039 -> 00:00:14,320] 我在看到一個同學他能不能完成工作
[00:00:14,519 -> 00:00:15,960] 我是看他的工作結果對吧
[00:00:15,960 -> 00:00:17,920] 我不需要知道他腦子裡邊
[00:00:17,920 -> 00:00:20,440] 我不會去想他腦子裡邊是在發生什麼
[00:00:20,440 -> 00:00:23,239] 我知道他哦你做的這個結果挺不錯的
[00:00:23,239 -> 00:00:25,120] 那我就認為你是一個可以勝任的人
[00:00:25,320 -> 00:00:28,000] 我建議就是對ChaiGBT也抱這樣的觀點
[00:00:00,000 -> 00:00:02,600] 就是ChangeBT交给你一个工作能不能做好
[00:00:02,600 -> 00:00:04,400] 能做好OK你可以做好这个工作
[00:00:06,480 -> 00:00:07,599] 这是第一个问题
[00:00:07,879 -> 00:00:09,359] 那第二个问题就是
[00:00:09,359 -> 00:00:11,800] ChangeBT到底是一个什么样的意义
[00:00:11,800 -> 00:00:13,279] 就是怎么去理解它的意义
[00:00:13,480 -> 00:00:15,080] 我一开始我写了以后我觉得
[00:00:15,080 -> 00:00:15,400] 嗯
[00:00:16,199 -> 00:00:17,039] 说得有点大
[00:00:17,160 -> 00:00:18,039] 但是后来呢
[00:00:18,440 -> 00:00:19,600] 比尔盖茨写了一个文章
[00:00:19,600 -> 00:00:21,199] 然后我就去翻了这个SamUltimate
[00:00:21,199 -> 00:00:23,199] 就是那个OpenAI CEO
[00:00:23,399 -> 00:00:24,640] 他写的文章我发现
[00:00:24,640 -> 00:00:24,920] 嗯
[00:00:25,199 -> 00:00:26,719] 他们的理解和我是差不多的
[00:00:26,719 -> 00:00:28,640] 所以说我就对这个事情多了很多信
[00:00:28,640 -> 00:00:29,440] 呃信任
[00:00:00,000 -> 00:00:03,040] 就是比爾蓋茲說這是一個GUI時刻
[00:00:03,040 -> 00:00:05,879] 然後奧特曼說的是這是一個萬物馬爾定律
[00:00:07,519 -> 00:00:09,000] 對那
[00:00:11,119 -> 00:00:12,560] GUI時刻怎麼理解
[00:00:12,560 -> 00:00:15,320] 就是在這個計算機的歷史上
[00:00:15,320 -> 00:00:17,960] 其實我們就是在三件事情上去做進步
[00:00:17,960 -> 00:00:20,399] 第一個是算力和存儲硬件
[00:00:20,399 -> 00:00:22,920] 第二個是數據的產生和使用
[00:00:24,320 -> 00:00:26,120] 比如說我們在刷抖音的時候
[00:00:26,480 -> 00:00:28,519] 抖音的那個視頻是一個數據
[00:00:00,000 -> 00:00:01,720] 你的浏览行为是一个数据
[00:00:01,720 -> 00:00:05,040] 抖音你和其他人的关系都能演示出去
[00:00:05,040 -> 00:00:06,400] 所以抖音把这些数据用完了以后
[00:00:06,400 -> 00:00:08,160] 就可以给你推荐你想刷的视频
[00:00:08,160 -> 00:00:10,880] 而且是让用户去产生这些东西去给你使用
[00:00:10,880 -> 00:00:12,880] 最后就形成了一个这么大的商业模式
[00:00:12,880 -> 00:00:13,279] 对吧
[00:00:13,279 -> 00:00:15,839] 这个是数据的使用和生产
[00:00:16,320 -> 00:00:20,960] 第三个就是把上面的算力存储和数据进行更好的组合
[00:00:20,960 -> 00:00:22,640] 然后更好的去使用它
[00:00:23,120 -> 00:00:26,960] 之前我们使用的方法就是写代码
[00:00:00,000 -> 00:00:03,459] 但是我們在日常正常人消費不會寫代碼的時候怎麼辦
[00:00:03,459 -> 00:00:05,299] 我們就是用這個GUI
[00:00:05,299 -> 00:00:08,640] 其實比爾蓋茲就是大家都知道比爾蓋茲
[00:00:08,640 -> 00:00:12,439] 他在那個文章裡面第一句話寫的是
[00:00:12,439 -> 00:00:15,480] 他這輩子只見過兩個顛覆性的科技
[00:00:15,480 -> 00:00:17,679] 第一個是GUI第二個是柴脂筆貼
[00:00:17,679 -> 00:00:18,719] 包括GPT
[00:00:18,719 -> 00:00:19,320] 對
[00:00:19,320 -> 00:00:22,120] 就在他的角度來說這不是一個iPhone時刻
[00:00:22,120 -> 00:00:23,179] 這不是一個電腦時刻
[00:00:23,179 -> 00:00:24,519] 這不是一個互聯網時刻
[00:00:24,519 -> 00:00:25,960] 這是一個GUI時刻
[00:00:25,960 -> 00:00:28,859] 就是他在他這輩子之前只見過GUI
[00:00:00,000 -> 00:00:02,000] 現在見到GPT
[00:00:02,000 -> 00:00:04,000] 這兩個是最重要的
[00:00:04,000 -> 00:00:06,000] 那怎麼理解它跟GUI的關係呢
[00:00:06,000 -> 00:00:08,000] 就是我們在刷抖音
[00:00:08,000 -> 00:00:10,000] 大家想一想如果說我們沒有這個
[00:00:10,000 -> 00:00:12,000] GUI就是那個圖形交互介面
[00:00:12,000 -> 00:00:14,000] 就是按鈕啊這些東西
[00:00:14,000 -> 00:00:16,000] 如果我們沒有手機上的這些按鈕
[00:00:16,000 -> 00:00:18,000] 可以打開抖音APP然後去
[00:00:18,000 -> 00:00:20,000] 看到抖音然後去這樣刷
[00:00:20,000 -> 00:00:22,000] 那我們想去看到這些短視頻怎麼辦
[00:00:22,000 -> 00:00:24,000] 就寫代碼是吧
[00:00:24,000 -> 00:00:26,000] 瘋狂寫代碼可能我們要寫
[00:00:26,000 -> 00:00:28,000] 不知道從這擺到那那麼長的代碼
[00:00:00,000 -> 00:00:02,120] 它有可能刷出來幾個視頻
[00:00:02,120 -> 00:00:04,320] 但是我們現在只需要點開
[00:00:04,320 -> 00:00:05,320] 就是我們不會去想
[00:00:05,320 -> 00:00:07,120] 然後直接就可以去刷這個視頻了
[00:00:07,120 -> 00:00:09,400] 所以說GUI是特別特別牛逼的
[00:00:09,400 -> 00:00:11,560] 它讓一些就是這個
[00:00:11,560 -> 00:00:15,199] 算力存儲和這個數據的這個交互
[00:00:15,199 -> 00:00:16,719] 變得簡單了很多
[00:00:16,719 -> 00:00:18,879] 對人每個人都可以去使用它
[00:00:18,879 -> 00:00:21,559] 對 大家出現了這個東西以後
[00:00:21,559 -> 00:00:22,559] 就覺得它很正常
[00:00:22,559 -> 00:00:24,399] 這包括搜索
[00:00:24,399 -> 00:00:26,320] 在Google出來的之前
[00:00:26,320 -> 00:00:28,239] 我相信全世界沒有任何一個人
[00:00:00,000 -> 00:00:02,560] 比如說美國總統啊 然後什麼皇帝啊
[00:00:02,560 -> 00:00:05,000] 他都沒有我們現在手上這個信息量
[00:00:05,000 -> 00:00:08,000] 就是美國總統那個時候想知道一個信息
[00:00:08,000 -> 00:00:09,500] 應該是很麻煩的吧
[00:00:09,500 -> 00:00:10,699] 他要去問很多專家
[00:00:10,699 -> 00:00:13,400] 這些專家要去找很多這個圖書
[00:00:13,400 -> 00:00:14,800] 就是我讀書的時候
[00:00:14,800 -> 00:00:16,000] 暴露年齡啊
[00:00:16,000 -> 00:00:18,500] 就是還要去學怎麼樣子去查書
[00:00:18,500 -> 00:00:21,300] 我相信大家現在都不需要去查書了吧
[00:00:21,300 -> 00:00:24,500] 就是圖書檢索本來是一門很嚴肅的科學
[00:00:24,500 -> 00:00:28,000] 就是你想快速檢索出你想要的信息
[00:00:00,000 -> 00:00:02,799] 你要把这个信息组织起来是个很难很难的事情
[00:00:02,799 -> 00:00:04,200] 但是搜索出来以后
[00:00:04,200 -> 00:00:06,200] 每个人随随便便天天都可以搜索
[00:00:06,200 -> 00:00:09,800] 这件事情是一个很不可思议的技术进步
[00:00:09,800 -> 00:00:11,599] 当然大家已经习以为常了
[00:00:12,400 -> 00:00:12,800] 对
[00:00:12,800 -> 00:00:16,399] 我们接下来就会遇到另外一个方向的
[00:00:16,399 -> 00:00:17,399] 很不可思议的
[00:00:17,399 -> 00:00:19,600] 今天大家完全不可想象的技术进步
[00:00:19,600 -> 00:00:22,800] 就是GUI它可以刚说了是很厉害
[00:00:22,800 -> 00:00:25,199] 但是它有一个问题就是它特别专
[00:00:25,199 -> 00:00:27,399] 它一个按钮只能干一件事情
[00:00:27,399 -> 00:00:29,500] 它可以让这件事情干得特别的高效
[00:00:00,000 -> 00:00:01,600] 然後特別特別的符合直覺
[00:00:01,600 -> 00:00:03,299] 但是他只能幹這麼一件事
[00:00:03,299 -> 00:00:03,799] 對吧
[00:00:03,799 -> 00:00:06,500] 我們其實日常生活中要幹很多事
[00:00:06,500 -> 00:00:08,300] 我們今天之所以沒有這個按鈕去做
[00:00:08,300 -> 00:00:09,699] 是因為做這個按鈕不經濟
[00:00:09,699 -> 00:00:12,099] 或者說這個事情就是很難通過按鈕去做
[00:00:12,099 -> 00:00:13,300] 在JPT呢
[00:00:13,300 -> 00:00:15,500] 你可以通過跟計算機的對話
[00:00:15,500 -> 00:00:18,100] 讓計算機去調動算力存儲和數據
[00:00:18,100 -> 00:00:19,699] 幫你去做你想做的事
[00:00:19,699 -> 00:00:23,000] 所以這是為什麼提問的能力變得這麼重要了
[00:00:23,000 -> 00:00:25,500] 你要真的去看到生活中的問題
[00:00:25,500 -> 00:00:27,300] 而且這個生活中的問題一定不是說
[00:00:27,300 -> 00:00:28,500] 我今天寫了一個PPT
[00:00:28,500 -> 00:00:29,699] 我明天怎麼寫得更快一點
[00:00:00,000 -> 00:00:03,000] 它一定是你之前從來沒有幹過的一件事
[00:00:03,000 -> 00:00:04,400] 然後你突然發現
[00:00:04,400 -> 00:00:08,000] 這件事我有沒有可能用XGBT來解決
[00:00:08,000 -> 00:00:10,500] 這也不多說了
[00:00:10,500 -> 00:00:14,000] 然後第二個那個萬物摩爾定律的意思是什麼呢
[00:00:14,000 -> 00:00:20,000] 就是摩爾定律我們手機啊
[00:00:20,000 -> 00:00:21,500] 然後電腦啊
[00:00:21,500 -> 00:00:22,500] 然後電視啊
[00:00:22,500 -> 00:00:24,000] 它是被摩爾定律影響的
[00:00:24,000 -> 00:00:26,000] 我們看到它的價格一直在往下走
[00:00:26,000 -> 00:00:29,500] 就是今天的電視比10年前的電視性能好很多
[00:00:00,000 -> 00:00:01,340] 當然價格也要便宜
[00:00:01,340 -> 00:00:03,339] 可是我們的人
[00:00:03,339 -> 00:00:05,040] 就是跟治理相關的
[00:00:05,040 -> 00:00:06,639] 不能被墨爾丁利所影響的
[00:00:06,639 -> 00:00:08,939] 比如說醫生 教師
[00:00:08,939 -> 00:00:11,980] 和這個就是法律等等
[00:00:11,980 -> 00:00:14,179] 它其實價格是在往上走的
[00:00:14,179 -> 00:00:17,420] 那這個最重要的原因是
[00:00:17,420 -> 00:00:18,620] 我們的人力資本
[00:00:18,620 -> 00:00:20,420] 其實是需要花很多很多錢
[00:00:20,420 -> 00:00:21,719] 才能建立起來
[00:00:21,719 -> 00:00:25,420] 但是我們分發的這個成本是極高的
[00:00:25,420 -> 00:00:27,260] 我今天過來跟大家聊這些東西
[00:00:27,260 -> 00:00:28,699] 我就不能出現在一個別的地方
[00:00:28,699 -> 00:00:29,460] 跟其他人聊
[00:00:00,000 -> 00:00:01,199] 我也不能去干别的事
[00:00:01,199 -> 00:00:05,679] 所以说就是这一小时对我来说是很贵的
[00:00:05,679 -> 00:00:09,080] 可是如果有了XGBT
[00:00:09,080 -> 00:00:11,359] 我让大家就是稍稍想象一下
[00:00:11,359 -> 00:00:14,599] 就是今天的XGBT还做不到
[00:00:14,599 -> 00:00:17,839] 但是在它的能力范围之内
[00:00:17,839 -> 00:00:22,719] 还没有开放的是它假设可以记住你一些句话
[00:00:22,719 -> 00:00:26,480] 就是大家今天知道XGBT可以记住你很多句话
[00:00:26,480 -> 00:00:27,719] 但是它的开放
[00:00:00,000 -> 00:00:02,560] 他现在他那个产品只不过是一个小的demo
[00:00:02,560 -> 00:00:03,560] 大家记住这一点
[00:00:03,560 -> 00:00:05,879] 就是他的产品只不过他连正式产品都没有
[00:00:05,879 -> 00:00:07,679] 他的现在的产品只不过是个demo
[00:00:07,679 -> 00:00:10,640] 然后他这个demo的情况下他能记住你一些话
[00:00:10,640 -> 00:00:12,240] 但是假设有一天开放了
[00:00:12,240 -> 00:00:13,800] 可以记住你一千句话一万句话
[00:00:13,800 -> 00:00:15,560] 然后你的这个权重是可调的
[00:00:15,839 -> 00:00:17,559] 你这一万句话你可以干什么
[00:00:17,839 -> 00:00:19,519] 你可以把你的知识交给他
[00:00:19,519 -> 00:00:21,399] 你可以跟他通过对话告诉他
[00:00:21,399 -> 00:00:23,120] 你应该做什么什么什么
[00:00:23,120 -> 00:00:25,960] 比如说有写这个市场方案的同学
[00:00:25,960 -> 00:00:27,559] 有做数据分析的同学
[00:00:00,000 -> 00:00:02,960] 那你就把你知道的这些知识全都跟他讲
[00:00:02,960 -> 00:00:05,200] 然后你还可以把一些数据喂给他
[00:00:05,200 -> 00:00:08,119] 你还可以告诉他怎么去使用工具去查询很多东西
[00:00:08,480 -> 00:00:10,400] 之后他做了这些结果
[00:00:10,400 -> 00:00:11,599] 你还可以给他提供反馈
[00:00:11,720 -> 00:00:12,800] 你耐心的教他
[00:00:12,800 -> 00:00:14,279] 我觉得这个好那个不好
[00:00:14,279 -> 00:00:15,679] 为什么我觉得这个好那个不好
[00:00:15,880 -> 00:00:17,039] 一万句话之后
[00:00:17,399 -> 00:00:20,519] 他的能力有没有可能达到你做这件事情的80%到
[00:00:20,559 -> 00:00:21,719] 我不知道百分之八百
[00:00:22,079 -> 00:00:23,800] 我觉得完全是可以的
[00:00:24,559 -> 00:00:25,480] 那在这种情况下
[00:00:25,480 -> 00:00:28,600] 你的这个GPT就是你的这个智力的一个分发工具
[00:00:00,000 -> 00:00:04,400] 比如說今天我去醫院去看醫生
[00:00:04,400 -> 00:00:05,559] 醫生給我的診斷
[00:00:05,559 -> 00:00:07,960] 我只能相信這邊的醫生是一個懂的人
[00:00:07,960 -> 00:00:11,000] 但是其實一半的醫生是不到平均值的
[00:00:11,000 -> 00:00:11,279] 對吧
[00:00:11,279 -> 00:00:13,599] 這個是一個不到中位數的
[00:00:13,599 -> 00:00:15,119] 就是我可能運氣不好
[00:00:15,119 -> 00:00:16,559] 就遇到了一個不好的醫生
[00:00:16,559 -> 00:00:20,239] 但是假設今天有一個GPT出來說
[00:00:20,239 -> 00:00:24,239] 這是世界上最牛逼的醫院的專家們
[00:00:24,239 -> 00:00:26,440] 調教了十萬句話
[00:00:00,000 -> 00:00:03,799] 然後調教了半年
[00:00:03,799 -> 00:00:05,280] 然後來給你做診斷
[00:00:05,280 -> 00:00:07,799] 他會問你足夠細緻的問題
[00:00:08,199 -> 00:00:10,679] 他會有最新的科學理論
[00:00:10,720 -> 00:00:12,279] 他來幫你做一下診斷
[00:00:12,320 -> 00:00:13,720] 你願不願意去使用它
[00:00:14,039 -> 00:00:16,800] 如果說它的效果你真的發現
[00:00:16,800 -> 00:00:20,000] 比我去醫院的醫生要好的時候
[00:00:20,440 -> 00:00:24,120] 那是不是醫療這件事情就完全被民主化
[00:00:24,120 -> 00:00:28,600] 和它的編輯成本就極大的下降了呢
[00:00:00,000 -> 00:00:02,319] 我觉得这一天就是很快就会到来的
[00:00:02,319 -> 00:00:04,639] 就是他今天能力完全是可以做到这件事情的
[00:00:07,080 -> 00:00:09,519] 第三个问题就是它有多难
[00:00:09,839 -> 00:00:12,199] 这个我也不多详细展开了吧
[00:00:12,199 -> 00:00:14,439] 总之我会觉得不要觉得它很简单
[00:00:14,439 -> 00:00:16,920] 就是觉得它很简单可能会有几个原因
[00:00:16,920 -> 00:00:19,839] 第一个是我们之前的模型开源都是
[00:00:20,239 -> 00:00:21,359] 之前的模型是开源的
[00:00:21,359 -> 00:00:24,000] 然后之前复现这些开源模型调个包
[00:00:24,000 -> 00:00:25,800] 然后在自己设置上用一下调一调
[00:00:25,879 -> 00:00:26,600] 没有什么难度
[00:00:26,600 -> 00:00:28,719] 所以说大家都觉得那件事情是很简单的
[00:00:00,000 -> 00:00:01,720] 可是今天模型是必然的
[00:00:01,720 -> 00:00:04,919] 關鍵是它的這個工程難度很高的情況下
[00:00:04,919 -> 00:00:07,919] 它需要一個月到兩個月才能迭代一次
[00:00:07,919 -> 00:00:10,000] 所以說不要低估這個
[00:00:10,000 -> 00:00:11,800] 複現這件事情的難度
[00:00:11,800 -> 00:00:12,800] 就這句話吧
[00:00:12,800 -> 00:00:15,199] 然後我的這個決策路徑是
[00:00:15,199 -> 00:00:17,800] 我會看谷歌什麼時候能複現出來
[00:00:17,800 -> 00:00:19,719] 就是谷歌是有
[00:00:19,719 -> 00:00:21,120] 就是提出了Transformer
[00:00:21,120 -> 00:00:23,440] 提出了T5等等所有
[00:00:23,440 -> 00:00:25,719] 這裡邊大約模型裡面重要的模型
[00:00:25,719 -> 00:00:27,120] 它有足夠的算力
[00:00:27,120 -> 00:00:28,440] 有足夠的工程人才
[00:00:28,440 -> 00:00:29,960] 然後它有足夠的商業動機
[00:00:00,000 -> 00:00:01,280] 去把这件事情搞定
[00:00:01,639 -> 00:00:05,759] 但是谷歌Bart可能离拆GPT还差了一年以上
[00:00:06,160 -> 00:00:08,080] 如果谷歌差一年以上的话
[00:00:08,320 -> 00:00:10,439] 我希望有人告诉我为什么我们可以
[00:00:10,880 -> 00:00:12,320] 一年之内就搞定这件事
[00:00:12,320 -> 00:00:13,279] 不然的话我就不信了
[00:00:14,279 -> 00:00:16,719] 但是我觉得也有可能就是拆这件事情不难
[00:00:16,839 -> 00:00:17,519] 我不知道
[00:00:17,679 -> 00:00:19,879] 如果说谷歌三个月之内搞定了
[00:00:19,879 -> 00:00:22,039] 那我就说这件事情没有我想象的这么难
[00:00:22,039 -> 00:00:24,239] 所以说大家去复刻这件事情
[00:00:24,239 -> 00:00:25,559] 我觉得是更加有意义
[00:00:25,800 -> 00:00:26,719] 不然的话
[00:00:27,079 -> 00:00:27,839] 大家想一下
[00:00:27,839 -> 00:00:29,079] 就是浏览器出来的时候
[00:00:00,000 -> 00:00:01,659] 然后我们真的有很多浏览器吗
[00:00:01,659 -> 00:00:02,660] 操作系统出来的时候
[00:00:02,660 -> 00:00:03,660] 我们有很多SOLID系统吗
[00:00:03,660 -> 00:00:06,960] 然后这个iPhone出来的时候
[00:00:06,960 -> 00:00:09,460] 我都还记得一大堆几十个神仔机
[00:00:09,460 -> 00:00:10,660] 都是我们做iPhone
[00:00:10,660 -> 00:00:11,960] 酷拍iPhone什么的
[00:00:11,960 -> 00:00:14,160] 但是最后全都没有留下来
[00:00:14,160 -> 00:00:14,859] 对吧
[00:00:14,859 -> 00:00:15,660] 就这种感觉
[00:00:17,859 -> 00:00:19,960] 第四个问题是我们怎么使用它
[00:00:21,859 -> 00:00:22,460] 不说了吧
[00:00:23,460 -> 00:00:26,160] 就太抽象了
[00:00:28,660 -> 00:00:29,460] 感兴趣的话
[00:00:00,000 -> 00:00:02,000] 看這個和我的文章裡都有
[00:00:04,000 -> 00:00:06,000] 但是一個我覺得歷史
[00:00:06,000 -> 00:00:08,000] 我現在正在上傳
[00:00:08,000 -> 00:00:10,000] 就是我來之前正在往B站上上傳
[00:00:10,000 -> 00:00:12,000] 可能現在已經上傳好了
[00:00:12,000 -> 00:00:14,000] 就是這個歷史的教訓
[00:00:14,000 -> 00:00:16,000] 歷史的教訓或者經驗是什麼呢
[00:00:16,000 -> 00:00:20,000] 就是我們在一個新技術剛出現的時候
[00:00:20,000 -> 00:00:22,000] 會發生兩件事
[00:00:22,000 -> 00:00:26,000] 第一個是我們會嚴重高估它的短期影響
[00:00:26,000 -> 00:00:28,000] 且嚴重低估它的長期影響
[00:00:00,000 -> 00:00:02,520] 所以说比如说互联网刚出来的时候
[00:00:02,520 -> 00:00:03,839] 那个.com bubble对吧
[00:00:03,839 -> 00:00:05,639] 就是一大堆的人去想象
[00:00:05,639 -> 00:00:08,519] 我们把所有线下所有东西搬到一个什么.com上
[00:00:08,519 -> 00:00:11,119] 然后它就估值涨10倍1万倍
[00:00:11,119 -> 00:00:13,039] 最后发现全都是泡沫
[00:00:13,039 -> 00:00:16,519] 我们今天就是移动互联网出现的时候也会这样子
[00:00:16,519 -> 00:00:17,399] 然后等等等等吧
[00:00:17,399 -> 00:00:19,079] 我觉得今天Chashpity也一定会这样子的
[00:00:19,079 -> 00:00:20,280] 就是我们现在出的
[00:00:20,280 -> 00:00:22,960] 大家对它的想法很多都是高估的
[00:00:22,960 -> 00:00:25,879] 它不会那么快的对我们的生活产生那么大的影响
[00:00:00,000 -> 00:00:04,200] 但是他可能三年五年十年對人們產生的影響
[00:00:04,200 -> 00:00:05,400] 我們都嚴重低估了
[00:00:05,679 -> 00:00:09,640] 就是為什麼我會去真的去看調酒這件事情
[00:00:09,679 -> 00:00:12,679] 我真的是覺得三年之後我的工作存不存在
[00:00:12,679 -> 00:00:13,800] 不一定真的不一定
[00:00:15,000 -> 00:00:18,000] 我當然我更希望的是我調教出來的一個
[00:00:18,000 -> 00:00:19,519] XGBT幫我打工
[00:00:19,559 -> 00:00:21,039] 然後我自己去調酒
[00:00:21,079 -> 00:00:22,600] 這樣是比較好的
[00:00:23,359 -> 00:00:25,199] 大家在笑啊大家在笑
[00:00:25,879 -> 00:00:27,640] 我覺得要不這樣子啊
[00:00:00,000 -> 00:00:02,960] 就是如果大家愿意的话可以打开手机
[00:00:02,960 -> 00:00:04,559] 然后打开你的日历
[00:00:04,559 -> 00:00:06,080] 今天是4月几号
[00:00:06,080 -> 00:00:07,480] 16号
[00:00:07,480 -> 00:00:11,519] 两年之后就是2025年的4月16号
[00:00:11,519 -> 00:00:12,880] 大家在上面留一条
[00:00:13,320 -> 00:00:15,679] 说回来看一下这个视频
[00:00:15,679 -> 00:00:18,320] 我给大家一个建议就是大家去学一个手艺
[00:00:18,320 -> 00:00:20,879] 大家可以两年之后回来看一下
[00:00:21,719 -> 00:00:23,960] 有没有后悔没有听我今天的这个建议
[00:00:28,280 -> 00:00:29,879] 好吧
[00:00:00,000 -> 00:00:03,000] 然後就是怎麼抓住那個
[00:00:03,000 -> 00:00:05,000] 真正大的和重要的一個機會
[00:00:05,000 -> 00:00:06,500] 就是怎麼抓住那個長期的機會
[00:00:06,500 -> 00:00:07,500] 這是我剛剛說的
[00:00:07,500 -> 00:00:09,000] 它不是在於怎麼樣子
[00:00:09,000 -> 00:00:10,000] 讓我們今天的這個工作
[00:00:10,000 -> 00:00:11,500] 工作得更高效一點
[00:00:11,500 -> 00:00:12,500] 而是在於怎麼樣子解決
[00:00:12,500 -> 00:00:15,000] 我們今天完全不可能解決的問題
[00:00:15,000 -> 00:00:17,000] 因為我們今天這些問題
[00:00:17,000 -> 00:00:18,500] 完全不可能得到解決
[00:00:18,500 -> 00:00:20,000] 所以說我們根本就沒有去
[00:00:20,000 -> 00:00:21,500] 思考這些問題
[00:00:21,500 -> 00:00:24,500] 但是真正的機會是在這裡邊的
[00:00:00,000 -> 00:00:08,000] 给大家半分钟的时间去写一下那个日历
[00:00:08,000 -> 00:00:12,919] 然后如果要去洗手间什么的休息的话
[00:00:12,919 -> 00:00:13,960] 我觉得随时来吧
[00:00:13,960 -> 00:00:15,720] 因为我们到时候也是个提问的形式
[00:00:15,759 -> 00:00:16,800] 以及都会录下来
[00:00:16,800 -> 00:00:18,679] 然后大家回头想看的话可以看的
[00:00:18,679 -> 00:00:21,239] 所以说就随意一点
[00:00:28,440 -> 00:00:29,600] 这边是一些个人观点
[00:00:00,000 -> 00:00:01,340] 也不用太在意了
[00:00:02,220 -> 00:00:06,379] 然後第五個就是人類和Chai GPT的區別到底是什麼
[00:00:10,259 -> 00:00:11,220] 暫時先不展開吧
[00:00:11,220 -> 00:00:13,019] 可能到時候提問的時候來提問
[00:00:13,820 -> 00:00:14,519] 行
[00:00:14,519 -> 00:00:16,219] 這是最後一個一頁
[00:00:16,219 -> 00:00:22,820] 就是我們當時在做一件事情的時候呢
[00:00:22,820 -> 00:00:26,320] 人們的反應會是一個偵探分佈
[00:00:26,320 -> 00:00:27,420] 最傻逼的左邊那個
[00:00:00,000 -> 00:00:03,640] 就是做点人们想要的就行了
[00:00:03,640 -> 00:00:06,139] 然后中间就会有一个非常复杂的分析
[00:00:06,139 -> 00:00:07,120] 互称盒是什么呀
[00:00:07,120 -> 00:00:08,779] 这个商业模式的颠覆性是什么呀
[00:00:08,779 -> 00:00:11,099] 然后我们怎么样子去看我们的竞争对手啊
[00:00:11,099 -> 00:00:12,119] 等等等等
[00:00:12,119 -> 00:00:16,440] 然后最后面就是大师会说做个人们想要的东西就行了
[00:00:16,440 -> 00:00:17,239] 对
[00:00:21,440 -> 00:00:22,199] 好
[00:00:22,199 -> 00:00:25,399] 我我的东西就讲完了
[00:00:25,399 -> 00:00:26,800] 现在我就是一个拆纸别提了
[00:00:26,800 -> 00:00:27,559] 大家来问问题吧
[00:00:00,000 -> 00:00:03,279] 我先提一个问题
[00:00:08,279 -> 00:00:14,199] 首先人类它为什么走到今天
[00:00:14,199 -> 00:00:17,480] 并不是因为人类的发展是最先进的
[00:00:17,480 -> 00:00:21,480] 因为曾经有过直立人和智人同时存在
[00:00:21,480 -> 00:00:24,800] 但是直立人有唯一的一个优势就是
[00:00:24,800 -> 00:00:27,079] 我们的语言系统更先进
[00:00:00,000 -> 00:00:04,799] 曾经科学家一直认为我们人类跟其他动物的区别是
[00:00:04,799 -> 00:00:06,799] 我们的大脑有语言中枢
[00:00:06,799 -> 00:00:10,300] 这就是因为这个现象而来的一个误区
[00:00:10,300 -> 00:00:13,000] 所以首先我不觉得人类有什么优势
[00:00:13,000 -> 00:00:17,199] 但其实就是我觉得人类包括其他动物乌鸦
[00:00:17,199 -> 00:00:21,699] 最大的优势就是具有这个抽象任何物体的能力
[00:00:21,699 -> 00:00:24,800] 这是我们随时随地都在做的事情
[00:00:24,800 -> 00:00:28,100] 包括所谓的刻板印象等等等等
[00:00:00,000 -> 00:00:03,000] 就是人類的抽象能力是非常強的
[00:00:03,000 -> 00:00:04,519] 那我想問就是
[00:00:04,519 -> 00:00:09,199] TreadGBT是否具有這種高度抽象物體的能力
[00:00:10,640 -> 00:00:16,160] 你前面那個結論我不確定是你想的還是別人說的
[00:00:16,160 -> 00:00:20,359] 就是人類和其他動物的本質區別是人類具有抽象的能力
[00:00:20,359 -> 00:00:25,239] 我是說人類和其他動物都具有這種能力
[00:00:25,239 -> 00:00:26,320] 都具有抽象能力
[00:00:26,320 -> 00:00:26,760] 對對對
[00:00:26,760 -> 00:00:28,719] 但是TreadGBT是否具有這種能力
[00:00:00,000 -> 00:00:02,160] 我觉得如果他还没有的话
[00:00:02,160 -> 00:00:05,240] 那他距离彻底取代人类还有一定的距离
[00:00:05,240 -> 00:00:06,799] 他不可能彻底取代人类的
[00:00:06,799 -> 00:00:09,919] 就是首先就我没有说他能彻底取代人类
[00:00:09,919 -> 00:00:12,039] 我是那要不我再说一下这个吧
[00:00:12,039 -> 00:00:13,039] 就是呃
[00:00:13,039 -> 00:00:14,800] 拆GBT和人是有很多区别的
[00:00:14,800 -> 00:00:17,399] 包括那个自己的文章列了七个
[00:00:17,760 -> 00:00:18,640] 很重要的区别
[00:00:18,920 -> 00:00:20,600] 就拆GBT的其实是个工具
[00:00:20,640 -> 00:00:22,879] 他不是什么取代人的东西
[00:00:23,120 -> 00:00:25,120] 就是他没有自己的目标
[00:00:25,120 -> 00:00:26,160] 他也没有自己的
[00:00:26,440 -> 00:00:27,039] 啊你要说
[00:00:27,199 -> 00:00:29,000] 可能我没表达很清楚
[00:00:00,000 -> 00:00:03,200] 因为我觉得抽象能力是人类最重要的一个能力
[00:00:03,200 -> 00:00:07,400] 就是我想了解你这个抽象能力是人类最重要的能力
[00:00:07,400 -> 00:00:08,679] 这句话的来源是什么
[00:00:09,240 -> 00:00:11,759] 就从人类的进化史来看就是这样的呀
[00:00:11,759 -> 00:00:12,839] 这是你的观点
[00:00:12,919 -> 00:00:13,519] 对对对
[00:00:13,640 -> 00:00:14,240] 个人观点
[00:00:14,240 -> 00:00:16,839] 我想知道就是Chain-Gb是否有这种能力呢
[00:00:16,879 -> 00:00:20,039] 就是他能否把一件很复杂的事情
[00:00:20,079 -> 00:00:21,719] 描述成很定的很复杂的事情
[00:00:21,719 -> 00:00:23,640] 或者是进行某种对应
[00:00:23,679 -> 00:00:26,120] 而不是他自己总结出一种抽象
[00:00:27,199 -> 00:00:29,280] 我不知道我表达是否清楚
[00:00:00,000 -> 00:00:06,240] 首先我對你那個結論抱有一個懷疑態度
[00:00:06,240 -> 00:00:10,039] 我不認為人類最重要的能力是抽象能力
[00:00:10,039 -> 00:00:12,919] 我不知道啊 可能是 但是我不知道
[00:00:12,919 -> 00:00:16,280] 就是我還沒有足夠的辦法去判斷你的這個結論
[00:00:16,280 -> 00:00:17,960] 然後你剛剛問的問題是
[00:00:17,960 -> 00:00:20,000] Chai GPT是否具有抽象能力
[00:00:20,000 -> 00:00:21,879] 我原來想直接回答有
[00:00:21,879 -> 00:00:25,559] 但是你好像又提了一個抽象能力有不同的層次
[00:00:25,559 -> 00:00:26,399] 這件事兒
[00:00:00,000 -> 00:00:06,299] 就是你不覺得XGBT今天表現出來的這個歸納等等是抽象能力
[00:00:06,299 -> 00:00:08,500] 我也用過,我深度用過
[00:00:08,500 -> 00:00:12,660] 因為我感覺它就是一種對數據的一種生成
[00:00:12,660 -> 00:00:15,660] 它並不具備所謂人類這種帶有
[00:00:15,660 -> 00:00:18,019] 就是我們理解成這種意識的抽象
[00:00:18,019 -> 00:00:19,980] 你能不能舉一個具體的例子
[00:00:19,980 -> 00:00:23,379] 就是你認為人類具備的這個抽象能力是什麼
[00:00:23,379 -> 00:00:24,859] 比方說數學
[00:00:24,859 -> 00:00:28,780] 數學它的最基礎就是幾條公理幾條東西
[00:00:00,000 -> 00:00:04,440] 然後人就可以根據此去不斷的推演各種各種定律
[00:00:04,440 -> 00:00:06,599] 甚至可以應用到現實生活中
[00:00:06,599 -> 00:00:10,560] 但是這一件事情我不確定確確確確是否有這個能力
[00:00:11,800 -> 00:00:13,519] 能取一個就是我們日常
[00:00:13,519 -> 00:00:15,199] 你說我們每天都在抽象是吧
[00:00:15,199 -> 00:00:17,640] 能取一個更具體的嗎
[00:00:17,640 -> 00:00:20,399] 比方說我們比方說廣東人
[00:00:20,399 -> 00:00:22,160] 大家都會覺得幾種印象
[00:00:22,160 -> 00:00:24,120] 大家已經腦袋就浮出來了
[00:00:24,120 -> 00:00:25,480] 比方說東北人
[00:00:25,480 -> 00:00:27,960] 我說出這三個字大家已經有一個概念了
[00:00:27,960 -> 00:00:29,079] 對不對
[00:00:00,000 -> 00:00:02,000] 標籤化我覺得是
[00:00:02,000 -> 00:00:04,000] 嗯
[00:00:04,000 -> 00:00:06,000] 實在不好意思啊
[00:00:06,000 -> 00:00:08,000] 我對這個詞不是很了解
[00:00:08,000 -> 00:00:10,000] 因為我剛剛想你這個
[00:00:10,000 -> 00:00:12,000] 比如說大家說廣東人
[00:00:12,000 -> 00:00:14,000] 想到了幾個對應的東西
[00:00:14,000 -> 00:00:16,000] 第一我們是標籤化
[00:00:16,000 -> 00:00:18,000] 第二我們是把每一個人的具體
[00:00:18,000 -> 00:00:20,000] 每個人都具備了抽象能力
[00:00:20,000 -> 00:00:22,000] 我想說這個問題
[00:00:22,000 -> 00:00:24,000] 就是人類的這一種抽象能力
[00:00:24,000 -> 00:00:26,000] Quality是否具備呢
[00:00:26,000 -> 00:00:28,000] 我是比較懷疑
[00:00:00,000 -> 00:00:01,000] 是這樣想的
[00:00:02,640 -> 00:00:05,080] 我只能具體的說就是
[00:00:05,080 -> 00:00:08,119] 你問他廣東人有什麼特性
[00:00:08,119 -> 00:00:09,839] 他可以給你回答廣東人什麼特性
[00:00:11,320 -> 00:00:11,759] 對吧
[00:00:11,759 -> 00:00:14,599] 然後你問他大家對廣東人有什麼刻板印象
[00:00:14,599 -> 00:00:15,480] 他也能給你回答
[00:00:15,480 -> 00:00:17,760] 廣東人什麼刻板印象
[00:00:20,000 -> 00:00:22,440] 但是我不知道這是不是你想表達那個抽象能力
[00:00:23,879 -> 00:00:24,640] 或者你可以問一下
[00:00:24,640 -> 00:00:26,920] 大家有沒有人就是能理解更好
[00:00:26,920 -> 00:00:28,839] 就是能換一種方式表示你的
[00:00:00,000 -> 00:00:01,600] 表述你说的这个抽象能力
[00:00:01,600 -> 00:00:02,600] 表达不太清楚
[00:00:02,600 -> 00:00:05,000] 不是不是 我觉得我很难理解
[00:00:05,000 -> 00:00:06,599] 因为我抽象能力可能不够
[00:00:06,599 -> 00:00:07,799] 那个就 对
[00:00:09,400 -> 00:00:10,599] 好 我理解的话
[00:00:10,599 -> 00:00:12,599] 这个问题应该是说
[00:00:12,599 -> 00:00:16,000] AI能不能自己做特征工程这方面的事情
[00:00:16,000 -> 00:00:16,399] 能
[00:00:16,399 -> 00:00:18,399] 比如说现在一堆人
[00:00:18,399 -> 00:00:20,600] 有的是东北人 有的是东南人
[00:00:20,600 -> 00:00:22,000] 东北人这些特征
[00:00:22,000 -> 00:00:25,000] 现在的模型训练是人类给到标签
[00:00:25,000 -> 00:00:27,000] 然后包括给到已有的特征
[00:00:27,399 -> 00:00:29,800] 没有 现在也是自动的呀
[00:00:00,000 -> 00:00:02,000] 就是之前的機器學習
[00:00:02,000 -> 00:00:04,400] 就不用拆GPT的話
[00:00:04,400 -> 00:00:06,080] 就像Neural Network
[00:00:06,080 -> 00:00:07,360] 就是Unsupervised Learning
[00:00:07,360 -> 00:00:08,960] 不就是自己去做標籤
[00:00:08,960 -> 00:00:10,640] 所以我想補充的是
[00:00:10,640 -> 00:00:15,279] 我個人覺得是已經可以做到類似的學習
[00:00:16,199 -> 00:00:18,079] 然後其實我拿麥克風
[00:00:18,079 -> 00:00:20,000] 其實我自己有兩個問題想問
[00:00:20,000 -> 00:00:21,519] 不知道那個同學
[00:00:21,519 -> 00:00:23,120] 但剛剛回答一下
[00:00:23,120 -> 00:00:25,079] 就是在拆GPT之前
[00:00:25,079 -> 00:00:27,440] 機器已經能做這種層次的抽象
[00:00:27,440 -> 00:00:29,120] 且它的抽象和人類的抽象
[00:00:00,000 -> 00:00:01,600] 完全不是一個
[00:00:01,600 -> 00:00:03,520] 就是我們是沒有辦法理解機器
[00:00:03,520 -> 00:00:05,000] 去理解那些feature的對吧
[00:00:05,000 -> 00:00:06,559] 就是所以說我們在這
[00:00:06,559 -> 00:00:08,800] 這是IED工作的一個非常奇怪的點
[00:00:08,800 -> 00:00:10,720] 就是我回來以後就會覺得
[00:00:10,720 -> 00:00:13,000] 就明明機器在做很有用的feature
[00:00:13,000 -> 00:00:15,759] 但是我們經常在工作溝通的時候
[00:00:15,759 -> 00:00:18,199] 一定要把它總結成人類可以理解的畫像
[00:00:18,199 -> 00:00:21,359] 最後就會導致我們的這個算法的效果
[00:00:21,359 -> 00:00:23,120] 不如理想中的好
[00:00:23,120 -> 00:00:24,960] 你沒有必要把它強行總結成畫像
[00:00:24,960 -> 00:00:28,079] 但是總之就是機器是有自己去抽象
[00:00:28,079 -> 00:00:29,440] 和歸納人的方式的
[00:00:00,000 -> 00:00:01,879] 如果就是能回答你的问题的话
[00:00:01,879 -> 00:00:04,480] 反正不用拆GPT机器就能干这件事了
[00:00:04,480 -> 00:00:05,599] 好
[00:00:05,599 -> 00:00:08,320] 那接下来是我个人的两个问题
[00:00:08,320 -> 00:00:12,679] 第一个就是假如说前提在一个知识领域
[00:00:12,679 -> 00:00:16,199] 然后消费者在使用大模型的过程中
[00:00:16,199 -> 00:00:18,160] 他不知道大模型这些原理
[00:00:18,160 -> 00:00:20,679] 只知道输入和输出
[00:00:20,679 -> 00:00:23,960] 然后面对90%的查询的问题
[00:00:23,960 -> 00:00:26,519] 比如说今天历史上发生了什么事
[00:00:26,519 -> 00:00:29,920] 那各家的大模型过拆除结果都一样的话
[00:00:00,000 -> 00:00:06,440] 那消费者会不会更加在乎说品牌价值这一些人类更理解的事情
[00:00:06,879 -> 00:00:08,759] 那作为上一层的创业机会的话
[00:00:08,759 -> 00:00:12,640] 我们是否可以从现在开始更加注重品牌价值
[00:00:12,640 -> 00:00:14,480] 就比如说这个大模型
[00:00:14,519 -> 00:00:17,280] 我就是融入了我这样一个品牌
[00:00:17,280 -> 00:00:21,199] 然后消费者在我这里可以得到同样的答案
[00:00:21,239 -> 00:00:24,519] 但是我的答案是带有我的品牌价值的一个思考的
[00:00:24,559 -> 00:00:26,679] 是否会有这样的趋势出现
[00:00:28,320 -> 00:00:28,879] 谢谢啊
[00:00:28,879 -> 00:00:29,800] 很好的问题
[00:00:00,000 -> 00:00:03,680] 第一我不觉得会有很多个大模型
[00:00:03,680 -> 00:00:05,879] 我觉得可能就那么一两家就像云计算一样
[00:00:09,519 -> 00:00:10,320] 不好意思
[00:00:10,320 -> 00:00:14,279] 然后第二就是应该大家给的答案是不一样的
[00:00:14,279 -> 00:00:16,160] 不然的话确实就没什么区别了
[00:00:16,160 -> 00:00:16,719] 对吧
[00:00:17,239 -> 00:00:19,679] 但是第三品牌的价值是不是很重要
[00:00:19,679 -> 00:00:20,719] 我觉得是重要的
[00:00:21,640 -> 00:00:24,320] 但是它未必体现在是哪一家的大模型上
[00:00:24,320 -> 00:00:27,120] 而是就比如说我刚刚说的智力分发
[00:00:00,000 -> 00:00:03,399] 那不同的人不調教出來了不同的GPT
[00:00:03,399 -> 00:00:05,000] 只教你GPT好了
[00:00:05,000 -> 00:00:07,000] 有可能有不同家的這個大模型
[00:00:07,000 -> 00:00:11,400] 那最後這個結果就是我選擇相信誰的大模型的結果
[00:00:11,400 -> 00:00:16,000] 選擇相信誰基於同樣的大模型調教出來的不同的結果的
[00:00:16,000 -> 00:00:18,800] 那估計還是相信背後的這個人
[00:00:18,800 -> 00:00:21,399] 而且我相信就是很多東西都是主觀判斷
[00:00:21,399 -> 00:00:25,300] 舉個例子就是大眾點評出了一個推薦這個
[00:00:25,300 -> 00:00:28,000] 就是基於他的各種各樣數據出了一個推薦
[00:00:00,000 -> 00:00:03,000] 给你推荐餐馆的对话机器人
[00:00:03,000 -> 00:00:07,000] 然后某小红书APP主出了一个
[00:00:07,000 -> 00:00:09,000] 然后不知道腾讯出了一个
[00:00:09,000 -> 00:00:11,000] 那你最后选择谁
[00:00:11,000 -> 00:00:12,000] 你恐怖派还是选择
[00:00:12,000 -> 00:00:14,000] 那个人的推荐是不是合你的胃口
[00:00:14,000 -> 00:00:16,000] 他不是一个基于能力的
[00:00:16,000 -> 00:00:18,000] 而是基于这种match的
[00:00:18,000 -> 00:00:20,000] OK 我还有第二个问题
[00:00:20,000 -> 00:00:23,000] 就是这是一个非常个人化有趣的问题
[00:00:23,000 -> 00:00:26,000] 然后我直接想听一下孙老师您的意见
[00:00:00,000 -> 00:00:04,799] 我在前一段时间跟一个朋友聊天的时候说
[00:00:04,799 -> 00:00:06,519] 我们都比较喜欢喝酒
[00:00:06,519 -> 00:00:09,720] 那假如说我们有一天自己有了小孩
[00:00:09,720 -> 00:00:13,240] 那我们会不会愿意让一个AI在家里带小孩
[00:00:13,240 -> 00:00:16,320] 给我们时间让我们去喝酒去调酒
[00:00:16,320 -> 00:00:19,879] 假如说孙老师您现在遇到这样一个场景
[00:00:19,879 -> 00:00:21,199] 就是说有自己小孩
[00:00:21,199 -> 00:00:24,399] 但你今晚特别想去调酒特别想去喝酒
[00:00:24,399 -> 00:00:28,199] 那您愿不愿意让AI来去在家里带这个小孩呢
[00:00:00,000 -> 00:00:02,000] 這是個陷阱問題
[00:00:02,000 -> 00:00:04,000] 這是什麼呢
[00:00:04,000 -> 00:00:06,000] 我當然願意讓AI去取代我的工作了
[00:00:06,000 -> 00:00:08,000] 當然我可能讓AI去幹活
[00:00:08,000 -> 00:00:10,000] 然後我去調酒
[00:00:10,000 -> 00:00:12,000] 然後我再花很多時間在小孩上
[00:00:12,000 -> 00:00:14,000] 對 我不會去
[00:00:14,000 -> 00:00:16,000] 讓AI去教小孩
[00:00:16,000 -> 00:00:18,000] 是為了讓我可以去調酒
[00:00:18,000 -> 00:00:20,000] 這是另外的取捨 但是你可能 你問題的本質是
[00:00:20,000 -> 00:00:22,000] 我願不願意讓AI帶小孩
[00:00:22,000 -> 00:00:24,000] 對吧 願意
[00:00:24,000 -> 00:00:26,000] 我有啥不願意的
[00:00:26,000 -> 00:00:28,000] 就我們今天 讓孩子看電視
[00:00:00,000 -> 00:00:00,920] 還是用互聯網
[00:00:00,920 -> 00:00:01,919] 這都是工具對吧
[00:00:01,919 -> 00:00:02,640] 他只是另外
[00:00:02,640 -> 00:00:04,639] 在我看來只是另外一個工具而已
[00:00:06,120 -> 00:00:07,080] 我問題結束了
[00:00:11,720 -> 00:00:12,560] 都有機會
[00:00:12,839 -> 00:00:14,439] 您好我有個問題就是
[00:00:14,640 -> 00:00:17,280] 我在現在在用這個Chunk GP的時候
[00:00:17,280 -> 00:00:20,079] 我發現他不能分清哪些是基本事實
[00:00:20,079 -> 00:00:21,839] 哪些是一些
[00:00:21,839 -> 00:00:23,879] 可能他的預料中會有一些很多網友
[00:00:23,879 -> 00:00:25,160] 一些胡謅的一些
[00:00:25,440 -> 00:00:26,640] 一些開玩笑的很多說法
[00:00:26,640 -> 00:00:28,160] 發現他分不清這個東西
[00:00:00,000 -> 00:00:03,339] 在我拿一些問題去問他的時候
[00:00:03,339 -> 00:00:07,339] 他可能給我一個偏離事實很遠的回答
[00:00:07,339 -> 00:00:10,539] 或者說我向他問一些論文的一些
[00:00:10,539 -> 00:00:13,539] 所以他給我都是一些胡謅的一些回答
[00:00:13,539 -> 00:00:14,539] 如何避免這個問題
[00:00:14,539 -> 00:00:17,839] 這個就是很典型的那個hallucination
[00:00:17,839 -> 00:00:18,839] 就是說胡話的問題
[00:00:18,839 -> 00:00:20,839] 然後這個說胡話的問題是一個
[00:00:20,839 -> 00:00:21,839] 很容易解決的問題
[00:00:21,839 -> 00:00:23,339] 其實病已經把它解決了
[00:00:23,339 -> 00:00:24,839] 就是如果你用那個
[00:00:24,839 -> 00:00:26,839] 就是微軟的那個Neo病
[00:00:26,839 -> 00:00:29,339] 它是帶TGPG對話在搜索是吧
[00:00:00,000 -> 00:00:03,040] 它上面給你的答案都是它搜出來的回答
[00:00:03,040 -> 00:00:05,440] 就是你起碼可以去驗證它的好壞
[00:00:05,440 -> 00:00:07,240] 然後出現這個問題是什麼呢
[00:00:07,240 -> 00:00:08,839] 第一是個Demo
[00:00:08,839 -> 00:00:09,119] 對吧
[00:00:09,119 -> 00:00:10,119] 它只是一個Demo
[00:00:10,119 -> 00:00:14,000] 然後它的機制是產生下一個詞和你對話
[00:00:14,000 -> 00:00:15,720] 它不是為了給你準確的信息
[00:00:15,720 -> 00:00:18,960] 所以說不應該把它當成一個搜索引擎去使用它
[00:00:18,960 -> 00:00:22,839] 而是把它當成一個這種對話機器和這個叫Reasoning Machine
[00:00:22,839 -> 00:00:24,760] 就是它可以給你輸出一些
[00:00:24,760 -> 00:00:26,600] 就是思考
[00:00:26,600 -> 00:00:27,839] 它可以給你輸出一些理解
[00:00:27,839 -> 00:00:29,679] 它可以幫你總結歸納知識
[00:00:00,000 -> 00:00:02,560] 但是你不是把它当成一个搜索引擎去使用
[00:00:02,560 -> 00:00:04,799] 它也没有针对这方面进行优化
[00:00:04,799 -> 00:00:08,560] 但是你如果想针对这方面进行优化是一个很简单的事情
[00:00:08,560 -> 00:00:10,119] 比如说你有病就感到了
[00:00:10,119 -> 00:00:13,240] 然后你比如说我想在医学领域去做一个
[00:00:13,240 -> 00:00:15,919] 那我就把高质量的医学的事实给他
[00:00:15,919 -> 00:00:19,519] 然后跟GPT说你在跟人家说话的时候
[00:00:19,519 -> 00:00:21,000] 一定要对一下这里面的事实
[00:00:21,000 -> 00:00:24,199] 然后我教你怎么样子去对这里面的事实就行了
[00:00:24,199 -> 00:00:25,839] 我想补充一个问题
[00:00:25,839 -> 00:00:29,719] 就是Query GPT就是3.5版本
[00:00:00,000 -> 00:00:01,740] 經常有胡亂說話
[00:00:01,740 -> 00:00:04,259] 就是你問他一些事實錯誤的問題
[00:00:04,259 -> 00:00:05,940] 他也會接著瞎編
[00:00:05,940 -> 00:00:09,720] 就是他瞎編這個情況有沒有得到改善
[00:00:09,720 -> 00:00:11,339] 就是你和他問的是一個問題
[00:00:11,339 -> 00:00:12,259] 我剛剛回答了
[00:00:12,259 -> 00:00:14,939] 他已經是通過牛病的問題解決了嗎
[00:00:15,419 -> 00:00:15,939] 對呀
[00:00:16,899 -> 00:00:17,899] 就是他可以解決
[00:00:17,899 -> 00:00:19,339] 就是牛病沒有解決百分之百
[00:00:19,339 -> 00:00:21,059] 因為他有三立合格各方面的限制
[00:00:21,059 -> 00:00:23,339] 但是我是說這是一個簡單的
[00:00:23,339 -> 00:00:24,739] 很容易被解決的問題
[00:00:27,859 -> 00:00:28,739] 那邊哦
[00:00:00,000 -> 00:00:07,000] 我想先回應一下,不是回應,我想先幫第一位同學解釋一下
[00:00:07,000 -> 00:00:13,000] 就是說我理解的,他的抽象問題應該是指的是來源於赫拉利的《本人類簡史》的書
[00:00:13,000 -> 00:00:18,000] 然後赫拉利的原書的意思應該是人類有討論虛構物的能力
[00:00:18,000 -> 00:00:22,000] 就是虛構物的對象的能力,而不是說我有抽象的能力
[00:00:22,000 -> 00:00:25,000] 因為一般,比如說老鼠看到蛇,牠會跑
[00:00:00,000 -> 00:00:04,799] 不可能說老鼠就沒有抽象能力,牠不能抽象蛇這個動物
[00:00:04,799 -> 00:00:09,000] 所以說我想討論
[00:00:09,000 -> 00:00:13,000] 當然這會帶出一個問題
[00:00:13,000 -> 00:00:18,000] 其實GPT,生存式的預訓練模型
[00:00:18,000 -> 00:00:20,000] 它所基於的語料都是符號的
[00:00:20,000 -> 00:00:25,000] 我們的文字本身就是一個抽象的符號
[00:00:25,000 -> 00:00:29,000] 比如說我是蛇,我不可能表述牠是哪一樣蛇
[00:00:00,000 -> 00:00:02,640] 比如說每一片樹葉都是不一樣的
[00:00:02,640 -> 00:00:07,679] 所以說GPT的輸入已經是一個高度抽象跟符號化的結果
[00:00:07,679 -> 00:00:11,279] 那麼我想帶出來的問題是
[00:00:11,279 -> 00:00:15,919] 您這個presentation裡面問到的問題
[00:00:15,919 -> 00:00:17,920] 就是說它是否是一個烏鴉
[00:00:17,920 -> 00:00:22,640] 我看了您的長文啊
[00:00:22,640 -> 00:00:25,920] 我的觀點跟周老師的觀點是類似的
[00:00:25,920 -> 00:00:28,480] 我覺得它不是烏鴉,它只是一個巨鷹
[00:00:00,000 -> 00:00:05,000] 那我分享一下我的觀點,其實跟周老師基本上一樣,他沒有實踐的能力
[00:00:05,000 -> 00:00:12,000] 那麼我讀的書裡面有一本是那個為什麼,就是那個Perl,博爾寫的那個為什麼
[00:00:12,000 -> 00:00:17,000] 基於因果推理的新科學,有這本書
[00:00:17,000 -> 00:00:21,000] 然後它裡面說到了三個因果推理的階段
[00:00:21,000 -> 00:00:28,000] 第一個就是通過這種數據的關聯進行推理
[00:00:00,000 -> 00:00:02,000] 第二個就是通過實踐能力推理
[00:00:02,000 -> 00:00:04,000] 第三個就是通過What if
[00:00:04,000 -> 00:00:06,000] 就是說反式實踐能力推理
[00:00:06,000 -> 00:00:08,000] 那麼實際上
[00:00:08,000 -> 00:00:10,000] 就是
[00:00:10,000 -> 00:00:12,000] GPT嘛就是Per Training
[00:00:12,000 -> 00:00:14,000] 它其實
[00:00:14,000 -> 00:00:16,000] 已經預訓練了,它沒有任何的
[00:00:16,000 -> 00:00:18,000] 實踐的能力在
[00:00:18,000 -> 00:00:20,000] 除非你對後期在做
[00:00:20,000 -> 00:00:22,000] 那個Instruct,可能它會
[00:00:22,000 -> 00:00:24,000] 有一些feedback,但是
[00:00:24,000 -> 00:00:26,000] 我覺得它並不是一個
[00:00:26,000 -> 00:00:28,000] 烏鴉,就是它不能
[00:00:00,000 -> 00:00:03,240] 因為他是pre-training的東西
[00:00:03,240 -> 00:00:05,040] 然後他也沒有實踐的能力
[00:00:05,040 -> 00:00:07,599] 就是說我去擊一個台球
[00:00:07,599 -> 00:00:09,039] 那麼我去擊了這個白球
[00:00:09,039 -> 00:00:10,480] 我撞到一個紅球
[00:00:10,480 -> 00:00:11,640] 他沒有這種能力
[00:00:11,640 -> 00:00:13,039] 就是他沒有感知器
[00:00:13,039 -> 00:00:15,439] 他的所有語料都是文字
[00:00:15,439 -> 00:00:18,480] OK 所以這是我自己的觀點
[00:00:21,960 -> 00:00:24,440] 我覺得很精彩
[00:00:24,440 -> 00:00:26,839] 然後我在想怎麼樣子去
[00:00:26,839 -> 00:00:28,039] 因為你沒問題吧
[00:00:28,039 -> 00:00:29,079] 沒有我問一個問題
[00:00:00,000 -> 00:00:02,040] 我就想怎么评价一下你刚刚说
[00:00:02,040 -> 00:00:05,000] 就是或者说我来讨论一下
[00:00:05,000 -> 00:00:07,240] 就是第一个点我觉得很有趣
[00:00:07,240 -> 00:00:11,720] 就是你把它变成了就是抽象
[00:00:11,720 -> 00:00:13,519] 但是这个我就更容易理解了
[00:00:13,519 -> 00:00:14,800] 就是我们可以去讨论概念
[00:00:14,800 -> 00:00:17,000] 但是其他人不能讨论概念对吧
[00:00:17,000 -> 00:00:18,800] 然后GPT都是来自于符号的
[00:00:18,800 -> 00:00:20,199] 所以说它只是在概念上
[00:00:20,199 -> 00:00:24,440] 它反而今天缺的就是这个现实世界的感知
[00:00:24,440 -> 00:00:25,839] 我觉得这个是很有道理的
[00:00:25,839 -> 00:00:29,199] 以及这可能也是所谓的AGI
[00:00:00,000 -> 00:00:01,740] 就是我們今天像SAM
[00:00:01,740 -> 00:00:03,540] 我覺得CHPT是AGI
[00:00:03,540 -> 00:00:04,540] 因為他已經能
[00:00:04,540 -> 00:00:07,419] 就是在很多事情上表現得像一個
[00:00:07,419 -> 00:00:09,039] 優秀的人類一樣了
[00:00:09,039 -> 00:00:11,039] 我覺得他就是AGI
[00:00:11,039 -> 00:00:13,300] 但是SAM他自己不覺得是AGI
[00:00:13,300 -> 00:00:15,460] 對吧 他覺得他沒有到那個程度
[00:00:15,460 -> 00:00:17,260] 那但是大家都會覺得
[00:00:17,260 -> 00:00:19,379] 你想從今天的CHPT到一個
[00:00:19,379 -> 00:00:20,800] 那個大家想像中的AGI
[00:00:20,800 -> 00:00:22,879] 最重要的就是增加動脈態
[00:00:22,879 -> 00:00:24,679] 增加動脈態最重要的目標
[00:00:24,679 -> 00:00:27,260] 就是為了在現實生活中
[00:00:27,260 -> 00:00:29,379] 或者說是那些具體的東西中
[00:00:00,000 -> 00:00:02,359] 得到具體的信息和具體的反饋
[00:00:02,359 -> 00:00:04,639] 所以說我覺得對這個點很重要
[00:00:04,639 -> 00:00:07,280] 然後確實是現在模型的短板
[00:00:07,280 -> 00:00:09,880] 但是希望未來可以照這個方向去解決
[00:00:09,880 -> 00:00:13,240] 那第二個你說的是不是烏鴉
[00:00:13,240 -> 00:00:15,560] 還是我還是就是我聽懂了
[00:00:15,560 -> 00:00:16,320] 我覺得我聽懂了
[00:00:16,320 -> 00:00:20,480] 就是它按照那個三個因果關係的方式
[00:00:20,480 -> 00:00:20,679] 是吧
[00:00:20,679 -> 00:00:22,879] 它缺第二個和第三個的只有第一個
[00:00:22,879 -> 00:00:24,440] 可是在這種情況下
[00:00:24,440 -> 00:00:26,600] 就是我們之前是
[00:00:26,600 -> 00:00:28,519] 就沒有一種生物
[00:00:00,000 -> 00:00:03,240] 是可以在不接觸現實
[00:00:03,240 -> 00:00:05,639] 不接觸具體的時候直接接觸抽象的
[00:00:05,639 -> 00:00:08,960] 就是人在形成抽象的時候必須要從具體來
[00:00:08,960 -> 00:00:11,640] 所以說我們其實並沒有認真去思考
[00:00:11,640 -> 00:00:15,839] 你的這個具體是不是因果關係的必備條件
[00:00:17,120 -> 00:00:19,160] 但是GPT沒有接觸現實
[00:00:19,160 -> 00:00:20,800] 對呀就是你是不是一個
[00:00:20,800 -> 00:00:22,199] 假設我們今天有一個人
[00:00:22,199 -> 00:00:23,679] 今天這個人是不存在的
[00:00:23,679 -> 00:00:26,320] 因為他必須要先接觸具體才能學習語言
[00:00:26,320 -> 00:00:27,519] 然後才能去思考
[00:00:27,519 -> 00:00:29,399] 如果今天這個人他只思考
[00:00:00,000 -> 00:00:02,819] 就是這個鋼鐘大腦它不需要接觸任何的GPT
[00:00:02,819 -> 00:00:05,440] 它能不能產生因果關係
[00:00:05,440 -> 00:00:07,679] 就是這個鋼鐘大腦能不能產生因果關係
[00:00:07,679 -> 00:00:08,640] 是這個問題
[00:00:08,640 -> 00:00:11,000] 它不存在所以說我們沒法回答
[00:00:11,000 -> 00:00:13,720] 但是我覺得它不能排除這樣的可能性吧
[00:00:13,720 -> 00:00:16,320] 我最後抽象一下
[00:00:16,320 -> 00:00:17,800] 就是我對GPT的理解
[00:00:17,800 -> 00:00:20,559] 就是我抽象成一個這個思維實驗
[00:00:20,559 -> 00:00:22,399] 就是哲學經常搞的這種思維實驗
[00:00:22,399 -> 00:00:24,399] 就是其實就是一個小女孩
[00:00:24,399 -> 00:00:26,600] 她在一個封閉的房間裡面
[00:00:26,600 -> 00:00:29,719] 然後她手頭上有大量的無限的書籍
[00:00:00,000 -> 00:00:02,439] 當然這些書籍可能沒有圖像
[00:00:02,439 -> 00:00:05,200] 因為我只是基於GPT 3.5
[00:00:05,200 -> 00:00:06,160] 那麼他沒有圖像
[00:00:06,160 -> 00:00:09,839] 那麼他能夠對外部的人做出任何的回答
[00:00:09,839 -> 00:00:12,919] 他可以有色彩學完整的一套色彩學知識
[00:00:12,919 -> 00:00:14,199] 他讀的世界上所有書
[00:00:14,199 -> 00:00:16,679] 對他不知道紅色是什麼顏色
[00:00:16,679 -> 00:00:18,879] 對他這個是最核心的問題
[00:00:18,879 -> 00:00:21,559] 他沒有實踐的能力
[00:00:21,559 -> 00:00:24,079] 我覺得這個是GPT的一個侷限性
[00:00:24,079 -> 00:00:25,320] 當然我不確定
[00:00:25,320 -> 00:00:25,800] 等等等等
[00:00:25,800 -> 00:00:27,600] 剛才你那個問題接下來呢
[00:00:27,600 -> 00:00:29,600] 就是你本來想說的是
[00:00:00,000 -> 00:00:02,399] 它有沒有烏鴉的那種
[00:00:02,439 -> 00:00:04,200] 就是 reasoning comprehension
[00:00:04,200 -> 00:00:05,040] deduction
[00:00:05,040 -> 00:00:08,240] 就是我自己用具體的詞去理解
[00:00:08,240 -> 00:00:10,039] 它就是它有推理能力
[00:00:10,160 -> 00:00:11,279] 它有理解能力
[00:00:11,400 -> 00:00:13,279] 然後它有演繹的能力
[00:00:13,560 -> 00:00:14,320] 對
[00:00:14,320 -> 00:00:17,160] 它甚至可能會有一些這個隱身的能力
[00:00:17,320 -> 00:00:17,839] 對
[00:00:18,039 -> 00:00:21,120] 這些東西跟現實其實我不覺得
[00:00:21,160 -> 00:00:23,039] 就是剛剛你說的那個小女孩
[00:00:23,160 -> 00:00:25,679] 你本來的問題是它有沒有這些
[00:00:25,679 -> 00:00:28,440] 就是邏輯抽象或者是等等是吧
[00:00:00,000 -> 00:00:03,600] 但是沒有辦法認識到紅色是什麼顏色
[00:00:03,600 -> 00:00:04,799] 這個問題重要嗎
[00:00:04,799 -> 00:00:07,799] 就是它可以給你完美的描述紅色
[00:00:07,799 -> 00:00:09,800] 它不是一個
[00:00:09,800 -> 00:00:12,599] 我覺得它這樣的一個東西
[00:00:12,599 -> 00:00:14,599] 它不能夠稱為一個 AGI
[00:00:14,599 -> 00:00:16,000] 當然我不知道 AGI 這個定義
[00:00:16,000 -> 00:00:20,000] AGI 就是那個微軟最近出了一個
[00:00:20,000 -> 00:00:22,000] 就是 Evaluate GPT
[00:00:22,000 -> 00:00:23,600] GPT-4 的
[00:00:23,600 -> 00:00:26,000] 然後它在那裡邊開篇
[00:00:26,000 -> 00:00:29,199] 以長篇磊讀的去講我們怎麼定義 AGI
[00:00:00,000 -> 00:00:03,720] 他說他舉了這個市面上所有的
[00:00:03,720 -> 00:00:06,280] 就是主流的去定義AGI的方式
[00:00:06,280 -> 00:00:07,519] 都是有問題的
[00:00:07,519 -> 00:00:09,880] 然後他們選擇了一個可能爭議最小的
[00:00:09,880 -> 00:00:12,000] 那應該是上世紀90年代
[00:00:12,000 -> 00:00:13,039] 還是具體什麼時間忘了
[00:00:13,039 -> 00:00:14,240] 一堆心理學家
[00:00:14,240 -> 00:00:16,879] 然後放到一起去做了定義
[00:00:16,879 -> 00:00:18,519] 其實大概就是
[00:00:19,679 -> 00:00:24,280] 他能不能像人一樣去做人類的任務
[00:00:24,879 -> 00:00:25,440] 對
[00:00:25,440 -> 00:00:26,480] 那在這種情況下
[00:00:26,480 -> 00:00:29,960] 就是我知道他不知道紅色
[00:00:00,000 -> 00:00:02,399] 或者說他不能像我一樣認識紅色
[00:00:02,799 -> 00:00:05,000] 但是他能給你完整的描述紅色
[00:00:05,299 -> 00:00:08,699] 他能基於這個顏色的描述去做所有
[00:00:08,900 -> 00:00:10,800] 顏色描述需要做到的任務
[00:00:11,199 -> 00:00:13,099] 那他現實的他沒有辦法
[00:00:13,199 -> 00:00:14,699] 他沒有辦法識別紅色是嗎
[00:00:14,900 -> 00:00:15,900] 他沒有感受器
[00:00:16,399 -> 00:00:17,600] 就是現在的3.5
[00:00:17,899 -> 00:00:21,000] 對你在描述一個事實
[00:00:21,000 -> 00:00:22,300] 但是這個事實我就說
[00:00:22,300 -> 00:00:23,899] 那他的implication是什麼
[00:00:23,899 -> 00:00:24,399] 就是
[00:00:25,699 -> 00:00:27,000] 比如說他要去調酒
[00:00:28,000 -> 00:00:29,199] 他當然不能去調酒了
[00:00:00,000 -> 00:00:02,960] 我不認為調酒是我去認識他的
[00:00:02,960 -> 00:00:05,160] 我去判斷他的任何的重要的條件
[00:00:05,879 -> 00:00:07,679] 就是所以說認不認識紅色
[00:00:07,679 -> 00:00:10,560] 也不是我判斷他能力的一個條件
[00:00:10,880 -> 00:00:13,759] 就是他他為什麼需要認識紅色呢
[00:00:13,759 -> 00:00:16,800] 就是無法完成完成跟色彩相關的任務
[00:00:16,960 -> 00:00:17,760] 問題就是這樣子
[00:00:17,760 -> 00:00:20,199] 那他可以不需要完成需要必須我要
[00:00:20,199 -> 00:00:23,719] 那您剛剛的定義相違背了
[00:00:23,719 -> 00:00:27,199] 就是說您需要AGI去幫您完成某一項任務
[00:00:27,280 -> 00:00:29,000] 但是我的我的觀點
[00:00:00,000 -> 00:00:01,679] 也就是说他没有实践的能力
[00:00:01,679 -> 00:00:05,759] 所以说他只能够作为一个我的智库这样的一个角色
[00:00:05,759 -> 00:00:09,560] 但是他无法帮我去做动作
[00:00:09,580 -> 00:00:10,800] 我是这么一个观点
[00:00:10,800 -> 00:00:13,720] 我写一个PPT是具体的做动作吗
[00:00:13,740 -> 00:00:14,320] 对对的
[00:00:14,320 -> 00:00:16,879] 我知道他现在确实有很多
[00:00:16,879 -> 00:00:20,320] 不就是我在想我的这个公司是怎么来的
[00:00:20,320 -> 00:00:22,800] 一大半都是来自于我的PPT
[00:00:22,800 -> 00:00:25,120] 那他有没有做动作呢
[00:00:25,120 -> 00:00:26,640] PPT他
[00:00:00,000 -> 00:00:06,519] 首先PPT它是一個文字性的工作
[00:00:06,519 -> 00:00:08,480] 我還是回到色彩學
[00:00:08,480 -> 00:00:11,000] 就是他沒有辦法認識到紅色
[00:00:11,000 -> 00:00:14,279] 他能不能操作我的電腦
[00:00:14,279 -> 00:00:16,320] 然後給我的PPT做出美觀
[00:00:16,320 -> 00:00:17,800] 我說什麼樣的東西是美觀的
[00:00:17,800 -> 00:00:19,719] 然後他操作我的電腦
[00:00:19,739 -> 00:00:22,399] 通過顏色把我的PPT調到一個好看的
[00:00:23,679 -> 00:00:24,120] 可以吧
[00:00:24,120 -> 00:00:24,679] 類似吧
[00:00:24,679 -> 00:00:28,160] 但是當然計算機內肯定是有符號
[00:00:00,000 -> 00:00:02,060] 他計算機可以就是計算機
[00:00:02,060 -> 00:00:04,280] 就是您發出一個紅色指令
[00:00:04,280 -> 00:00:07,919] 他計算機可能他會能夠識別這個指令
[00:00:07,919 -> 00:00:11,199] 就是說GPT給計算機發一個紅色指令
[00:00:11,199 -> 00:00:14,240] 對他可能可以會是可以識別
[00:00:14,240 -> 00:00:15,720] 對就是
[00:00:15,720 -> 00:00:19,839] 所以就是剛剛回到我們兩個可能有那麼一細微的不同
[00:00:19,839 -> 00:00:23,199] 就是你覺得他必須要像我一樣理解紅色
[00:00:23,199 -> 00:00:24,800] 他才能有用
[00:00:24,800 -> 00:00:26,000] 他才能實踐
[00:00:26,000 -> 00:00:26,760] 他才能有用
[00:00:00,000 -> 00:00:02,720] 現實社會互動 現實空間
[00:00:02,720 -> 00:00:04,719] 好 我再說一個
[00:00:04,719 -> 00:00:06,919] 我可能背後有一個另外的理解
[00:00:06,919 -> 00:00:09,279] 就是有一個App主是老蔣聚靠普
[00:00:09,279 -> 00:00:10,640] 然後他有說我們這個
[00:00:10,640 -> 00:00:13,039] 其實我們是活在四個世界裡面
[00:00:13,039 -> 00:00:14,759] 一個是現實世界
[00:00:14,759 -> 00:00:17,120] 一個是感知世界
[00:00:17,120 -> 00:00:18,440] 就是現實世界是現實
[00:00:18,440 -> 00:00:20,399] 但是我睜開眼看到了以後
[00:00:20,399 -> 00:00:21,760] 它變成了我的感知
[00:00:21,760 -> 00:00:23,120] 這是我看到的東西
[00:00:23,120 -> 00:00:25,039] 然後第三個是意義世界
[00:00:25,039 -> 00:00:27,519] 然後說在這個計算機發現了以後
[00:00:27,519 -> 00:00:29,039] 有一個數字世界
[00:00:00,000 -> 00:00:02,000] 然后这四个世界会互相挤占
[00:00:02,520 -> 00:00:03,240] 我现在说呢
[00:00:03,240 -> 00:00:05,160] 就是我的绝大多数工作
[00:00:05,160 -> 00:00:07,960] 其实是在数字世界和意义世界里面完成的
[00:00:08,199 -> 00:00:11,519] 这是我拿到工资的绝大多数的来源
[00:00:11,800 -> 00:00:15,000] 就是我也不需要去现实世界中改变任何事
[00:00:15,640 -> 00:00:18,760] 我的PPT是我的工资的来源
[00:00:20,320 -> 00:00:25,920] Chai GPT它的这个能力是可以在这个意义世界和
[00:00:26,320 -> 00:00:29,399] 这个数字世界里面去进行操作的对吧
[00:00:00,000 -> 00:00:02,759] 它沒有辦法去觸碰到那個真實世界
[00:00:02,759 -> 00:00:03,839] 它也沒有感知
[00:00:03,839 -> 00:00:05,559] 或者說它的感知方式和我們不一樣
[00:00:05,559 -> 00:00:07,080] 它只能通過我們的符號
[00:00:07,080 -> 00:00:09,359] 和我們通過其他的知識去進行感知
[00:00:09,359 -> 00:00:11,160] 但是它能在這裡邊去完成任務
[00:00:11,160 -> 00:00:13,439] 它能在上兩層世界完成任務
[00:00:13,439 -> 00:00:15,160] 這是我的可能和你的區別
[00:00:15,160 -> 00:00:18,399] 就是在我看來這樣的東西
[00:00:18,399 -> 00:00:20,920] 就已經是一個在這兩個世界上
[00:00:20,920 -> 00:00:23,800] 足夠有用且足夠能幫助到人類的方式了
[00:00:23,800 -> 00:00:27,079] 它不需要去跟真實世界去產生交互
[00:00:27,079 -> 00:00:29,559] 可能我倆對於AGI的定義有些不一樣
[00:00:00,000 -> 00:00:04,000] 沒有人定義是一樣的 這是問題
[00:00:04,000 -> 00:00:09,000] 或者說我也不想就是去用AGI這樣一個
[00:00:09,000 -> 00:00:12,000] 大家沒有準確定義的東西去說它
[00:00:12,000 -> 00:00:15,000] 我說的是我會去思考它能做到什麼
[00:00:15,000 -> 00:00:17,000] 然後我會思考
[00:00:17,000 -> 00:00:19,000] 它已經能賺我70%的薪水了
[00:00:19,000 -> 00:00:23,000] 我帶線上的同學問一下
[00:00:00,000 -> 00:00:11,000] GVT有人類普遍認知範圍內的審美能力嗎?
[00:00:11,000 -> 00:00:16,000] 在文學、繪畫、音樂等等方面有可能爆發出emergency嗎?
[00:00:16,000 -> 00:00:19,000] 什麼是人類普遍認知的審美能力?
[00:00:19,000 -> 00:00:26,000] 這個孫老師可以以您的理解來回答
[00:00:00,000 -> 00:00:02,000] 我也只是個代問
[00:00:02,000 -> 00:00:03,000] 我也只是個代問
[00:00:03,000 -> 00:00:06,000] 這是一個非常之深刻的哲學問題
[00:00:06,000 -> 00:00:08,000] 就是人類普遍
[00:00:08,000 -> 00:00:10,000] 審美是主觀還是客觀的
[00:00:10,000 -> 00:00:11,000] 這個有答案嗎
[00:00:14,000 -> 00:00:16,000] 我只是個代問
[00:00:16,000 -> 00:00:20,000] 我希望他能回答一下
[00:00:20,000 -> 00:00:22,000] 那我們現場問題優先一些
[00:00:22,000 -> 00:00:23,000] 好的
[00:00:24,000 -> 00:00:26,000] 話說剛才那個問題
[00:00:26,000 -> 00:00:27,000] 我自己的回答就是
[00:00:27,000 -> 00:00:29,000] 審美是主觀的還是客觀的
[00:00:00,000 -> 00:00:04,799] 最後的答案是主觀和客觀是一個錯誤的分類方式
[00:00:04,799 -> 00:00:07,099] 就是審美才是唯一的
[00:00:07,099 -> 00:00:09,000] 然後主觀和客觀其實不重要
[00:00:09,000 -> 00:00:11,800] 你好 孫老師你好
[00:00:11,800 -> 00:00:13,500] 然後我是一個就是
[00:00:13,500 -> 00:00:15,500] 因為我不像其他同學那樣
[00:00:15,500 -> 00:00:17,500] 就那麼高深的去想一些
[00:00:17,500 -> 00:00:18,800] 裡面深入一些知識
[00:00:18,800 -> 00:00:20,600] 我做一個普通的普通人
[00:00:20,600 -> 00:00:22,600] 我想關注的是一個
[00:00:22,600 -> 00:00:24,600] 我們都是狗
[00:00:24,600 -> 00:00:27,100] 對 就是一個開發領域
[00:00:00,000 -> 00:00:04,500] 或者是一個manly的角度去想AI
[00:00:04,500 -> 00:00:07,500] 然後我這裡有三個小問題
[00:00:07,500 -> 00:00:13,300] 第一個是說最近湧現出大量的魔法寫作者
[00:00:13,300 -> 00:00:17,859] 類似於說怎麼樣好利用那個指令去
[00:00:17,859 -> 00:00:19,859] 吩咐AI為我們做事情
[00:00:19,859 -> 00:00:21,859] 那這個語言的話
[00:00:21,859 -> 00:00:26,260] 我們有沒有必要去深入學習
[00:00:00,000 -> 00:00:01,740] 或者是在
[00:00:01,740 -> 00:00:03,779] 因為以後的平台會越來越多
[00:00:03,779 -> 00:00:04,740] 4.0也好
[00:00:04,740 -> 00:00:08,279] 然後Google的Gamma或者Newbeam什麼
[00:00:08,279 -> 00:00:09,339] 就越來越多平台
[00:00:09,339 -> 00:00:11,939] 那我們現在現階段有沒有必要去說
[00:00:11,939 -> 00:00:13,640] 去學這個魔法語言
[00:00:13,640 -> 00:00:18,339] 這個魔法語言有很多大量工作機會
[00:00:18,339 -> 00:00:19,440] 在市場中
[00:00:19,440 -> 00:00:20,179] 特別在美國
[00:00:20,179 -> 00:00:23,239] 可能現在中國可能稍微沒那麼流行
[00:00:23,239 -> 00:00:26,379] 但是我看到已經很多那種機會在裡面
[00:00:26,379 -> 00:00:28,899] 所以我想這是第一個問題
[00:00:00,000 -> 00:00:02,799] 然後我是一個一個回還是三個問題是相關的
[00:00:02,799 -> 00:00:04,000] 所以我聽完三個再回
[00:00:04,639 -> 00:00:06,559] 哦就就看看看老師念
[00:00:06,559 -> 00:00:07,280] 我怕記不住
[00:00:07,280 -> 00:00:08,000] 哈哈對
[00:00:08,800 -> 00:00:10,359] 有相關的話我就聽完三個
[00:00:10,359 -> 00:00:12,080] 不是那麼相關的話我就一個一個回
[00:00:12,400 -> 00:00:14,960] 呃可能還是先算一下吧
[00:00:14,960 -> 00:00:16,079] 我先簡單回一下吧
[00:00:16,079 -> 00:00:19,280] 然後就是然後再你可以到時候你再一起問
[00:00:19,399 -> 00:00:21,079] 啊就第一他沒有賺那麼多錢
[00:00:21,079 -> 00:00:22,760] 那些那些新聞是一些
[00:00:23,280 -> 00:00:24,239] 博眼球的新聞
[00:00:24,440 -> 00:00:25,960] 就沒有出現那麼多工作細回
[00:00:25,960 -> 00:00:27,000] 然後那些照片
[00:00:27,199 -> 00:00:27,800] 可能可能
[00:00:00,000 -> 00:00:02,560] 也不知道就是他可能今天照片就没有了
[00:00:02,560 -> 00:00:05,400] 但是更重要的就是他的这个prompting
[00:00:05,400 -> 00:00:06,879] 对吧就是你怎么样去跟他对话
[00:00:06,879 -> 00:00:09,720] 然后去我们很明显看到不同人跟他对话
[00:00:09,720 -> 00:00:11,560] 他的质量会非常的差的非常大
[00:00:11,560 -> 00:00:13,359] 甚至我跟他用不同的方式对话
[00:00:13,359 -> 00:00:14,919] 我得到的答案也会差的非常大
[00:00:14,919 -> 00:00:18,079] 然后我这更深入的一个思考就是
[00:00:18,079 -> 00:00:19,839] 他到底是一个更工程向的能力
[00:00:19,839 -> 00:00:21,920] 还是一个更PM向的能力
[00:00:21,920 -> 00:00:24,280] 就是你需要用产品经营的方式去跟他对话
[00:00:24,280 -> 00:00:26,160] 还是用工程师的方式去跟他对话
[00:00:26,160 -> 00:00:27,600] 区别是什么呢
[00:00:00,000 -> 00:00:02,200] 那就是工程师问的是
[00:00:02,200 -> 00:00:04,799] 去跟他对话的方式是教他怎么做
[00:00:04,799 -> 00:00:07,400] 就是我在这有这样的几个做法
[00:00:07,400 -> 00:00:08,300] 我告诉你怎么做
[00:00:08,300 -> 00:00:11,300] 然后产品经理呢是我要什么东西
[00:00:11,300 -> 00:00:13,400] 我告诉你什么是我想要的
[00:00:13,400 -> 00:00:14,400] 什么是我不想要的
[00:00:14,400 -> 00:00:17,899] 今天绝大多数的好的这个prompting
[00:00:17,899 -> 00:00:18,699] 都是工程能力
[00:00:18,699 -> 00:00:19,699] 都是告诉他怎么做
[00:00:19,699 -> 00:00:21,500] 我相信未来会越来越多的
[00:00:21,500 -> 00:00:23,399] 我告诉你我要什么
[00:00:23,399 -> 00:00:25,300] 因为他自己模型能力在上升
[00:00:25,300 -> 00:00:27,899] 大概是一个这种感觉
[00:00:27,899 -> 00:00:28,899] 谢谢
[00:00:00,000 -> 00:00:06,780] 然後,然後孟老師第二個問題是說,如果就是老師您以後要創業的話
[00:00:06,780 -> 00:00:07,580] 就結合……
[00:00:07,580 -> 00:00:08,380] 沒有啊
[00:00:08,380 -> 00:00:13,380] 就打個比方,就打個比方,就是副業也好,什麼也好,就是anyway
[00:00:13,380 -> 00:00:20,780] 然後結合如果現在AI的話,以老師您的這個現在的認知或者是您的高度
[00:00:20,780 -> 00:00:24,379] 在未來十年的話,對AI這個領域結合
[00:00:00,000 -> 00:00:02,960] 因為我的個人觀點是
[00:00:02,960 -> 00:00:05,679] 我認為的上一個起點應該是
[00:00:05,679 -> 00:00:08,439] Internet 就是互聯網家
[00:00:08,439 -> 00:00:11,480] 那我覺得以後的未來市場應該是AI家
[00:00:11,480 -> 00:00:13,380] 那有什麼行業是
[00:00:13,380 -> 00:00:15,919] 孫老師您覺得如果您要創業
[00:00:15,919 -> 00:00:19,960] 您覺得對於我們一般要有什麼領域
[00:00:19,960 -> 00:00:22,160] 可以覺得前提是比較好的
[00:00:22,160 -> 00:00:22,920] 我
[00:00:22,920 -> 00:00:27,920] 對 然後例如我現在有一些領域上的
[00:00:00,000 -> 00:00:04,320] 例如AI結合股票、AI結合彩票
[00:00:04,320 -> 00:00:06,320] 就是這種
[00:00:06,320 -> 00:00:08,320] 腦洞
[00:00:08,320 -> 00:00:10,320] 然後結合這種應用開發
[00:00:10,320 -> 00:00:12,320] 如果是對於
[00:00:12,320 -> 00:00:16,559] 因為我不清楚AI對於預測能力的一些發展
[00:00:16,559 -> 00:00:18,559] 或者是它的一些能力
[00:00:18,559 -> 00:00:22,800] 所以我在這個領域上會有點想要問孫老師您的
[00:00:22,800 -> 00:00:24,800] 然後還有一個相關
[00:00:24,800 -> 00:00:26,800] 就是跟這個一樣的
[00:00:26,800 -> 00:00:28,800] 這幾天出來一個叫auto
[00:00:00,000 -> 00:00:06,240] 跟現在GDP 4.0具體有什麼差別?
[00:00:06,240 -> 00:00:07,839] 就這麼樣
[00:00:07,839 -> 00:00:10,320] 我先回答第三個吧,簡單一點
[00:00:10,320 -> 00:00:14,080] 它是基於GPT的能力的一個應用方式
[00:00:14,080 -> 00:00:18,640] 對,就是你需要調GPT或GPT 4的API
[00:00:18,640 -> 00:00:20,960] 你如果沒有GPT 4的API,你用不了那個東西
[00:00:20,960 -> 00:00:24,800] 你也不能用GPT 4的方式去用那個東西
[00:00:24,800 -> 00:00:26,320] 那個東西所謂做的東西
[00:00:00,000 -> 00:00:05,719] 他做的就是去把你的任务用他的东西去拆解
[00:00:05,719 -> 00:00:07,839] 然后不断的去问GPT怎么做
[00:00:07,839 -> 00:00:08,919] 然后再去执行
[00:00:09,199 -> 00:00:11,560] 其实所以叫Auto GPT
[00:00:11,679 -> 00:00:12,119] 对
[00:00:12,119 -> 00:00:16,760] 就是你原来可能是就是让一个人在没有想清楚怎么做的时候
[00:00:16,760 -> 00:00:18,239] 让GPT帮他想怎么做
[00:00:18,280 -> 00:00:19,559] 大概是这样的一个东西
[00:00:19,839 -> 00:00:21,559] 回到第二点
[00:00:21,760 -> 00:00:26,079] 我自己没有去思考这个问题
[00:00:26,079 -> 00:00:26,719] 为什么
[00:00:26,760 -> 00:00:28,920] 因为我觉得创业最重要的一点
[00:00:00,000 -> 00:00:02,500] 最重要的兩點,一個是Parallel Market Fit
[00:00:02,500 -> 00:00:04,799] 就是你市場和這個產品是要結合的
[00:00:04,799 -> 00:00:07,799] 當然還有一個同樣重要的點就是Founder Market Fit
[00:00:07,799 -> 00:00:10,500] 就是你自己要適合做這件事
[00:00:10,500 -> 00:00:12,300] 股票加AI跟我有啥關係
[00:00:12,300 -> 00:00:13,800] 就根本不是我的機會
[00:00:13,800 -> 00:00:16,500] 彩票加AI更不是我的這個機會了對吧
[00:00:16,500 -> 00:00:18,500] 所以說我今天看到的所有東西
[00:00:18,500 -> 00:00:20,500] 我不覺得那些東西屬於我
[00:00:20,500 -> 00:00:22,500] 如果我看到了我可能會去創業
[00:00:22,500 -> 00:00:24,500] 但是我沒有看到任何屬於我的東西
[00:00:24,500 -> 00:00:27,300] 就其實想要了解那個
[00:00:00,000 -> 00:00:05,519] AI對預測性就是這種預測或者基於大數據會對未來預測那個
[00:00:05,519 -> 00:00:10,960] 呃判斷或他能力能去到一個現階段能去到一個哪裡的一個總和
[00:00:10,960 -> 00:00:12,320] 預測股價嗎
[00:00:12,320 -> 00:00:16,320] 呃可能打個比方可能有一些小項目可能是
[00:00:16,320 -> 00:00:20,120] 大家一起拼團去買一個什麼六二彩
[00:00:20,120 -> 00:00:21,120] 那
[00:00:21,120 -> 00:00:22,519] 預測不了
[00:00:22,519 -> 00:00:23,719] 預測不了
[00:00:23,719 -> 00:00:24,600] 是完全不可能
[00:00:24,600 -> 00:00:28,320] 除非他的那個彩票本身的那個就是
[00:00:00,000 -> 00:00:03,200] 生成彩票號碼的機制本身是有規律的
[00:00:03,200 -> 00:00:06,540] 但是六合彩最基本的大家會去買的核心假設
[00:00:06,540 -> 00:00:08,160] 就是它的那個是沒有規律的
[00:00:08,740 -> 00:00:10,759] 所以說如果不存在這個規律
[00:00:10,759 -> 00:00:12,160] 他也沒有辦法抓到這個規律
[00:00:14,880 -> 00:00:18,000] 對 剛剛沒有說完的就是
[00:00:18,000 -> 00:00:19,079] 不是我的機會
[00:00:19,079 -> 00:00:21,000] 但是我覺得我要做的很重要的事情
[00:00:21,000 -> 00:00:23,579] 剛給大家建議就是學一門手藝
[00:00:25,500 -> 00:00:26,920] 學手藝是很重要的
[00:00:26,920 -> 00:00:29,379] 再一個就是如果說GPT真的帶來了
[00:00:00,000 -> 00:00:01,340] 那提高自己的判斷力
[00:00:01,340 -> 00:00:02,839] 提高自己的平凡性思維
[00:00:02,839 -> 00:00:04,339] 會非常非常重要
[00:00:04,339 -> 00:00:05,540] 因為只有這個東西
[00:00:05,540 -> 00:00:07,540] 才是你和其他人的區別
[00:00:07,540 -> 00:00:08,839] 不然的話你只是知識
[00:00:08,839 -> 00:00:10,000] 或者簡單的性能
[00:00:10,000 -> 00:00:11,339] 絕對是大家都一樣的
[00:00:11,339 -> 00:00:12,500] 很快就會被
[00:00:12,500 -> 00:00:14,339] 只要GPT不是在做那個
[00:00:14,339 -> 00:00:15,380] 現實世界中的東西
[00:00:15,380 -> 00:00:16,379] 是吧 在異議世界裡面
[00:00:16,379 -> 00:00:17,539] 很快都可以取代的
[00:00:17,539 -> 00:00:19,539] 那你和別人的區別
[00:00:19,539 -> 00:00:21,039] 和為什麼你調教出來
[00:00:21,039 -> 00:00:22,339] GPT和別人不一樣
[00:00:22,339 -> 00:00:23,339] 那就是你的判斷力
[00:00:23,339 -> 00:00:24,339] 和平凡性思維了
[00:00:24,339 -> 00:00:25,339] 好 謝謝
[00:00:25,339 -> 00:00:26,339] 謝謝書德兄
[00:00:00,000 -> 00:00:10,000] 老師你好,就是可能剛才咱們聊的可能是一些偏理論的一些東西,可能就是對於我來說,我可能想請教幾個就是偏應用側的一些問題。
[00:00:10,000 -> 00:00:19,000] 其實我的第一個問題就是說,既然GPT它是基於咱們的一些內容去學習,先得到一個答案,可以這樣說。
[00:00:19,000 -> 00:00:25,000] 那麼是否會存在就是說,當人們大量應用GPT的時候,大家產出的內容都是一致的。
[00:00:00,000 -> 00:00:02,000] 那么这个时候他再去学习
[00:00:02,000 -> 00:00:04,000] 那他这个内容是否能产生
[00:00:04,000 -> 00:00:06,000] 其实我的一个观点就是说
[00:00:06,000 -> 00:00:11,000] 他是否会对人类现有的一些可能普通的真理性的一些内容
[00:00:11,000 -> 00:00:13,000] 产生了不一样的一些看法
[00:00:13,000 -> 00:00:16,000] 但是因为觉得这个是对的人变多了
[00:00:16,000 -> 00:00:17,000] 他们就变成了真理
[00:00:17,000 -> 00:00:19,000] 会不会有这种可能性
[00:00:19,000 -> 00:00:21,000] 我一个一个回答还是
[00:00:21,000 -> 00:00:22,000] 啊好啊
[00:00:22,000 -> 00:00:23,000] 我觉得这个也是个很好的问题
[00:00:23,000 -> 00:00:26,000] 我也经过几次的认知的想法吧
[00:00:26,000 -> 00:00:28,000] 就是第一我第一次一开始
[00:00:00,000 -> 00:00:02,759] 一開始我其實是個GPT降臨派
[00:00:02,759 -> 00:00:04,320] 就是立場盛名
[00:00:04,320 -> 00:00:07,480] 我是很希望GPT來多取代一些人的
[00:00:07,480 -> 00:00:08,000] 為什麼
[00:00:08,000 -> 00:00:09,640] 因為我看內容
[00:00:09,640 -> 00:00:10,720] 我看小紅書
[00:00:10,720 -> 00:00:12,240] 出來了一條爆款內容
[00:00:12,240 -> 00:00:13,720] 誇全都是類似的爆款內容
[00:00:13,720 -> 00:00:15,519] 然後又加很多假的
[00:00:15,519 -> 00:00:17,839] 就比如說你要上面去搜減肥搜健身
[00:00:17,839 -> 00:00:19,359] 一大半的人是騙你的吧
[00:00:19,359 -> 00:00:19,839] 對吧
[00:00:19,839 -> 00:00:21,640] 所以說我覺得取代這些內容
[00:00:21,640 -> 00:00:22,600] 那是一件好事
[00:00:22,600 -> 00:00:23,440] 功德無量
[00:00:23,440 -> 00:00:25,079] 然後後來我又在想
[00:00:25,079 -> 00:00:28,280] 確實就是會有很多人去產生那些高質量的內容
[00:00:00,000 -> 00:00:04,799] 但是如果GPT去把這些人的創作動機給取代了
[00:00:04,799 -> 00:00:06,759] 那這些人就不會產生了 對吧
[00:00:06,759 -> 00:00:08,960] 那確實就是你剛剛說的這個問題
[00:00:08,960 -> 00:00:10,119] 但是我後來在想
[00:00:10,119 -> 00:00:12,119] 就是說或者說之前聽了一個播客
[00:00:12,119 -> 00:00:14,400] 是那個清華的劉嘉老師
[00:00:14,400 -> 00:00:17,160] 我很推薦在小宇宙上叫劉嘉
[00:00:17,160 -> 00:00:18,320] 就是那個嘉賓的嘉
[00:00:18,320 -> 00:00:19,679] 劉嘉老師的播客
[00:00:19,679 -> 00:00:24,480] 他就講說他是做這個認知科學的這些
[00:00:24,480 -> 00:00:26,039] 然後心理學
[00:00:26,039 -> 00:00:28,039] 然後他就說我們其實每個人都知道
[00:00:28,039 -> 00:00:28,879] 素質教育重要
[00:00:00,000 -> 00:00:02,640] 但是我們學校裡邊到現在都不教素質教育
[00:00:02,919 -> 00:00:03,439] 為啥
[00:00:04,120 -> 00:00:05,080] 因為高考不考
[00:00:05,559 -> 00:00:07,480] 對 所以說為什麼高考不考
[00:00:07,480 -> 00:00:09,679] 因為那些東西好考 那些東西容易量化
[00:00:09,679 -> 00:00:11,400] 你的知識技能那些東西容易量化
[00:00:11,400 -> 00:00:12,880] 你的素質是很難量化的
[00:00:13,160 -> 00:00:15,480] 批判性思維是很難量化的 對吧
[00:00:15,720 -> 00:00:17,679] 我們跟一個人聊三天天也不一定知道
[00:00:17,679 -> 00:00:18,559] 他有沒有批判性思維
[00:00:18,559 -> 00:00:19,839] 而在輔助別人觀點
[00:00:20,039 -> 00:00:24,879] 所以說當你的那些知識這些東西沒有用了以後
[00:00:24,879 -> 00:00:26,879] 人們如果是要去改變
[00:00:26,879 -> 00:00:28,719] 人們是要再適應這個新環境
[00:00:00,000 -> 00:00:04,000] 可能就是要去多注重批判性這些東西
[00:00:04,000 -> 00:00:07,000] 那可能反而會促使那些
[00:00:07,000 -> 00:00:09,599] 所謂的有原創性內容的人
[00:00:09,599 -> 00:00:11,000] 他們變得更重要
[00:00:11,000 -> 00:00:15,000] 和就是更多的人會變成像他們那樣的人吧
[00:00:15,000 -> 00:00:15,800] 不猜
[00:00:15,800 -> 00:00:16,800] 這是我的希望啊
[00:00:16,800 -> 00:00:19,000] 就是還有可能是實施另一個發展
[00:00:19,000 -> 00:00:20,600] 好的我可能還有兩個
[00:00:20,600 -> 00:00:22,000] 就是可能沒那麼相關的問題
[00:00:22,000 -> 00:00:23,000] 也想請教一下
[00:00:23,000 -> 00:00:26,000] 就是說最近包括像百度啊
[00:00:26,000 -> 00:00:28,000] 像阿里比如說百度的文青一言
[00:00:28,000 -> 00:00:29,600] 然後阿里的同意千問
[00:00:00,000 -> 00:00:06,599] 它其实都是基于这些可能大语言模型去得到的一些这样的产品
[00:00:06,599 -> 00:00:12,400] 那么可能赞老师你看他们之间最目前来说他们之间的差距大概是什么样子
[00:00:12,400 -> 00:00:14,900] 包括他们之间的一些合作差距是什么
[00:00:15,400 -> 00:00:18,500] 我有一个视频专门说这个
[00:00:18,500 -> 00:00:23,300] 然后我的那个就是主标题是国产大模大模线有希望吗
[00:00:23,500 -> 00:00:27,000] 然后我的副标题是中国国足能进世界杯能拿世界杯吗
[00:00:00,000 -> 00:00:04,799] 就是這樣就是就是你會看大家都是在踢球對吧
[00:00:04,799 -> 00:00:07,500] 但是踢球的水平是差很大的
[00:00:07,500 -> 00:00:10,400] 就是這種感覺就是大家都看上去在對話
[00:00:10,400 -> 00:00:15,000] 但是背後的那個就是智慧程度差很大
[00:00:15,000 -> 00:00:19,800] 嗯好那老師我可能有最後一個問題就是說包括像咱們就是說
[00:00:19,800 -> 00:00:22,399] China GPT 然後國內的這些大型模型
[00:00:22,399 -> 00:00:27,699] 那麼其實會不會說這些產品他們所帶來的應用場景
[00:00:27,699 -> 00:00:29,500] 也可以做一些差異化的一些東西
[00:00:00,000 -> 00:00:02,799] 比如說GPT它可能更注重一些
[00:00:02,799 -> 00:00:05,200] 可能它的某些能力更厲害
[00:00:05,200 -> 00:00:08,000] 那麼它對應的一些場景
[00:00:08,000 -> 00:00:10,699] 可能就不是做小紅書內容這種場景
[00:00:10,699 -> 00:00:13,000] 那假如說像是這種通銀千問
[00:00:13,000 -> 00:00:16,699] 它可能就是適合國內的這種中文的這種語言模型
[00:00:16,699 -> 00:00:19,300] 它是不是會對這種小紅書的文案優化
[00:00:19,300 -> 00:00:21,399] 然後甚至說一些這種商品
[00:00:21,399 -> 00:00:24,399] 類似現在一些做外貿的商品詳細的優化
[00:00:24,399 -> 00:00:28,199] 會不會就是說他們的一些應用場景有一些差異化呢
[00:00:00,000 -> 00:00:04,799] 我在想要不要讲这个
[00:00:04,799 -> 00:00:07,799] 我来这样通过打比方吧
[00:00:07,799 -> 00:00:12,000] 就是不会
[00:00:12,000 -> 00:00:20,000] 因为你找一个聪明高潜的同学干任何事情
[00:00:20,000 -> 00:00:24,600] 他都会比一个智障干的好
[00:00:24,600 -> 00:00:27,800] 无论他是做小红书还是做什么东西
[00:00:00,000 -> 00:00:03,879] 我的观点可能就是说就是在于这种语言
[00:00:03,879 -> 00:00:06,320] 就是比如说中文和英文这种没有区别
[00:00:06,320 -> 00:00:06,960] 没有区别是吧
[00:00:06,960 -> 00:00:07,480] 没有区别
[00:00:07,480 -> 00:00:09,640] 就是我自己读博士的时候
[00:00:09,640 -> 00:00:13,400] 我看中国问题是中国人写的文章
[00:00:13,400 -> 00:00:14,640] 但是都是用英文看的
[00:00:15,000 -> 00:00:17,239] 就是高学校论文都是用英文发表的
[00:00:17,280 -> 00:00:20,800] 我的思维方式就是我在我在思考的时候
[00:00:20,800 -> 00:00:21,920] 我仍然是在用中文思考
[00:00:21,920 -> 00:00:23,160] 有的时候可能变成了英文思考
[00:00:23,160 -> 00:00:25,000] 但大多数时间是用中文思考
[00:00:25,120 -> 00:00:26,480] 可是我读的信息是英文
[00:00:27,000 -> 00:00:27,640] 就一样的
[00:00:00,000 -> 00:00:02,799] 就是你想去更好的理解中文
[00:00:02,799 -> 00:00:05,960] 首先你要去把你的最底層的那個能力激活出來
[00:00:05,960 -> 00:00:09,080] 就是這個就是你底層能力起來了以後
[00:00:09,080 -> 00:00:10,800] 翻譯其實是一個簡單的事
[00:00:10,800 -> 00:00:12,519] 包括文化理解也是一個簡單的事
[00:00:12,519 -> 00:00:14,960] 我有一次就直接拿這個東西去問GPT
[00:00:14,960 -> 00:00:17,359] 就是有六個領導
[00:00:17,359 -> 00:00:18,640] 然後你有七個領導
[00:00:18,640 -> 00:00:19,920] 你有六支煙怎麼分
[00:00:19,920 -> 00:00:22,280] 然後第一次問他給的那個問題非常奇怪
[00:00:22,280 -> 00:00:23,879] 就給的回答又很很那是什麼
[00:00:23,879 -> 00:00:24,519] 讓地上淺
[00:00:24,519 -> 00:00:26,000] 我覺得好像有這樣的回答
[00:00:26,000 -> 00:00:26,600] 對
[00:00:26,600 -> 00:00:28,679] 但是後來有這樣一個
[00:00:28,679 -> 00:00:29,879] 就是我自己試的
[00:00:00,000 -> 00:00:03,600] 就是你是一個中國文化專家
[00:00:03,960 -> 00:00:06,320] 然後你現在遇到了這樣一個問題
[00:00:06,480 -> 00:00:09,119] 請你告訴我應該怎麼做
[00:00:09,279 -> 00:00:11,640] 他給了幾個答案
[00:00:11,839 -> 00:00:12,880] 就是第一
[00:00:12,880 -> 00:00:17,039] 你要先把煙給這裡邊資歷最深的領導
[00:00:17,640 -> 00:00:18,960] 表達你的尊敬
[00:00:19,120 -> 00:00:22,199] 然後接下來你要按照大家的資歷去一個一個排
[00:00:22,320 -> 00:00:23,079] 到了第七個
[00:00:23,600 -> 00:00:26,800] 你去主動提出來和他一起去買一些煙
[00:00:26,800 -> 00:00:29,839] 在這個時候你既表達了對廠商人的尊敬
[00:00:00,000 -> 00:00:03,399] 然後你又能有一個和他溝通的好的機會
[00:00:04,280 -> 00:00:05,879] 我覺得這他媽的情商比我高多了
[00:00:05,879 -> 00:00:07,480] 就是我是想不到這樣問題的
[00:00:07,839 -> 00:00:10,880] 就所以說這已經是現在的GPT了
[00:00:10,880 -> 00:00:14,720] 這是不是意味著就是以後比如說中文
[00:00:14,720 -> 00:00:15,919] 英語各種各樣的語言
[00:00:15,919 -> 00:00:20,399] 他們其實就是他們基於語言這個模型就已經統一了
[00:00:20,399 -> 00:00:22,079] 就像就像人工智能
[00:00:22,079 -> 00:00:24,960] 他已經發現了語言最下面的那個一致性
[00:00:25,679 -> 00:00:27,719] 就語言最下面本來就是有一致性的
[00:00:27,719 -> 00:00:29,600] 你的這樣說話的邏輯是什麼
[00:00:00,000 -> 00:00:01,399] 你的語言的邏輯是什麼
[00:00:01,399 -> 00:00:02,799] 然後你英文的邏輯是什麼
[00:00:02,839 -> 00:00:06,240] 他可能是找到最下面的一層作為語言的一致性
[00:00:08,000 -> 00:00:08,839] 好問題
[00:00:08,839 -> 00:00:10,279] 就是語言的一致性
[00:00:10,279 -> 00:00:13,119] 我猜90%以上可能是一致的
[00:00:13,599 -> 00:00:14,080] 對吧
[00:00:14,560 -> 00:00:17,519] 然後然後然後那些不一致的
[00:00:17,519 -> 00:00:19,359] 我們也可以通過學外語來解決
[00:00:19,359 -> 00:00:21,559] 其實沒有那麼多不同的
[00:00:21,760 -> 00:00:23,079] 就是思維方式的
[00:00:23,320 -> 00:00:24,719] 完全思維方式不同的語言
[00:00:24,879 -> 00:00:26,440] 如果你說數學是門語言的話
[00:00:26,440 -> 00:00:28,719] 那可能數學的思維方式是不一樣的
[00:00:00,000 -> 00:00:02,000] 那你有沒有接觸過數學這個符號
[00:00:02,000 -> 00:00:05,000] 可能你的思維方式是完全不一樣的對吧
[00:00:05,000 -> 00:00:07,799] 但是你反正代碼可能又是另外一種語言
[00:00:07,799 -> 00:00:10,000] 就是你接觸了代碼這個符號以後
[00:00:10,000 -> 00:00:11,500] 你又帶來了代碼的思維方式
[00:00:11,500 -> 00:00:13,699] 但GPT都已經學完了
[00:00:13,699 -> 00:00:17,500] 所以說我覺得就是那個差距對他來說可能不太存在
[00:00:17,500 -> 00:00:21,800] 他有沒有窮盡語言能帶來的所有知識我覺得遠遠不到
[00:00:21,800 -> 00:00:25,500] 但是多語言對他來說是不是問題肯定不是問題
[00:00:25,500 -> 00:00:27,500] 謝謝老師
[00:00:00,000 -> 00:00:11,500] 我先介绍一下我自己,以及我为什么要问这样的问题。
[00:00:11,500 -> 00:00:15,500] 我之前也是腾讯的,ShanMagic的,后来我去了阿里。
[00:00:15,500 -> 00:00:20,500] 然后我做了很多年国内各个high tech的跟渠道的跟业的合作,
[00:00:20,500 -> 00:00:22,500] 就是把它融合去刷一块电线。
[00:00:22,500 -> 00:00:26,000] 然后你刚才有提到说你想做调酒师,其实我一点也不惊讶,
[00:00:26,000 -> 00:00:29,500] 因为我自己也搞了一个服务业,我是7年的身心灵疗愈师,
[00:00:00,000 -> 00:00:03,259] 然后把国内国外的很多的这个系统都学习了一遍
[00:00:03,259 -> 00:00:07,120] 我觉得如果AI再发展的话也不可能取代意识和智慧吧
[00:00:07,120 -> 00:00:09,279] 所以我觉得还挺有意思的
[00:00:09,279 -> 00:00:11,759] 那我想问的问题就是像比如说
[00:00:11,759 -> 00:00:14,439] 其实国内包括国外有很多的hack tech
[00:00:14,439 -> 00:00:19,640] 比如之前有比如指纹制作 生物世界技术 大数据 实验室等等
[00:00:19,640 -> 00:00:21,719] 其实我也做了很多落地的东西
[00:00:21,719 -> 00:00:23,519] 有的时候就是眼见它起高楼
[00:00:23,519 -> 00:00:25,320] 然后眼见它楼落地
[00:00:25,320 -> 00:00:26,719] 我觉得有两点比较重要
[00:00:26,719 -> 00:00:29,079] 一个就是技术本身它的精准性
[00:00:00,000 -> 00:00:05,400] 就是第二个就是是不是有一个够聪明的人把它落地到一个可商业化的东西
[00:00:05,400 -> 00:00:11,240] 因为我觉得任何事情想要长久的话,最后是归于到商业模式和财务模式上的
[00:00:11,240 -> 00:00:15,439] 这样才能让这个科技公司更有动力去不断的去挖掘它
[00:00:15,439 -> 00:00:20,800] 以及在C端用户的话,它是否有足够的感知去参与到其中,产生更大的市场价值
[00:00:20,800 -> 00:00:25,519] 那其实Chadbow的我关注是,其实我是从商业角度,我完全不懂技术
[00:00:25,519 -> 00:00:28,120] 但我其实更关注它未来会发展成什么样
[00:00:00,000 -> 00:00:10,500] 就比如說像馬斯克做的火箭其實很貴,但是馬斯克想如果把火箭回收之後,當成本降低的時候,它就變成了一個普通人可以參與的東西。
[00:00:10,500 -> 00:00:23,000] 所以我就和一些像跨境的公司的老闆去交流,我們想到一些場景,就比如說它是否能跟電商結合,我更關注的是它是否能成為一個新的私域流量的池子,
[00:00:00,000 -> 00:00:03,339] 以及這個資源的池子最後會掌握在誰的手裡
[00:00:03,339 -> 00:00:05,759] 以及用戶的標籤會儲存在哪裡
[00:00:05,759 -> 00:00:07,879] 它是否會在某一個公司
[00:00:07,879 -> 00:00:10,480] 還是說只能是在微軟發明就是微軟的
[00:00:10,480 -> 00:00:12,119] 然後百度發明就是百度的
[00:00:12,119 -> 00:00:15,839] 其實這一塊就是中間實現形式我沒有太想清楚
[00:00:15,839 -> 00:00:17,079] 因為我不懂技術
[00:00:19,320 -> 00:00:21,079] 然後第二個就是我想問一下
[00:00:21,079 -> 00:00:22,440] 你個人有沒有想過
[00:00:22,440 -> 00:00:24,640] 比如說除了我剛剛所說的電商產品之外
[00:00:24,640 -> 00:00:27,440] 你覺得還有哪些可能2B 2C領域的
[00:00:00,000 -> 00:00:06,320] 最后到呼吸的这种范行业的应用场景有哪些比较有价值的展现形式。
[00:00:06,320 -> 00:00:09,480] 就比如说像比如说它如果变成了私域流量的话,
[00:00:09,480 -> 00:00:11,880] 它可能会是一种新的流量变现的渠道,
[00:00:11,880 -> 00:00:13,560] 因为我最近几年在做流量变现,
[00:00:13,560 -> 00:00:18,399] 那实际上传统的SEO形式的这种或者竞价付费买量其实已经很off了,
[00:00:18,399 -> 00:00:19,800] 而且成本非常非常的高,
[00:00:19,800 -> 00:00:23,839] 所以现在很多人不论是大企业还是小企业还是客户人都在做私域流量,
[00:00:23,839 -> 00:00:27,760] 那比较传统的方式就是你搞一个社群运营,
[00:00:00,000 -> 00:00:02,720] 但其实是很耗费人力成本的
[00:00:02,720 -> 00:00:03,799] 所以我又想到说
[00:00:03,799 -> 00:00:07,440] 那拆透的试图可以代替这种比较好的人工客服
[00:00:07,440 -> 00:00:09,640] 来取代这个传统的人工客服
[00:00:09,640 -> 00:00:11,720] 而不仅仅单纯的是说你好
[00:00:11,720 -> 00:00:14,119] 你要什么一些定制化的QA的回答
[00:00:14,119 -> 00:00:17,960] 它会变成一个比较高智商的一个高级的一个客服的一个质量
[00:00:17,960 -> 00:00:20,719] 来替代这种低质廉价的人工
[00:00:20,719 -> 00:00:22,519] 我觉得这几个观众问题
[00:00:22,519 -> 00:00:23,480] 我听懂了
[00:00:23,480 -> 00:00:28,160] 然后本来你的后面那个问题我是想说我不是很关注
[00:00:00,000 -> 00:00:02,560] 結果後來發現你問到了我的工作本身
[00:00:02,560 -> 00:00:04,320] 我就不能說我不是很關注了
[00:00:04,320 -> 00:00:06,320] 對我自己是做增長的嘛
[00:00:06,320 -> 00:00:07,879] 我是給這是AEG的讀書會
[00:00:07,879 -> 00:00:10,400] 我是給各位AEG的爸爸們做增長的
[00:00:10,400 -> 00:00:11,000] 然後
[00:00:12,839 -> 00:00:15,279] 我是覺得它在現有的情況下
[00:00:15,279 -> 00:00:16,960] 包括你說的那個思域的運營
[00:00:16,960 -> 00:00:18,280] 它會是很重要的點
[00:00:18,320 -> 00:00:22,120] 因為我們過去的流量變成遊戲玩家
[00:00:22,120 -> 00:00:22,399] 對吧
[00:00:22,399 -> 00:00:24,359] 你看到了一個市面上任何一個
[00:00:25,120 -> 00:00:26,600] 你可以坐下不用站
[00:00:26,640 -> 00:00:28,480] 對就是我們看到任何一個流量
[00:00:00,000 -> 00:00:02,000] 我们想把它变成一个你的客户
[00:00:02,000 -> 00:00:05,599] 确实是主要是通过推荐和转化的逻辑
[00:00:05,599 -> 00:00:09,599] 我觉得XGPT可以让这件事情变成一个销售的逻辑
[00:00:09,599 -> 00:00:10,800] 就是你不是给它看一遍
[00:00:10,800 -> 00:00:12,599] 而是有一个更好的
[00:00:12,599 -> 00:00:14,199] 不管是通过对话吧
[00:00:14,199 -> 00:00:15,400] 还是通过理解
[00:00:15,400 -> 00:00:20,199] 总之它会有一个更销售向的去提高转化率的方式
[00:00:20,199 -> 00:00:22,600] 以及它的这个所谓的社群运营
[00:00:22,600 -> 00:00:24,399] 就是它未必是现在的一个形式
[00:00:24,399 -> 00:00:28,000] 但是我们想知道这个老游戏它的那个
[00:00:00,000 -> 00:00:03,680] 很多活跃都是来自于你可能产生了一些内容事件
[00:00:03,680 -> 00:00:04,599] 或者各种事件
[00:00:04,599 -> 00:00:05,759] 然后就来了很多
[00:00:05,759 -> 00:00:07,799] 就是乡村派对等等
[00:00:07,799 -> 00:00:09,080] 就都有这样的例子
[00:00:09,080 -> 00:00:12,720] 在过去我们人是没有办法控制这些东西的
[00:00:12,720 -> 00:00:14,480] 因为我没有那个内容生产能力
[00:00:14,480 -> 00:00:19,440] 或者说是去理解发生所有事情的这些这样的方法
[00:00:19,440 -> 00:00:20,640] 但是现在可能有了
[00:00:20,640 -> 00:00:23,359] 它可能就是一个比较大的增长机会
[00:00:23,359 -> 00:00:27,199] 但是你如果你的问题是落在今天的Chai GPT
[00:00:27,199 -> 00:00:29,839] 怎么样子用在我今天的业务场景下
[00:00:00,000 -> 00:00:03,200] 怎麼樣子快速的落地和快速的幫我賺錢
[00:00:03,200 -> 00:00:04,280] 我沒有答案
[00:00:04,280 -> 00:00:07,000] 這個我覺得創業者們會去figure out的
[00:00:07,000 -> 00:00:09,839] 就是我也聽說很多創業者在做這樣的東西
[00:00:09,839 -> 00:00:12,320] 比如說你的那個視頻
[00:00:12,320 -> 00:00:15,679] 你電視上的那些視頻怎麼樣子把它做得更好一點
[00:00:15,679 -> 00:00:17,480] 對我沒有答案
[00:00:17,480 -> 00:00:20,079] 我也不打算照這個方向想
[00:00:20,079 -> 00:00:22,800] 就是回到這個的第二個延伸
[00:00:22,800 -> 00:00:26,600] 就是我覺得現有的場景用XJPG去做增效
[00:00:26,600 -> 00:00:29,440] 是30%到300%的機會了不起了
[00:00:00,000 -> 00:00:03,319] 但是真正的我說那個長期影響
[00:00:03,319 -> 00:00:04,679] 就是那個大的長期影響
[00:00:04,679 -> 00:00:06,759] 那個3000%到30,000%的東西
[00:00:06,759 -> 00:00:09,320] 是在現有根本就沒有解決的問題裡面的
[00:00:09,320 -> 00:00:12,439] 請問現在技術接口是怎麼開通的嗎?
[00:00:12,439 -> 00:00:14,400] 好,剛剛還有第二個問題
[00:00:14,400 -> 00:00:15,679] 就是它是怎麼開通的
[00:00:15,679 -> 00:00:17,039] 數據是怎麼儲存的
[00:00:17,039 -> 00:00:20,160] 我覺得這個我還想多聊一下這一點
[00:00:20,160 -> 00:00:22,559] 就是Chairpt是OpenAI做的
[00:00:22,559 -> 00:00:24,120] 不是微軟做的
[00:00:24,120 -> 00:00:25,519] 然後OpenAI
[00:00:25,519 -> 00:00:28,280] 就微軟是OpenAI的一個很重要的投資人
[00:00:00,000 -> 00:00:02,040] 但是他們這個投資的方式
[00:00:02,040 -> 00:00:04,540] 其實和市面上存在的
[00:00:04,540 -> 00:00:06,839] 投資方式都很不一樣
[00:00:06,839 -> 00:00:08,140] OpenAI的組織架構
[00:00:08,140 -> 00:00:09,519] 是它有一個非盈利的架構
[00:00:09,519 -> 00:00:10,779] 然後這個非盈利的架構
[00:00:10,779 -> 00:00:12,619] 控制一個盈利的架構
[00:00:12,619 -> 00:00:13,679] 但是這個盈利的架構
[00:00:13,679 -> 00:00:15,720] 它的這個盈利是有上限的
[00:00:15,720 -> 00:00:18,320] 它叫一個capitalism
[00:00:18,320 -> 00:00:21,019] 就是它是一個有上限的資本主義
[00:00:21,019 -> 00:00:23,399] 所以說這個盈利的架構
[00:00:23,399 -> 00:00:25,500] 賺夠錢了以後
[00:00:25,500 -> 00:00:28,600] 它就所有東西都回歸到這個非盈利
[00:00:00,000 -> 00:00:02,200] 並且這個非盈利對這個盈利的決定
[00:00:02,200 -> 00:00:04,400] 就是日常經營的決策權
[00:00:04,400 -> 00:00:05,960] 是有絕對決策權的
[00:00:05,960 -> 00:00:08,500] 微軟投的和其他投資人投的是這個
[00:00:08,500 -> 00:00:10,140] CAPT盈利機構
[00:00:10,140 -> 00:00:11,339] 它是有上限的
[00:00:11,339 -> 00:00:14,179] 所以微軟在這裡邊它有很多這個
[00:00:14,179 -> 00:00:15,439] 收益權
[00:00:15,439 -> 00:00:18,019] 但是第一它不能控制你到底做什麼
[00:00:18,019 -> 00:00:19,839] 第二它的收益有上限
[00:00:19,839 -> 00:00:21,640] 在這個架構之下
[00:00:21,640 -> 00:00:24,679] 就是我覺得它費了這麼大的心思
[00:00:24,679 -> 00:00:26,179] 去設置這個架構
[00:00:26,179 -> 00:00:27,260] 如果它不設置這個架構
[00:00:27,260 -> 00:00:28,559] 它今天融資會比今天
[00:00:00,000 -> 00:00:02,339] 它歷史就GPT出了以後
[00:00:02,339 -> 00:00:04,339] 所有人都想給它掏錢了
[00:00:04,339 -> 00:00:05,719] 但是在GPT出之前
[00:00:05,719 -> 00:00:07,059] 他們融資是很艱難的
[00:00:07,059 -> 00:00:08,640] 就是因為一個很重要的原因
[00:00:08,640 -> 00:00:10,099] 就是因為這個組織架構
[00:00:10,099 -> 00:00:11,480] 所以說他們費了這麼大心思
[00:00:11,480 -> 00:00:13,679] 去搞了一個這麼處理不討好的組織架構
[00:00:13,679 -> 00:00:16,359] 我還是相信他們的很多這種
[00:00:16,359 -> 00:00:19,019] 為了全人類的這種願景吧
[00:00:19,019 -> 00:00:20,559] 這是一就是我覺得他們動機
[00:00:20,559 -> 00:00:22,059] 展現出來的是好的
[00:00:22,059 -> 00:00:24,019] 第二就是具體怎麼開放
[00:00:24,019 -> 00:00:27,039] 今天我們如果去看那個GPT API的
[00:00:27,039 -> 00:00:28,859] 這個Turbo Agreement就發現
[00:00:00,000 -> 00:00:02,359] 它其實對數據的使用是很嚴格的
[00:00:02,359 -> 00:00:04,639] 就是它沒有要去讀你那麼多東西
[00:00:04,639 -> 00:00:09,439] 它嚴格的保證了它的數據是用在去改進產品體驗上
[00:00:09,439 -> 00:00:11,720] 而不是去為那個模型上
[00:00:11,720 -> 00:00:14,720] 以及我猜未來第一
[00:00:14,720 -> 00:00:16,320] 它有沒有可能私有化部署不知道
[00:00:16,320 -> 00:00:20,120] 但是它應該可以有一個更容易的尊重隱私
[00:00:20,120 -> 00:00:24,120] 就讓你的數據更有安全感的這樣一個措施
[00:00:24,120 -> 00:00:27,879] 第二我自己還覺得GPT的數據使用方式
[00:00:00,000 -> 00:00:03,000] 你應該可以跟GPT約定一個加密算法
[00:00:03,000 -> 00:00:05,700] 然後通過這個加密算法的方式回傳過去
[00:00:05,700 -> 00:00:07,700] 然後他自己不需要去
[00:00:07,700 -> 00:00:10,500] 像人類那樣去理解他給你算出來
[00:00:10,500 -> 00:00:13,000] 然後其中是保證你數據是安全的
[00:00:13,000 -> 00:00:15,500] 就是我覺得數據安全不是一個很重要的問題
[00:00:15,500 -> 00:00:18,000] 因為第一他們是好人
[00:00:18,000 -> 00:00:20,500] 第二這個東西是可以被解決的
[00:00:20,500 -> 00:00:22,500] 中國政府在意這個問題啊
[00:00:22,500 -> 00:00:24,500] 我又 這我
[00:00:24,500 -> 00:00:26,500] 能說什麼呢
[00:00:26,500 -> 00:00:28,500] 中國政府還在意
[00:00:00,000 -> 00:00:04,000] 所以算了,很多問題,我也沒辦法
[00:00:00,000 -> 00:00:03,500] 人类是有自由的意志,是有自己的思维
[00:00:03,500 -> 00:00:07,500] 然后他从古希腊的这种哲学到后面的数学
[00:00:07,500 -> 00:00:10,500] 到后面的整个科学技术的发展
[00:00:10,500 -> 00:00:13,500] 那其实在我的认识当中
[00:00:13,500 -> 00:00:16,500] 就是说一个非常智能的机器人也罢
[00:00:16,500 -> 00:00:19,000] 或者说我们这种AI也罢
[00:00:19,000 -> 00:00:21,500] 它能够模仿人的这种思维
[00:00:21,500 -> 00:00:23,500] 那我的问题就是说
[00:00:23,500 -> 00:00:27,500] 在过往的研究经历当中
[00:00:00,000 -> 00:00:05,500] 目前的研究案例中,相關的科學家有沒有做過類似的,
[00:00:05,500 -> 00:00:14,000] 能夠在數學領域,能夠讓CVT,或者大圓模型,
[00:00:14,000 -> 00:00:20,000] 能夠做一些具有創造性的類似於數學,
[00:00:00,000 -> 00:00:08,500] 或者這種的 就是說這種發明或者說定理的推導 公式的推導等等
[00:00:08,500 -> 00:00:15,099] 就是說有沒有這種類似的 就是說能夠顛覆 能夠跟人類這種思維一樣的這種
[00:00:15,099 -> 00:00:21,100] 能夠顛覆這種人類文明的這種功力的推導 有沒有過這種的
[00:00:21,100 -> 00:00:28,500] 好問題 沒有 因為這個對人都是很稀有的技能
[00:00:00,000 -> 00:00:02,660] 但是他有沒有創作
[00:00:02,660 -> 00:00:03,779] 就是你問了兩個問題
[00:00:03,779 -> 00:00:06,780] 就是第一個是有沒有讓他去做有創造力的工作
[00:00:06,780 -> 00:00:09,419] 我們現在看到了很多AIGC的畫
[00:00:09,419 -> 00:00:11,699] 那都不是什麼新的東西
[00:00:11,699 -> 00:00:12,859] 但是大家都覺得
[00:00:12,859 -> 00:00:14,259] 這好像是挺有創造力的
[00:00:14,259 -> 00:00:14,820] 沒見過
[00:00:14,820 -> 00:00:16,059] 然後畫得挺好的
[00:00:16,059 -> 00:00:17,699] 所以說創造力這個詞
[00:00:17,699 -> 00:00:19,500] 我覺得也挺難定義的
[00:00:19,500 -> 00:00:22,980] 就是我們應該日常的很多創意型的工作
[00:00:22,980 -> 00:00:25,500] 大多數都只是智力的搬磚而已
[00:00:25,500 -> 00:00:27,699] 我們沒有真的去創造任何東西
[00:00:27,699 -> 00:00:28,219] 對
[00:00:00,000 -> 00:00:01,679] 所以說這方面的工作
[00:00:01,679 -> 00:00:03,399] XGPT能做得很好
[00:00:03,399 -> 00:00:07,120] 包括陶哲軒他是重度XGPT使用者
[00:00:07,120 -> 00:00:09,160] 然後他發了好幾個推特
[00:00:09,160 -> 00:00:11,919] 就說XGPT是怎麼樣子
[00:00:11,919 -> 00:00:15,279] 怎麼樣子讓我在這種非常理論
[00:00:15,279 -> 00:00:17,280] 非常高級的數學的工作中
[00:00:17,280 -> 00:00:19,480] 得到切切實實的增效的
[00:00:19,480 -> 00:00:22,519] 就是包括XGPT給他提供的很多理論
[00:00:22,519 -> 00:00:26,320] 他覺得這個推導的過程雖然是錯的
[00:00:26,320 -> 00:00:27,800] 但是給了他很多靈感
[00:00:27,800 -> 00:00:29,120] 也給了他很多思路
[00:00:00,000 -> 00:00:02,560] 而且記得這是個Demo
[00:00:02,560 -> 00:00:04,799] 有沒有辦法讓數位家具去把它訓練得更好
[00:00:04,799 -> 00:00:05,480] 讓它變得更好用
[00:00:05,480 -> 00:00:07,799] 我覺得是肯定會比今天做得好
[00:00:07,799 -> 00:00:09,119] 能做得多好我不知道
[00:00:09,119 -> 00:00:11,039] 但是另外一個就是我為什麼說不能呢
[00:00:11,039 -> 00:00:14,119] 就是這是我其實仔細想的一件事
[00:00:14,119 -> 00:00:16,359] 就是Eureka這個詞
[00:00:16,359 -> 00:00:18,600] 就是埃及米德在發明伏利定律的時候
[00:00:18,600 -> 00:00:21,399] 他就我發現了是吧跳出浴缸
[00:00:21,399 -> 00:00:25,239] 他這個詞就是他當時說的就是這個Eureka
[00:00:25,239 -> 00:00:27,239] Eureka其實有兩個步驟
[00:00:27,239 -> 00:00:29,359] 第一個步驟就是他發現了一個
[00:00:00,000 -> 00:00:01,700] 人類都沒有發現過的東西
[00:00:02,279 -> 00:00:05,259] 就是世界上所有的人都
[00:00:05,280 -> 00:00:08,619] 之前都不是所有人都在想
[00:00:08,640 -> 00:00:10,820] 但是總之世界上之前所有的人
[00:00:10,839 -> 00:00:12,099] 都沒有想到那個弗利定律
[00:00:12,119 -> 00:00:14,300] 說明它是一個離現有人類知識
[00:00:14,320 -> 00:00:15,380] 非常遠的一個東西
[00:00:15,400 -> 00:00:16,420] 很難想到對吧
[00:00:16,440 -> 00:00:17,620] 但是他想到了
[00:00:17,839 -> 00:00:19,460] 第二個是他馬上發現了
[00:00:19,480 -> 00:00:20,940] 這件事情是多麼重要
[00:00:21,679 -> 00:00:23,140] 他想到的瞬間他就知道了
[00:00:23,160 -> 00:00:24,500] 這是一個非常重要的事情
[00:00:24,839 -> 00:00:26,160] 我覺得Tragedy
[00:00:26,239 -> 00:00:29,239] 以他不斷的去蹦下一個詞的機制
[00:00:00,000 -> 00:00:01,879] 他应该是很难做到第一点的
[00:00:02,080 -> 00:00:04,320] 但是他有没有可能做到第一点我不知道
[00:00:04,400 -> 00:00:06,879] 可是他应该超级难做到第二点
[00:00:06,879 -> 00:00:10,359] 因为他没有办法判断什么东西是对人有用的
[00:00:10,359 -> 00:00:12,720] 他需要人去告诉他什么东西对人有用
[00:00:13,400 -> 00:00:17,039] 所以就是就是为什么我觉得他能不能取代我
[00:00:17,039 -> 00:00:18,000] 我觉得很有可能
[00:00:18,000 -> 00:00:19,719] 但是能不能取代牛顿我觉得不太可能
[00:00:20,399 -> 00:00:23,120] 那那我接下来就是还有一个问题
[00:00:23,120 -> 00:00:27,039] 就是刚才讲讲到我们CATCPD现在是一个demo
[00:00:27,120 -> 00:00:28,920] 那刚才也有好多同学的话
[00:00:00,000 -> 00:00:02,000] 讲到就是说它的一个商业化
[00:00:02,000 -> 00:00:03,000] 然后呢
[00:00:03,000 -> 00:00:05,000] 其实我不太想
[00:00:05,000 -> 00:00:08,000] 就是说讨论就是说它具体的一个商业化的应用模式
[00:00:08,000 -> 00:00:13,000] 但是我想请教他就是说他以后的一个未来的一个发展
[00:00:13,000 -> 00:00:14,000] 发展形态
[00:00:14,000 -> 00:00:16,000] 他可能就是说应用在不同的领域
[00:00:16,000 -> 00:00:17,000] 比如说举个例子
[00:00:17,000 -> 00:00:18,000] 这种客服呀
[00:00:18,000 -> 00:00:19,000] 或者怎样子
[00:00:19,000 -> 00:00:21,000] 就是说非常智能的一个客服
[00:00:21,000 -> 00:00:25,000] 它是第一个是应用在某个领域
[00:00:25,000 -> 00:00:28,000] 或者说它可能会应用在
[00:00:00,000 -> 00:00:03,000] 對某個人這種非常個性化的
[00:00:03,000 -> 00:00:05,000] 包括您剛才講到的就是說
[00:00:05,000 -> 00:00:07,000] 您那個就是說要去調酒
[00:00:07,000 -> 00:00:10,000] 那當這個AI機器人來幫著教育小孩
[00:00:10,000 -> 00:00:12,000] 那好了那這個AI機器人
[00:00:12,000 -> 00:00:13,000] 他說的不是我說的
[00:00:13,000 -> 00:00:14,000] 我沒有幹這種事情啊
[00:00:14,000 -> 00:00:16,000] 這個我們知道重要的是
[00:00:16,000 -> 00:00:18,000] 就是說這個AI機器人
[00:00:18,000 -> 00:00:19,000] 他是屬於那個
[00:00:19,000 -> 00:00:21,000] 他應該是屬於一個個人
[00:00:21,000 -> 00:00:22,000] 對吧personal的
[00:00:22,000 -> 00:00:23,000] 不是說一個就是說
[00:00:23,000 -> 00:00:26,000] 他的思維已經就是說
[00:00:26,000 -> 00:00:29,000] 是屬於或者說這個機器人
[00:00:00,000 -> 00:00:02,200] 它是屬於一個個人的產品了對吧
[00:00:02,200 -> 00:00:05,400] 不是不是不是
[00:00:05,400 -> 00:00:07,900] 應該絕大多數的機器人還是一個託用的
[00:00:07,900 -> 00:00:10,900] 就是可能GPT我們會發現它已經能解決絕大多數問題了
[00:00:10,900 -> 00:00:13,000] 你不需要一個個人的東西來繼續
[00:00:13,000 -> 00:00:14,900] 站在之上去幫你解決什麼問題
[00:00:14,900 -> 00:00:17,000] 就比如說我今天想編程是吧
[00:00:17,000 -> 00:00:19,600] GPT-4或者說它再改進一下
[00:00:19,600 -> 00:00:21,600] 它可能出了一個編程版
[00:00:21,600 -> 00:00:23,399] 就是它編程能力更強了
[00:00:23,399 -> 00:00:25,000] 那它就可以幫我編程了
[00:00:25,000 -> 00:00:27,199] 跟我沒有任何關係
[00:00:27,199 -> 00:00:29,800] OK那這樣表態下去的話
[00:00:00,000 -> 00:00:03,200] 您的意思就是说他OpenAI或者说文青一眼
[00:00:03,200 -> 00:00:05,719] 他只是他在以后的一个发展过程中
[00:00:05,719 -> 00:00:11,119] 他只能就是说是一个这种比较common的一个东西
[00:00:11,119 -> 00:00:12,720] 就不会进行一个个性化
[00:00:12,720 -> 00:00:15,720] 比如说他就是一个全能的专家
[00:00:15,720 -> 00:00:20,239] 或者就是说一个比较在会分层不同领域的这种专家
[00:00:20,239 -> 00:00:21,559] 您的意思是这样的吗
[00:00:21,719 -> 00:00:23,920] 我这只能都是猜想
[00:00:23,920 -> 00:00:26,679] 就是真正的是看他们怎么去开放
[00:00:26,679 -> 00:00:28,359] 我的猜想就是第一
[00:00:00,000 -> 00:00:02,240] 它的通用型可以解決絕大多數的問題
[00:00:02,240 -> 00:00:03,759] 就好像這個鋼鐵俠的盔甲
[00:00:03,759 -> 00:00:07,240] 他一個人已經可以打敗世界上所有的壞人了
[00:00:07,240 -> 00:00:10,000] 他不需要在這職場上去做什麼東西
[00:00:10,000 -> 00:00:13,240] 但是你看這裡邊就是他們每個用不同的那個鎧甲對吧
[00:00:13,240 -> 00:00:14,759] 他有專精的東西
[00:00:14,759 -> 00:00:19,399] 舉個例子就是通用型的GPT可能在醫療這件事情上
[00:00:19,399 -> 00:00:21,320] 他做得不夠好
[00:00:21,320 -> 00:00:23,160] 我們需要他做得更好
[00:00:23,160 -> 00:00:25,640] 那他可能就要去多讀這方面的文獻
[00:00:25,640 -> 00:00:27,800] 然後他甚至在那個
[00:00:27,800 -> 00:00:29,399] 就是人類給他feedback的時候
[00:00:00,000 -> 00:00:02,399] 他多用这方面的数据去给他feedback
[00:00:02,680 -> 00:00:05,559] 那他可能就是一个专精医疗专家GPT
[00:00:05,559 -> 00:00:07,000] 他有医疗方面的知识
[00:00:07,040 -> 00:00:09,839] 以及这些知识里边所提炼出来的一些能力
[00:00:10,279 -> 00:00:13,320] 他可能是这个是专业的GPT
[00:00:13,679 -> 00:00:15,839] 那在那之上就有一些非共识的东西
[00:00:16,199 -> 00:00:17,800] 比如说今天一个营销方案
[00:00:18,199 -> 00:00:19,760] 基本的质量我们是可以有的
[00:00:19,760 -> 00:00:21,760] 但是谁的营销方案到底更有用
[00:00:21,760 -> 00:00:23,199] 那其实不一样
[00:00:23,199 -> 00:00:25,760] 就是我们看到最牛逼的一些广告专家们
[00:00:26,079 -> 00:00:29,039] 就是他们出的那些东西不是一个共识的东西
[00:00:00,000 -> 00:00:02,000] 很多人都會覺得 哇 你這出了個什麼鬼
[00:00:02,000 -> 00:00:03,500] 結果發現效果就很好
[00:00:03,500 -> 00:00:07,500] 那這個東西我覺得也不太可能存在於人類現有知識裡面
[00:00:07,500 -> 00:00:09,000] 因為它完全是個非共識
[00:00:09,000 -> 00:00:11,000] 你也不可能通過邏輯去推導出來
[00:00:11,000 -> 00:00:15,500] 那這個東西可能是那個定制的那一派
[00:00:15,500 -> 00:00:20,000] 這個定制的話 那就是會出現各種比如說不同的公司
[00:00:20,000 -> 00:00:23,500] 它會就是說再採到了就是說
[00:00:23,500 -> 00:00:25,500] 像JPT或者說文信業之後
[00:00:25,500 -> 00:00:27,000] 它再去自己去訓練
[00:00:00,000 -> 00:00:02,000] 然後屬於自己的那個模型
[00:00:02,000 -> 00:00:05,500] 或者說屬於自己的產品是會有這種
[00:00:05,500 -> 00:00:07,000] 我不知道它會怎麼樣
[00:00:07,000 -> 00:00:10,000] 但是我覺得合理的方式仍然是
[00:00:10,000 -> 00:00:14,000] 就是我做一下未來的推斷吧
[00:00:14,000 -> 00:00:17,500] 但是這個推斷是基於一些我也不確定的事情
[00:00:17,500 -> 00:00:20,000] 比如說它這個模型門檻有多高
[00:00:20,000 -> 00:00:21,500] 它能不能被快速複製
[00:00:21,500 -> 00:00:24,000] 假設它的模型就SAM自己的推斷呢
[00:00:24,000 -> 00:00:26,000] 就是兩三年之後
[00:00:26,000 -> 00:00:28,000] 是讓出來幾百個大模型
[00:00:28,000 -> 00:00:29,000] 不同的大模型
[00:00:00,000 -> 00:00:02,560] 就是那个OpenAI的CEO
[00:00:02,560 -> 00:00:05,679] 他访谈里面说的是以后会有很多个大模型
[00:00:05,679 -> 00:00:07,679] 那些每个大模型它的能力是不一样的
[00:00:07,679 -> 00:00:08,640] 适配的东西不一样
[00:00:08,640 -> 00:00:09,519] 专心的是不一样的
[00:00:09,519 -> 00:00:12,199] 所以说就是像你说的那种情况
[00:00:12,199 -> 00:00:16,839] 我自己推断是世界上可能未来重要的只有OpenAI一家
[00:00:16,839 -> 00:00:19,320] 然后重要的只有GPT一个模型
[00:00:19,320 -> 00:00:20,760] 然后这个GPT的模型
[00:00:20,760 -> 00:00:22,199] 你不是去给他
[00:00:22,199 -> 00:00:25,079] 他可能会把这个feedback这一步开放给大家
[00:00:25,079 -> 00:00:29,000] 然后你在feedback这一步去把你的知识和你的
[00:00:00,000 -> 00:00:02,200] 一些想要的東西反饋給他
[00:00:02,200 -> 00:00:03,960] 然後去適配你的工作
[00:00:04,440 -> 00:00:05,719] 然後的話那我就
[00:00:05,960 -> 00:00:06,960] 之後可能這樣子的話
[00:00:06,960 -> 00:00:08,919] 我可能就表達一下我自己的擔憂
[00:00:08,919 -> 00:00:10,560] 我覺得科技的話是
[00:00:10,759 -> 00:00:12,880] 給大家更好的一個生活
[00:00:13,039 -> 00:00:14,720] 如果說這種如果
[00:00:15,000 -> 00:00:17,039] 只是一個比較共同的模型的話
[00:00:17,039 -> 00:00:18,320] 那對於我來講的話
[00:00:18,320 -> 00:00:19,480] 我可能不會跟他
[00:00:19,719 -> 00:00:22,039] 講我心裡的一些比較
[00:00:22,280 -> 00:00:24,679] 嗯比較個人隱私的一些東西
[00:00:24,679 -> 00:00:25,600] 你今天有沒有
[00:00:25,879 -> 00:00:28,480] 你今天有沒有用微信去講你的個人隱私
[00:00:00,000 -> 00:00:04,540] 我不會講特別的那種的意思
[00:00:04,540 -> 00:00:09,939] 就是涉及到一些宗教政治或者說自己人類內心的一些東西
[00:00:09,939 -> 00:00:12,419] 那其實我想再說每個人如果是這樣子的話
[00:00:12,419 -> 00:00:14,339] 每個人可能都會對它有所保留
[00:00:14,339 -> 00:00:16,300] 那我的意思就是說
[00:00:16,300 -> 00:00:18,579] 你內心那塊東西可能完全不拿出來
[00:00:18,579 -> 00:00:22,219] 但是你可能對科技的接受程度超過了你的想像
[00:00:22,219 -> 00:00:24,460] 蘋果是一家壟斷公司
[00:00:24,460 -> 00:00:26,260] 微信是一個壟斷APP
[00:00:26,260 -> 00:00:27,100] 不能說這句話
[00:00:27,100 -> 00:00:28,460] 微信是一個好用的
[00:00:00,000 -> 00:00:02,000] 大家都去主動使用他的
[00:00:02,960 -> 00:00:06,080] 我們很多這種事情都是在這種地方發生的
[00:00:06,080 -> 00:00:08,759] 所以說壟斷這件事我不覺得對人真的有那麼重要
[00:00:09,359 -> 00:00:10,599] 比如說那個
[00:00:10,599 -> 00:00:12,320] 他作為一個大語言模型
[00:00:12,320 -> 00:00:15,199] 就是說現在那個就是說需要更多的這種
[00:00:15,679 -> 00:00:16,600] 什麼
[00:00:17,239 -> 00:00:18,519] 就是說提詞工程師
[00:00:18,559 -> 00:00:21,480] 然後其實就是說如果說每個人就是說會給他
[00:00:21,679 -> 00:00:23,760] 更多的這種私有化這種問題的話
[00:00:23,760 -> 00:00:26,879] 其實對他這個語言模型的訓練對他自己也是有
[00:00:26,879 -> 00:00:28,120] 他不用他不用
[00:00:00,000 -> 00:00:02,879] 它不用你去跟它對話的東西去訓練那個模型
[00:00:03,759 -> 00:00:06,839] 它只用這些東西去改進你的使用體驗
[00:00:07,320 -> 00:00:11,960] 它的那個背後所GPT-4所調教出來的能力
[00:00:12,240 -> 00:00:15,720] 其實是基於人類積累的高質量信息
[00:00:15,919 -> 00:00:16,960] 你現在去跟它說一堆話
[00:00:16,960 -> 00:00:18,359] 它不覺得這是高質量信息
[00:00:20,239 -> 00:00:23,359] 這是大模型和那個
[00:00:23,359 -> 00:00:24,719] 就是這種類型的大模型
[00:00:24,719 -> 00:00:26,280] 和過去模型的一個很重要的區別
[00:00:26,280 -> 00:00:28,519] 就是過去的模型會非常注重
[00:00:00,000 -> 00:00:01,800] 你跟他对话的这个点
[00:00:01,800 -> 00:00:05,360] 因为他需要不断的去专精这个模型是吧
[00:00:05,360 -> 00:00:07,799] 更好的去适配你和他的对话
[00:00:07,799 -> 00:00:11,640] 你和他的对话才是这里边最真实的那个使用场景
[00:00:11,640 -> 00:00:14,759] 所以说他必须要拿你的对话数据去训练这个模型
[00:00:14,759 -> 00:00:16,359] 才能把这个模型做得更好
[00:00:16,359 -> 00:00:19,440] 但大模型是首先他要通过高质量数据
[00:00:19,440 -> 00:00:21,039] 把这个能力给体验出来
[00:00:21,039 -> 00:00:24,839] 然后再通过反馈去和你的喜好对齐
[00:00:24,839 -> 00:00:27,359] 所以说他最多最多也就是到反馈这一步
[00:00:27,359 -> 00:00:29,160] 他没有必要到顶上那一步
[00:00:00,000 -> 00:00:00,560] 好的
[00:00:00,560 -> 00:00:00,840] 谢谢
[00:00:08,359 -> 00:00:09,640] 苏老师你好
[00:00:09,640 -> 00:00:11,439] 我本身是你的一个粉丝
[00:00:11,439 -> 00:00:11,880] 看了你的节目
[00:00:11,880 -> 00:00:12,720] 谢谢
[00:00:12,720 -> 00:00:13,199] 对
[00:00:13,199 -> 00:00:14,800] 然后我的问题还比较多
[00:00:14,800 -> 00:00:17,839] 然后第一个是刚才说到这个垄断的问题
[00:00:17,839 -> 00:00:19,679] 然后我其实有点疑惑
[00:00:19,679 -> 00:00:21,239] 因为这也是我第一个想问的问题
[00:00:21,239 -> 00:00:24,519] 就是所谓现在说的我们的定义中的垄断
[00:00:24,519 -> 00:00:25,800] 应该是指商业垄断
[00:00:25,800 -> 00:00:26,239] 对吧
[00:00:27,760 -> 00:00:29,480] 他只有这一家可以做出来
[00:00:00,000 -> 00:00:01,919] 我觉得就是技术垄断你也可以这么说
[00:00:01,919 -> 00:00:03,200] 对对对技术垄断也OK
[00:00:03,439 -> 00:00:07,320] 但我不太担心所谓的商业垄断或者技术垄断
[00:00:07,320 -> 00:00:08,279] 这一点跟您说的一样
[00:00:08,279 -> 00:00:11,400] 但我担心的如果它是一个意识垄断
[00:00:11,400 -> 00:00:13,039] 或者是一个信息垄断
[00:00:13,160 -> 00:00:16,960] 我们怎么作为普通人怎么来判断大公司是否作恶
[00:00:17,000 -> 00:00:21,640] 或者传达给我们的比如说意识形态是不是是正确的
[00:00:22,920 -> 00:00:24,280] 你今天能判断吗
[00:00:25,000 -> 00:00:27,280] 就是我在那个文章里边有一个截图
[00:00:00,000 -> 00:00:05,259] 就是說一個某中國專家說評價股市的東西
[00:00:05,259 -> 00:00:07,099] 然後我們去問Chai GPT
[00:00:07,099 -> 00:00:08,839] 他這樣說是不是在偷換概念
[00:00:08,839 -> 00:00:09,880] 然後Chai GPT說對
[00:00:09,880 -> 00:00:11,339] 他只是偷換概念一二三
[00:00:11,339 -> 00:00:14,539] 就是我們今天的輿論已經被污染到了一個
[00:00:14,539 -> 00:00:15,919] 就是為什麼你會care
[00:00:15,919 -> 00:00:17,920] 一個工具是否
[00:00:17,920 -> 00:00:20,219] 就是這個工具會讓這件事情惡化嗎
[00:00:20,219 -> 00:00:23,019] 我覺得它反而會把很多理性給帶回來
[00:00:24,019 -> 00:00:24,620] 不知道
[00:00:24,620 -> 00:00:25,420] 這是一個討論
[00:00:25,420 -> 00:00:28,920] 就是你為什麼會擔心它壟斷你的意識形態
[00:00:00,000 -> 00:00:05,000] 而不是在擔心你的意識形態已經被污染到一個程度了
[00:00:05,000 -> 00:00:09,800] 對 就是都擔心 其實只是我會擔心更加劇
[00:00:09,800 -> 00:00:13,300] 我會覺得因為這個依賴一旦形成
[00:00:13,300 -> 00:00:15,300] 現在其實已經有信息解放了
[00:00:15,300 -> 00:00:18,800] 就是我們看到的抖音一定是自己本身就在看的
[00:00:18,800 -> 00:00:20,399] 好 我知道這個東西的答案
[00:00:20,399 -> 00:00:23,300] 就是 開發者自己發了一個博客
[00:00:00,000 -> 00:00:02,000] 他就是说他怎么样子去
[00:00:02,000 -> 00:00:04,480] 怎么样子去就是
[00:00:05,360 -> 00:00:07,480] 就是处理你这个问题
[00:00:07,480 -> 00:00:08,960] 就是你这个问题就是
[00:00:11,199 -> 00:00:12,519] 哪怕在美国是吧
[00:00:12,560 -> 00:00:14,199] 他有一个主流的意识形态
[00:00:14,240 -> 00:00:16,120] 然后尤其OpenAI这个公司在
[00:00:16,120 -> 00:00:17,160] 在舊金山
[00:00:17,359 -> 00:00:19,079] 他是有这样的一个意识形态
[00:00:19,079 -> 00:00:21,079] 且大家对在这个意识形态下
[00:00:21,079 -> 00:00:22,399] 会认为一些东西是对的
[00:00:22,399 -> 00:00:23,679] 另外一些东西是错的
[00:00:23,839 -> 00:00:25,359] 然后但是OpenAI说
[00:00:25,480 -> 00:00:27,440] 我给大家开放的这个产品
[00:00:00,000 -> 00:00:02,879] 我会遵守一定程度的意识形态
[00:00:02,879 -> 00:00:05,320] 但是我回头给大家开放的能力
[00:00:05,320 -> 00:00:07,280] 这些东西是可调的
[00:00:07,280 -> 00:00:09,439] 它要遵守最基本的原则
[00:00:09,439 -> 00:00:12,640] 但是它这些东西是希望让大家可以自由去定制
[00:00:12,640 -> 00:00:14,199] 明白 感谢
[00:00:14,199 -> 00:00:16,239] 然后第二个问题是
[00:00:16,239 -> 00:00:18,559] 因为刚才其实前面也提到的是
[00:00:18,559 -> 00:00:21,320] GP情况下的一个教育问题
[00:00:21,320 -> 00:00:25,600] 因为和以前的硬是比如说我们学东西是为了考试这样
[00:00:25,600 -> 00:00:29,120] 现在比如我在想在新的小朋友教育
[00:00:00,000 -> 00:00:04,599] 包括我之前应该是看过另外他们一个工程师和和老黄
[00:00:04,620 -> 00:00:07,400] 就是黄仁勋的一个对谈里面也提到就这个
[00:00:07,400 -> 00:00:10,320] 就是未来要怎么去传递信息
[00:00:10,320 -> 00:00:12,800] 或者是未来的人应该学习什么
[00:00:12,800 -> 00:00:14,160] 这个您是怎么看的
[00:00:15,599 -> 00:00:16,640] 没法回答
[00:00:16,660 -> 00:00:18,600] 但是就是我应该我刚笑了一下
[00:00:18,600 -> 00:00:21,120] 就是我那个我看我想到了一个梗图
[00:00:21,120 -> 00:00:23,839] 那个梗图是一个人说要找工作
[00:00:23,859 -> 00:00:26,120] 然后用GPT生成了一堆疫苗
[00:00:26,120 -> 00:00:28,280] 然后另外一个人收到那个疫苗以后
[00:00:00,000 -> 00:00:04,320] 用GPT總結 最後就是我要找工作和這個人想要這個工作兩句話
[00:00:04,320 -> 00:00:06,120] 然後中間一大堆廢話
[00:00:06,120 -> 00:00:08,960] 對 就是怎麼樣子去
[00:00:08,960 -> 00:00:13,759] 就是就是當我們的很多東西都被GPT取代和使用的時候
[00:00:13,759 -> 00:00:15,400] 就是我們如果只看第一步
[00:00:15,400 -> 00:00:16,039] 我們會發現
[00:00:16,039 -> 00:00:16,359] 哦
[00:00:16,359 -> 00:00:18,160] 我的寫作不需要了
[00:00:18,160 -> 00:00:19,960] 只看第二步我的理解不需要了
[00:00:19,960 -> 00:00:20,879] 然後第三步發現
[00:00:20,879 -> 00:00:21,039] 哦
[00:00:21,039 -> 00:00:24,000] 這個溝通過程其實已經被GPT完全取代的情況下
[00:00:24,000 -> 00:00:26,480] 那我們到底還剩下什麼
[00:00:26,480 -> 00:00:27,879] 我我我真的沒法回答
[00:00:00,000 -> 00:00:01,919] 我只能覺得就是
[00:00:01,919 -> 00:00:03,560] 所以說為什麼我會提那個第五問
[00:00:03,560 -> 00:00:05,280] 就是人和GPT的區別是什麼
[00:00:05,280 -> 00:00:08,240] 那我覺得你既然能識別到一些區別
[00:00:08,240 -> 00:00:11,000] 然後去增強你的區別應該是沒錯的吧
[00:00:11,000 -> 00:00:13,160] 那就是批判性思維的判斷力了
[00:00:13,160 -> 00:00:15,119] 或者說就是批判性思維吧
[00:00:15,119 -> 00:00:16,879] 這也是那個劉嘉老師的觀點啊
[00:00:16,879 -> 00:00:20,000] 我就是就是你剛剛你剛剛問這個問題
[00:00:20,000 -> 00:00:21,280] 我又想到了一個
[00:00:21,280 -> 00:00:24,239] 我那個時候應該是剛下
[00:00:24,239 -> 00:00:26,039] 反正我在打車
[00:00:26,039 -> 00:00:27,800] 週日的下午打車
[00:00:00,000 -> 00:00:04,040] 然後就看到了好多中小學生穿著校服出來
[00:00:04,040 -> 00:00:05,519] 我在想這發生了什麼
[00:00:05,519 -> 00:00:07,040] 他們應該是去補習班吧
[00:00:07,040 -> 00:00:08,359] 穿著校服去補習班
[00:00:08,359 -> 00:00:11,919] 然後那時候正在聽劉嘉老師那個播客
[00:00:11,919 -> 00:00:13,880] 劉嘉老師在說教育沒有意義了
[00:00:13,880 -> 00:00:17,960] 就是未來一定是你要想方設法提高平安性思維
[00:00:17,960 -> 00:00:19,480] 我當然在那聽著點頭點頭
[00:00:19,480 -> 00:00:21,679] 然後看到了一堆出補習班的小學生
[00:00:21,679 -> 00:00:24,839] 我就覺得趕緊不要去補習了
[00:00:24,839 -> 00:00:26,519] 就是趕緊去幹點有用的事吧
[00:00:27,839 -> 00:00:28,800] 好感謝
[00:00:00,000 -> 00:00:03,799] 然後第三個問題就是其實和倫理相關
[00:00:04,000 -> 00:00:05,599] 然後比如說
[00:00:05,799 -> 00:00:08,960] 因為我已經看到有些判例在美國發生
[00:00:09,160 -> 00:00:12,880] 其中一個是那個就比如說我先舉個例子吧
[00:00:12,880 -> 00:00:14,599] 就比如說我們訓練那個AI
[00:00:14,800 -> 00:00:17,800] 然後他作為律師或者是作為法官來判法
[00:00:17,800 -> 00:00:20,320] 或者說幫我們進行醫療
[00:00:20,440 -> 00:00:22,199] 如果他誤診的情況下
[00:00:22,440 -> 00:00:23,879] 那這個怎麼算
[00:00:23,879 -> 00:00:25,320] 好問題終於這個問題來了
[00:00:25,320 -> 00:00:26,359] 我可以吐槽一下
[00:00:26,800 -> 00:00:28,719] 我這條腿就是我現在動的時候
[00:00:28,719 -> 00:00:29,800] 它都會響
[00:00:00,000 -> 00:00:03,140] 就這個膝蓋會響為什麼我之前那個在美國踢球
[00:00:03,359 -> 00:00:07,000] 然後就是呃崴了一下然後這腿巨疼
[00:00:07,259 -> 00:00:08,500] 然後我就去看醫生
[00:00:08,699 -> 00:00:10,160] 然後看的是一個普通的醫生
[00:00:10,160 -> 00:00:11,560] 那個醫生說你去拍個X光片吧
[00:00:11,560 -> 00:00:12,960] 看看有沒有骨頭有沒有問題
[00:00:12,960 -> 00:00:14,160] 然後拍了以後說沒有問題
[00:00:14,160 -> 00:00:15,859] 那你休息兩個兩個月
[00:00:16,059 -> 00:00:18,039] 然後你就回去可以繼續踢球了
[00:00:18,100 -> 00:00:20,160] 我休息兩個月回去踢球上場五分鐘
[00:00:20,500 -> 00:00:21,440] 哗 又斷了
[00:00:21,559 -> 00:00:23,699] 然後超級疼就動不了
[00:00:23,960 -> 00:00:26,839] 然後我再去找找了另外一個醫生
[00:00:26,899 -> 00:00:27,760] 那個醫生說
[00:00:00,000 -> 00:00:03,500] 你這種傷必須要拍那個核磁共振
[00:00:03,500 -> 00:00:06,500] 因為你的韌帶X光是拍不到的
[00:00:06,500 -> 00:00:07,799] 然後拍了核磁共振
[00:00:07,799 -> 00:00:09,500] 我的前韌帶已經完全斷掉了
[00:00:09,500 -> 00:00:10,900] 就之前是斷了一點點
[00:00:10,900 -> 00:00:13,099] 然後第二次就一下子全都斷了
[00:00:13,099 -> 00:00:14,000] 對
[00:00:14,000 -> 00:00:16,300] 然後我就做了這個膝蓋重建
[00:00:16,300 -> 00:00:17,800] 然後就從這抽了一根筋
[00:00:17,800 -> 00:00:20,600] 然後再把這個膝蓋全身端上去
[00:00:20,600 -> 00:00:23,600] 再加上我的復健做的不是很那個有點偷懶啊
[00:00:23,600 -> 00:00:26,100] 所以說總之我現在這動是會響的
[00:00:26,100 -> 00:00:28,899] 那他媽的那第一個醫生誰去追責啊
[00:00:00,000 -> 00:00:01,919] 我沒辦法追他子了
[00:00:01,919 -> 00:00:02,960] 就是他給了我一個建議
[00:00:02,960 -> 00:00:03,759] 然後遵守了
[00:00:03,759 -> 00:00:04,919] 然後我腿廢了
[00:00:04,919 -> 00:00:05,679] 這樣感覺
[00:00:05,679 -> 00:00:09,080] 我之後就是不太能做那種橫向的球的運動
[00:00:09,080 -> 00:00:13,039] 我覺得今天其實我們社會機制沒有很追
[00:00:13,039 -> 00:00:19,480] 就是我絕對是有一半的醫生律師和教師是不夠格的
[00:00:19,839 -> 00:00:20,320] 對吧
[00:00:20,320 -> 00:00:23,359] 如果說你的夠格是有60%的水平線
[00:00:23,359 -> 00:00:25,239] 那肯定一半以上是不夠格的
[00:00:25,239 -> 00:00:27,280] 那這些不夠格的人他們仍然在做
[00:00:00,000 -> 00:00:04,280] 我們如果讓GPT可以把這些人的能力提高60%的話
[00:00:04,719 -> 00:00:07,120] 就是提到那個60%的這個合格獻賞的話
[00:00:07,120 -> 00:00:08,160] 那我覺得是個好事
[00:00:08,759 -> 00:00:11,720] 我相信就是一個醫療GPT
[00:00:11,720 -> 00:00:12,839] 他訓練了一年
[00:00:12,839 -> 00:00:14,439] 他不會讓我只拍個X光
[00:00:14,439 -> 00:00:16,719] 他會讓我去拍下那個核磁共振的
[00:00:17,679 -> 00:00:18,079] 明白
[00:00:18,079 -> 00:00:20,079] 我就基於這個稍微衍生一下
[00:00:20,079 -> 00:00:23,879] 就是因為比如說就極端的情況下
[00:00:23,879 -> 00:00:25,839] 那社會也支持我說
[00:00:25,839 -> 00:00:29,079] 比如說我去追這個責任或者保護我的權利
[00:00:00,000 -> 00:00:07,000] 然后有一个医生误诊,然后呢,因为这里面有一个判例是用stable diffusion的人,
[00:00:07,000 -> 00:00:11,000] 然后之前是有一个人用stable diffusion做了一幅画,
[00:00:11,000 -> 00:00:14,000] 然后另一个人呢拿这幅画去做了一个商业用途,
[00:00:14,000 -> 00:00:17,000] 然后另外一个人就假设,使用这个人叫B吧,
[00:00:17,000 -> 00:00:24,000] B他去那个,呃,申请,呃,那个,使用,商业化使用这幅画以后被A发现了,
[00:00:24,000 -> 00:00:28,000] A就说那这是我生成的一个逻辑,对,
[00:00:00,000 -> 00:00:02,600] 然后美国的法院的判罚是
[00:00:02,600 -> 00:00:04,700] 因为它是一个无主体的生产
[00:00:04,700 -> 00:00:07,099] 这是AI生产的,它是一个无主体的
[00:00:07,099 -> 00:00:10,800] 所以你也不能主张你的版权逻辑
[00:00:10,800 -> 00:00:12,400] 你的版权权利
[00:00:12,400 -> 00:00:15,800] 所以我会担心这后面像类似的
[00:00:15,800 -> 00:00:18,600] 大量的由于我们使用了AI
[00:00:18,600 -> 00:00:21,600] 或者说更广泛的在社会里面用以后
[00:00:21,600 -> 00:00:23,600] 会出现大量无主体的事情
[00:00:23,600 -> 00:00:26,199] 包括OpenAI我也看到它那个组织架构
[00:00:26,199 -> 00:00:28,300] 就是它有NTO这一层
[00:00:00,000 -> 00:00:03,200] 其實我覺得是一把雙刃劍
[00:00:03,200 -> 00:00:04,679] 它既好又不好
[00:00:04,679 -> 00:00:09,080] 那就變得我們沒有辦法去追溯這一個
[00:00:09,080 -> 00:00:11,240] 和它對抗的一個過程了
[00:00:11,240 -> 00:00:12,560] 對 如果它真的作惡了
[00:00:12,560 -> 00:00:17,719] 我們就毫無任何的一個立場點
[00:00:17,719 -> 00:00:18,120] 對
[00:00:18,120 -> 00:00:21,359] 有 就是利益不是你追責的邏輯
[00:00:21,359 -> 00:00:22,280] 它如果做了壞事
[00:00:22,280 -> 00:00:24,120] 你是可以追到那個組織的責任的
[00:00:24,120 -> 00:00:26,679] 包括那個Sam在Galaxy的那個
[00:00:26,679 -> 00:00:28,480] Podcast播放裡面
[00:00:00,000 -> 00:00:01,520] 他也非常
[00:00:01,520 -> 00:00:04,160] 我觉得就是非常英雄主义的去说
[00:00:04,160 -> 00:00:06,440] 就是能力是neutral的
[00:00:06,440 -> 00:00:08,359] 但是这个工具一定是有责任的
[00:00:08,359 -> 00:00:10,839] 然后他觉得工具的责任都在
[00:00:10,839 -> 00:00:12,519] 在OpenAI的所有人里边
[00:00:12,720 -> 00:00:14,880] 他是认为OpenAI的人是有责任
[00:00:14,880 -> 00:00:16,320] 去把这个工具用好
[00:00:16,320 -> 00:00:19,280] 和把AGI的可能性给控制好的
[00:00:19,440 -> 00:00:21,960] 所以说是有追责的可能性
[00:00:21,960 -> 00:00:22,760] 可以追到他们
[00:00:22,760 -> 00:00:24,160] 就是你不行你去骂他们
[00:00:24,160 -> 00:00:25,760] 你去怎么样都行
[00:00:25,960 -> 00:00:27,199] 是是有人的
[00:00:27,239 -> 00:00:28,559] 但是你刚刚说的那个版权
[00:00:00,000 -> 00:00:03,799] 我覺得版權法現在不是一個與時俱進的法律
[00:00:03,799 -> 00:00:06,080] 它在現有的情況下已經造成了很多問題
[00:00:06,080 -> 00:00:06,559] 對吧
[00:00:06,559 -> 00:00:09,240] 就是會讓一些人拿著舊版權
[00:00:09,240 -> 00:00:12,279] 然後可以到處去起訴別人去幹這種事件
[00:00:12,279 -> 00:00:16,160] 那他就更沒有去應對OpenAI
[00:00:16,160 -> 00:00:19,039] 就是他沒有在他更
[00:00:19,039 -> 00:00:20,440] 對 所以我覺得就是
[00:00:20,760 -> 00:00:24,800] 立法者去了解這個技術
[00:00:24,800 -> 00:00:28,160] 然後去給出一個有原則的監管
[00:00:28,160 -> 00:00:29,640] 我覺得這是很重要的
[00:00:00,000 -> 00:00:04,000] 但是我们不能说现有的立法没跟上
[00:00:04,000 -> 00:00:05,799] 然后就出现一个新技术
[00:00:06,599 -> 00:00:09,080] 反正我觉得答案肯定是你要立法跟上
[00:00:09,080 -> 00:00:11,480] 然后我最后一个问题
[00:00:11,480 -> 00:00:14,240] 因为我本身是做游戏的
[00:00:14,240 -> 00:00:18,160] 所以我对包括除了GP之外
[00:00:18,160 -> 00:00:20,800] 对SD他们那些也都有一些应用
[00:00:20,800 -> 00:00:23,359] 然后就有里面有一个话
[00:00:23,359 -> 00:00:24,719] 其实刚才宋老师也提到
[00:00:24,719 -> 00:00:26,320] 就是这个是智力的文法
[00:00:26,320 -> 00:00:28,719] 然后我在理解其他SD的时候
[00:00:00,000 -> 00:00:02,319] 我会理解这是一个生产力的分发
[00:00:02,319 -> 00:00:03,359] 对
[00:00:03,359 -> 00:00:06,799] 但是我的观点就是是不是生产力的分发
[00:00:06,799 -> 00:00:09,880] 反而对腾讯这样的大公司或者对于IG是
[00:00:09,880 -> 00:00:11,480] 其实不是一个好的事情
[00:00:11,480 -> 00:00:12,960] 因为在做游戏以前
[00:00:12,960 -> 00:00:16,280] 我们会认为美术我们是我们的护城河或者壁垒
[00:00:16,280 -> 00:00:18,359] 那现在被分发之后
[00:00:18,359 -> 00:00:19,239] 那其实呢
[00:00:19,239 -> 00:00:21,480] 大家真的就是拼创意了
[00:00:21,480 -> 00:00:24,440] 对这个不知道您是怎么看行业的呢
[00:00:25,920 -> 00:00:26,359] 没说
[00:00:27,839 -> 00:00:29,000] 我的文章里面有说
[00:00:00,000 -> 00:00:04,419] 其实就是他对大公司可能是件好事
[00:00:04,419 -> 00:00:06,139] 但是大公司没有把牌打好
[00:00:06,580 -> 00:00:07,860] 他的好事是什么呢
[00:00:07,860 -> 00:00:09,179] 大公司有现成的用户
[00:00:09,179 -> 00:00:10,060] 现成的场景
[00:00:10,539 -> 00:00:12,140] 然后这些比如说游戏
[00:00:12,140 -> 00:00:14,419] Chad GPT他不能取代游戏
[00:00:14,460 -> 00:00:17,059] 暂时不能取代游戏的这个内容本身对吧
[00:00:17,260 -> 00:00:19,420] 所以说你有现成的游戏用户
[00:00:19,420 -> 00:00:21,379] 你有现成的这个场景的情况下
[00:00:21,539 -> 00:00:23,579] 你能不能用Chad GPT去增强你
[00:00:23,859 -> 00:00:25,179] 我觉得是可以的
[00:00:25,420 -> 00:00:27,899] 尤其像企业微信同学文档
[00:00:27,899 -> 00:00:29,579] 这些微信这些东西
[00:00:00,000 -> 00:00:02,680] 它其实是可以被拆GPT非常好的增强的
[00:00:02,680 -> 00:00:05,240] 那你如果能增强的话这是一个加分项
[00:00:05,240 -> 00:00:07,160] 然后第二个呢就是拆GPT
[00:00:07,160 -> 00:00:08,080] 不好意思大家
[00:00:09,080 -> 00:00:11,000] 就不是站在资本家的立场上
[00:00:11,000 -> 00:00:12,279] 但是这是一个事实
[00:00:12,279 -> 00:00:15,000] 就是它会极大的降低管理成本
[00:00:17,120 -> 00:00:18,079] 就这句话吧
[00:00:18,079 -> 00:00:19,440] 就是极大降低管理成本
[00:00:19,440 -> 00:00:21,000] 那最终受益的应该
[00:00:21,000 -> 00:00:22,679] 最大的受益方应该是大公司
[00:00:23,039 -> 00:00:24,800] 这是两个加分项
[00:00:24,800 -> 00:00:29,359] 但是对于AEG来说确实就是我们相对其他的游戏大厂的优势
[00:00:00,000 -> 00:00:02,879] 是在于我们的工业化生产的能力对吧
[00:00:02,879 -> 00:00:04,919] 我们有一堆人可以去做这件事
[00:00:04,919 -> 00:00:07,719] 那它是不是会被极大的磨平
[00:00:07,719 -> 00:00:09,679] 我其实在那个文档里边有一个
[00:00:09,679 -> 00:00:11,480] 一上来不是列了很多点吗
[00:00:11,480 -> 00:00:12,679] 然后一个灰色点
[00:00:12,679 -> 00:00:14,759] 我不知道大家一般灰色大家会看不见
[00:00:14,759 -> 00:00:17,079] 但你回去看的话那个灰色点是
[00:00:17,079 -> 00:00:18,719] 一个小团队
[00:00:18,719 -> 00:00:20,839] 我觉得两年内大概率会发生的是
[00:00:20,839 -> 00:00:24,160] 一个小团队半年就可以做出一个3A级别的游戏
[00:00:25,079 -> 00:00:26,839] 然后他卖5块钱就可以赚钱
[00:00:00,000 -> 00:00:03,399] 這個對我們騰訊的業務邏輯其實是個很大的顛覆
[00:00:03,399 -> 00:00:05,400] 明白 感謝感謝
[00:00:05,400 -> 00:00:07,400] 你好老師
[00:00:07,400 -> 00:00:09,400] 其實我是學法律的
[00:00:09,400 -> 00:00:11,400] 然後前面您說的我讓我吃了八斗
[00:00:11,400 -> 00:00:13,400] 然後反省一下自己能力夠不夠
[00:00:13,400 -> 00:00:15,400] 就涉及到第一個反壟斷
[00:00:15,400 -> 00:00:17,399] 和包括內容安全一系列的
[00:00:17,399 -> 00:00:19,399] 包括生存是人工智能這些所有要求
[00:00:19,399 -> 00:00:21,399] 只是說目前國內外一直探討這些事情
[00:00:21,399 -> 00:00:23,399] 大家還沒有明確的一些定論
[00:00:23,399 -> 00:00:25,399] 大多從很公開的倫理上來說的話
[00:00:25,399 -> 00:00:27,399] 這個問題還不少
[00:00:27,399 -> 00:00:29,399] 然後我想問兩個問題
[00:00:00,000 -> 00:00:22,000] 我问两个问题,第一就是问一下您关于GPT其实让人思考人和以为人的事情,因为我大概是从2013年左右当时发生AlphaGo的事情,包括那个深度学习网络从2006年有突破性进展,2013年,包括2018年左右,当时GPT逐渐去衍生,其实已经关掉这个了,
[00:00:00,000 -> 00:00:06,200] 从我个人角度,因为这个法律人律师包草之后,其实在学英语的时候,当时就介绍了说
[00:00:06,200 -> 00:00:13,800] DeepMind它的模型已经让一个机器很快地理解语言,为什么我们学英语还要去背
[00:00:13,800 -> 00:00:23,000] 后来继续上学,我逐渐意识到说为什么很多东西看似好像我是理解了,但是我无法明确我自己的意识,或者我怎么理解它
[00:00:00,000 -> 00:00:12,000] 然后后来涉及到这个TPT的事情,我感觉说,就是我们现在在从应用层面去学它表面的功夫,表面一切从模型框架到应用,但它只是比相一小。
