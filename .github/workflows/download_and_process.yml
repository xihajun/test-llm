name: Download and Process MP3

on:
  workflow_dispatch:
    inputs:
      url:
        description: "YouTube视频URL"
        required: true
        type: string
        
permissions:
  contents: write
  
jobs:
  download-and-process:
    runs-on: downloader
    outputs:
      branch_id: ${{ steps.set_branch_id.outputs.branch_id }}
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set Branch ID
      id: set_branch_id
      run: |
        VIDEO_ID=$(echo "${{ github.event.inputs.url }}" | grep -oP "(?<=v=)[^&]*|(?<=be/)[^&]*")
        TIMESTAMP=$(date +%Y%m%d_%H%M%S)
        BRANCH_ID="${VIDEO_ID}_${TIMESTAMP}"
        echo "branch_id=${BRANCH_ID}" >> $GITHUB_OUTPUT
        
    - name: Download MP3 from YouTube
      run: yt-dlp -x --audio-format mp3 -o "output.mp3" "${{ github.event.inputs.url }}"
      
    - name: Commit MP3 to repository
      env:
        GITHUB_TOKEN: ${{ secrets.GH_TOKEN }}
      run: |
        git config --global user.name 'github-actions'
        git config --global user.email 'github-actions@github.com'
        git checkout -b "process-${{ steps.set_branch_id.outputs.branch_id }}"
        git add output.mp3
        git commit -m 'Add downloaded MP3 file'
        git push origin "process-${{ steps.set_branch_id.outputs.branch_id }}"

  process-mp3:
    runs-on: runner
    needs: download-and-process
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        ref: "process-${{ needs.download-and-process.outputs.branch_id }}"
        
    - name: Process MP3 with Whisper JAX
      run: |
      0, 10.08), 'text': ' We can peek at or get the last error by get last error and hit p-glasther.'}, {'timestamp': (10.08, 15.76), 'text': ' And we can also get a corresponding error string for each error code by say a simple hash'}, {'timestamp': (15.76, 24.28), 'text': ' define check where we query the hip error status and report an error on a board if we'}, {'timestamp': (0.0, 4.56), 'text': ' an encounter in error. That should be a hit peak lost error. It should have two ease.'}, {'timestamp': (4.56, 8.72), 'text': " It's just a typo. Sorry. We've reached the peak of our lost error. Yeah."}, {'timestamp': (12.24, 16.72), 'text': ' All right. In a very simple high level way, this is how we would put it all together.'}, {'timestamp': (16.72, 23.84), 'text': ' And this is a simple example of what a GPU enabled hip code may look like. So we start with our'}, {'timestamp': (0.0, 6.96), 'text': ' hit our main function and include our hip runtime header with hip slash hip runtime.'}, {'timestamp': (6.96, 11.34), 'text': ' This will be located in your rock and path, which is by default installed in slash op slash'}, {'timestamp': (11.34, 14.0), 'text': ' rock and then you want to.'}, {'timestamp': (14.0, 18.34), 'text': ' So we would start by first allocating some memory both on host and device and checking'}, {'timestamp': (18.34, 22.0), 'text': ' our error codes as we do so.'}, {'timestamp': (22.0, 26.52), 'text': ' Olipses sometime later after filling the data will move that data to the device by'}, {'timestamp': (0.0, 4.8), 'text': ' I hit Memcopy, launch our kernel to do some work on that device,'}, {'timestamp': (4.8, 7.52), 'text': ' and then copy the result back.'}, {'timestamp': (7.52, 9.76), 'text': " And then sometime later, once we've used our data"}, {'timestamp': (9.76, 12.12), 'text': ' to some satisfying way in our application,'}, {'timestamp': (12.12, 14.72), 'text': " we'll clean up after ourselves."}, {'timestamp': (14.72, 16.8), 'text': ' Somewhere else in my code, my kernel is declared.'}, {'timestamp': (16.8, 17.96), 'text': ' It could be in a separate file.'}, {'timestamp': (17.96, 20.4), 'text': ' It could just be above my main function.'}, {'timestamp': (20.4, 24.8), 'text': " And finally, there's a hash define for my error check."}, {'timestamp': (24.8, 28.76), 'text': ' So this, at a 10,000 foot view, would be what most applications'}, {'timestamp': (0.0, 5.0), 'text': ' may resemble when offloading some GPU accelerated portion'}, {'timestamp': (5.0, 7.0), 'text': ' to a GPU panel.'}]}

        source /home/junfan/test/venv/bin/activate
        export HIP_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
        python3 << EOF
        from transformers import pipeline

        # Initialize the pipeline with bfloat16 precision and batching
        asr_pipeline = pipeline("automatic-speech-recognition", model="openai/whisper-base", device=0)
    
        # Transcribe the audio file with timestamps
        result = asr_pipeline("output.mp3", return_timestamps=True)
    
        # Save the transcription with timestamps
        with open('transcript.srt', 'w', encoding='utf-8') as f:
            for i, segment in enumerate(result['chunks'], start=1):
                start = segment['timestamp'][0]
                end = segment['timestamp'][1]
                text = segment['text'].strip()
                
                f.write(f"{i}\n")
                f.write(f"{format_timestamp(start)} --> {format_timestamp(end)}\n")
                f.write(f"{text}\n\n")
    
        print("Transcription completed and saved to transcript.srt")
    


You said:
      0, 10.08), 'text': ' We can peek at or get the last error by get last error and hit p-glasther.'}, {'timestamp': (10.08, 15.76), 'text': ' And we can also get a corresponding error string for each error code by say a simple hash'}, {'timestamp': (15.76, 24.28), 'text': ' define check where we query the hip error status and report an error on a board if we'}, {'timestamp': (0.0, 4.56), 'text': ' an encounter in error. That should be a hit peak lost error. It should have two ease.'}, {'timestamp': (4.56, 8.72), 'text': " It's just a typo. Sorry. We've reached the peak of our lost error. Yeah."}, {'timestamp': (12.24, 16.72), 'text': ' All right. In a very simple high level way, this is how we would put it all together.'}, {'timestamp': (16.72, 23.84), 'text': ' And this is a simple example of what a GPU enabled hip code may look like. So we start with our'}, {'timestamp': (0.0, 6.96), 'text': ' hit our main function and include our hip runtime header with hip slash hip runtime.'}, {'timestamp': (6.96, 11.34), 'text': ' This will be located in your rock and path, which is by default installed in slash op slash'}, {'timestamp': (11.34, 14.0), 'text': ' rock and then you want to.'}, {'timestamp': (14.0, 18.34), 'text': ' So we would start by first allocating some memory both on host and device and checking'}, {'timestamp': (18.34, 22.0), 'text': ' our error codes as we do so.'}, {'timestamp': (22.0, 26.52), 'text': ' Olipses sometime later after filling the data will move that data to the device by'}, {'timestamp': (0.0, 4.8), 'text': ' I hit Memcopy, launch our kernel to do some work on that device,'}, {'timestamp': (4.8, 7.52), 'text': ' and then copy the result back.'}, {'timestamp': (7.52, 9.76), 'text': " And then sometime later, once we've used our data"}, {'timestamp': (9.76, 12.12), 'text': ' to some satisfying way in our application,'}, {'timestamp': (12.12, 14.72), 'text': " we'll clean up after ourselves."}, {'timestamp': (14.72, 16.8), 'text': ' Somewhere else in my code, my kernel is declared.'}, {'timestamp': (16.8, 17.96), 'text': ' It could be in a separate file.'}, {'timestamp': (17.96, 20.4), 'text': ' It could just be above my main function.'}, {'timestamp': (20.4, 24.8), 'text': " And finally, there's a hash define for my error check."}, {'timestamp': (24.8, 28.76), 'text': ' So this, at a 10,000 foot view, would be what most applications'}, {'timestamp': (0.0, 5.0), 'text': ' may resemble when offloading some GPU accelerated portion'}, {'timestamp': (5.0, 7.0), 'text': ' to a GPU panel.'}]}

        source /home/junfan/test/venv/bin/activate
        export HIP_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
        python3 << EOF
        from transformers import pipeline

        # Initialize the pipeline with bfloat16 precision and batching
        asr_pipeline = pipeline("automatic-speech-recognition", model="openai/whisper-base", device=0)
    
        # Transcribe the audio file with timestamps
        result = asr_pipeline("output.mp3", return_timestamps=True)
    
        def format_timestamp(seconds):
            hours = int(seconds // 3600)
            minutes = int((seconds % 3600) // 60)
            seconds = seconds % 60
            milliseconds = int((seconds - int(seconds)) * 1000)
            return f"{hours:02}:{minutes:02}:{int(seconds):02},{milliseconds:03}"

        # Save the transcription with timestamps
        with open('transcript.srt', 'w', encoding='utf-8') as f:
            for i, segment in enumerate(result['chunks'], start=1):
                start = segment['timestamp'][0]
                end = segment['timestamp'][1]
                text = segment['text'].strip()
                
                f.write(f"{i}\n")
                f.write(f"{format_timestamp(start)} --> {format_timestamp(end)}\n")
                f.write(f"{text}\n\n")
    
        print("Transcription completed and saved to transcript.srt")
    
        EOF

    - name: Commit transcript
      run: |
        git config --global user.name 'github-actions'
        git config --global user.email 'github-actions@github.com'
        git add .
        git commit -m 'Add transcript'
        git push origin "process-${{ needs.download-and-process.outputs.branch_id }}"
